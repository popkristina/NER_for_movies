{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import string\n",
    "import os\n",
    "from torch import cuda\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertConfig, BertModel\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import transformers\n",
    "from transformers import BertForTokenClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from seqeval.metrics import f1_score, accuracy_score, recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import RobertaTokenizer, RobertaForTokenClassification\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    #os.chdir('D:/TU_Graz/Thesis/Datasets/Reddit_features')\n",
    "    \n",
    "    train = pd.read_csv(\"../input/bio-tagged/train_final_all.csv\")\n",
    "    test = pd.read_csv(\"../input/bio-tagged/test_final_all.csv\")\n",
    "    \n",
    "    #train = pd.read_csv(\"train_final_all.csv\")\n",
    "    #test = pd.read_csv(\"test_final_all.csv\")\n",
    "    data = train.append(test)\n",
    "\n",
    "    return train, test, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_stats(data):\n",
    "    frequencies = data.BIO.value_counts()\n",
    "    tags = {}\n",
    "    for tag, count in zip(frequencies.index, frequencies):\n",
    "        if tag != \"O\":\n",
    "            if tag[2:5] not in tags.keys():\n",
    "                tags[tag[2:5]] = count\n",
    "            else:\n",
    "                tags[tag[2:5]] += count\n",
    "        continue\n",
    "    \n",
    "    print(\"Number of tags: {}\".format(len(data.BIO.unique())))\n",
    "    print(\"Tag frequencies: {}\".format(frequencies))\n",
    "    print(\"Categories: \")\n",
    "    print(sorted(tags.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_sentences(data, category):\n",
    "    all_sents = []\n",
    "    sent_ids = data['Sent_id'].unique()\n",
    "    for curr_id in sent_ids:\n",
    "        tmp_df = data[data['Sent_id'] == curr_id]\n",
    "        tmp_df = pd.concat([tmp_df['Token'], tmp_df[\"Token_index\"], tmp_df.iloc[:,4:149], tmp_df[category]], axis = 1)\n",
    "        records = tmp_df.to_records(index=False)\n",
    "        all_sents.append(records)\n",
    "    return all_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_processor_params():\n",
    "    device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    torch.cuda.get_device_name(0)\n",
    "    return device, n_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_session_params():\n",
    "    max_length = 300\n",
    "    bs = 16\n",
    "    epochs = 7\n",
    "    learning_rate = 1e-05\n",
    "    max_grad_norm = 10\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    #tokenizer = RobertaTokenizer.from_pretrained('roberta-large')\n",
    "    #tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')\n",
    "    return max_length, bs, epochs, learning_rate, max_grad_norm, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_sents_over_threshold(sents, threshold):\n",
    "    sentences = list()\n",
    "    for s in sents:\n",
    "        if len(s) < threshold:\n",
    "            sentences.append(s)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence, sentence_labels):\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "    for word, label in zip(sentence, sentence_labels):\n",
    "        str_word = str(word)\n",
    "        tokenized_word = tokenizer.tokenize(str_word) # Tokenize the word\n",
    "        n_subwords = len(tokenized_word) # Count subwords\n",
    "        tokenized_sentence.extend(tokenized_word) # Add to the final tokenized list\n",
    "        labels.extend([label] * n_subwords) # Add the same label of the original word to all of its subwords\n",
    "    return tokenized_sentence, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves():\n",
    "    sns.set(style='darkgrid')\n",
    "    sns.set(font_scale=1.5)\n",
    "    plt.rcParams[\"figure.figsize\"] = (6,6)\n",
    "    plt.plot(loss_values, 'b-o', label=\"training loss\")\n",
    "    plt.plot(validation_loss_values, 'r-o', label=\"validation loss\")\n",
    "    plt.title(\"Learning curve\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, data = read_data() # We'll save the test set for later\n",
    "#data_stats(data)\n",
    "device, n_gpu = set_processor_params()\n",
    "\n",
    "tag_values = list(set(train[\"BIO\"].values))\n",
    "tag_values.append(\"PAD\")\n",
    "tag2idx = {t: i for i, t in enumerate(tag_values)}\n",
    "idx2tag = {i: t for i, t in enumerate(tag_values)}\n",
    "\n",
    "sents = group_sentences(data, 'BIO')\n",
    "sents = remove_sents_over_threshold(sents, 300)\n",
    "sentences = [[word[0] for word in sentence] for sentence in sents]\n",
    "labels = [[tag2idx[w[len(w)-1]] for w in s] for s in sents]\n",
    "train_sents, test_sents, train_labels, test_labels = train_test_split(sentences, labels, test_size=0.25, shuffle = False)\n",
    "\n",
    "MAX_LEN, BATCH_SIZE, EPOCHS, LEARNING_RATE, MAX_GRAD_NORM, tokenizer = set_session_params()\n",
    "\n",
    "tokenized_texts_and_labels = [\n",
    "    tokenize(sentence, sentence_labels) for sentence, sentence_labels in zip(train_sents, train_labels)\n",
    "]\n",
    "\n",
    "tokenized_texts = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels]\n",
    "labels_subwords = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels]\n",
    "\n",
    "# Cut the token and label sequences to the max length\n",
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                          maxlen = MAX_LEN, dtype=\"long\", value=0.0, \n",
    "                          truncating=\"post\", padding=\"post\")\n",
    "input_tags = pad_sequences([[l for l in lab] for lab in labels_subwords],\n",
    "                           maxlen = MAX_LEN, value = tag2idx[\"PAD\"], \n",
    "                           padding=\"post\", dtype=\"long\", truncating=\"post\")\n",
    "attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]\n",
    "\n",
    "# Train and validation split\n",
    "tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, input_tags, test_size=0.2)\n",
    "tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids, test_size=0.2)\n",
    "tr_inputs = torch.tensor(tr_inputs)\n",
    "val_inputs = torch.tensor(val_inputs)\n",
    "tr_tags = torch.tensor(tr_tags)\n",
    "val_tags = torch.tensor(val_tags)\n",
    "tr_masks = torch.tensor(tr_masks)\n",
    "val_masks = torch.tensor(val_masks)\n",
    "\n",
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Pretrained model params\n",
    "model = RobertaForTokenClassification.from_pretrained('roberta-base',\n",
    "                                                      output_attentions = False,\n",
    "                                                      output_hidden_states = False)\n",
    "#model = BertForTokenClassification.from_pretrained(\"xlm-roberta-base\", \n",
    "#                                                   num_labels = len(tag2idx),\n",
    "#                                                   output_attentions = False,\n",
    "#                                                   output_hidden_states = False)\n",
    "model.cuda(); # Pass the model parameters to gpu\n",
    "\n",
    "# Set optimizer parameters\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5, eps=1e-8)\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * EPOCHS\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_values, validation_loss_values = [], []\n",
    "accuracy_values, validation_accuracy_values = [], []\n",
    "\n",
    "for i in trange(EPOCHS, desc=\"Epoch\"):\n",
    "    \n",
    "    # TRAINING\n",
    "    # Perform one full pass over the training set\n",
    "    model.train() # Put the model into training mode\n",
    "    total_loss, total_accuracy = 0, 0 # Reset the total loss and acc. for current epoch\n",
    "    \n",
    "    # Training loop\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch) # add batch to gpu\n",
    "        b_input_ids, b_input_mask, b_labels = batch # Input ids, mask and labels of the current batch\n",
    "        model.zero_grad() # Always clear any previously calculated gradients before performing a backward pass\n",
    "    \n",
    "        # Forward pass\n",
    "        # This will return the loss (rather than the model output) because we have provided the `labels`.\n",
    "        outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs[0]\n",
    "        \n",
    "        # Perform a backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "        total_loss += loss.item() # track train loss\n",
    "        \n",
    "        # Clip the norm of the gradient to help prevent the exploding gradients problem\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=MAX_GRAD_NORM)\n",
    "        \n",
    "        optimizer.step() # update parameters\n",
    "        scheduler.step() # Update the learning rate\n",
    "        \n",
    "    avg_train_loss = total_loss / len(train_dataloader) # Calc. avg loss over training data\n",
    "    print(\"Average train loss: {}\".format(avg_train_loss))\n",
    "    loss_values.append(avg_train_loss) # Store the loss value for plotting the learning curve\n",
    "\n",
    "    # VALIDATION\n",
    "    # After the completion of each training epoch, measure performance on validation set  \n",
    "    model.eval() # Put the model into evaluation mode\n",
    "    eval_loss, eval_accuracy = 0, 0 # Reset the validation loss for current epoch\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    predictions , true_labels = [], []\n",
    "    \n",
    "    # Validation loop\n",
    "    for batch in valid_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "    \n",
    "        # Telling the model not to compute or store gradients, to save memory and speed up validation\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions\n",
    "            # This will return the logits rather than the loss because we have not provided labels\n",
    "            outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "            \n",
    "            #encoded_input = tokenizer(text, return_tensors='pt')\n",
    "            #output = model(**encoded_input)\n",
    "        \n",
    "        logits = outputs[1].detach().cpu().numpy() # Move logits to cpu\n",
    "        label_ids = b_labels.to('cpu').numpy() # Move labels to cpu\n",
    "        eval_loss += outputs[0].mean().item() # Valid. loss for current batch\n",
    "        \n",
    "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        true_labels.extend(label_ids)\n",
    "        \n",
    "    eval_loss = eval_loss / len(valid_dataloader)\n",
    "    validation_loss_values.append(eval_loss)\n",
    "    print(\"Validation loss: {}\".format(eval_loss))\n",
    "    \n",
    "    # Calculate the accuracy for this batch of test sentences\n",
    "    pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)\n",
    "                                 for p_i, l_i in zip(p, l) if tag_values[l_i] != \"PAD\"]\n",
    "    valid_tags = [tag_values[l_i] for l in true_labels\n",
    "                                  for l_i in l if tag_values[l_i] != \"PAD\"]\n",
    "    validation_accuracy_values.append(accuracy_score(pred_tags, valid_tags))\n",
    "    print(\"Validation Accuracy: {}\".format(accuracy_score(pred_tags, valid_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Prepare test set...\")\n",
    "test_sentences = [[word for word in sentence] for sentence in test_sents]\n",
    "#test_sentences = [\" \".join(sentence) for sentence in test_sentences]\n",
    "#true_labels = [[tag for w in s] for s in test_labels]\n",
    "\n",
    "print(\"Tokenize and predict...\")\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "for lab in test_labels:\n",
    "    all_true_labels.extend(lab)\n",
    "    \n",
    "for test_sentence in test_sentences:\n",
    "    tokenized_sentence = tokenizer.encode(test_sentence)\n",
    "    input_ids = torch.tensor([tokenized_sentence]).cuda()\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids)\n",
    "    label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)\n",
    "    label_indices = label_indices[0]\n",
    "    label_indices = label_indices[1:-1]\n",
    "    \n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
    "    new_tokens, new_labels = [], []\n",
    "    for token, label_idx in zip(tokens, label_indices):\n",
    "        if token.startswith(\"##\"):\n",
    "            new_tokens[-1] = new_tokens[-1] + token[2:]\n",
    "        else:\n",
    "            new_labels.append(tag_values[label_idx])\n",
    "            new_tokens.append(token)\n",
    "    all_predictions.extend(new_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = [tag2idx[label] for label in all_predictions]\n",
    "report = classification_report(all_true_labels, all_preds)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token, label, true in zip(test_sentences[0], new_labels, true_labels[0]):\n",
    "    print(\"{}\\t{}\\t{}\".format(token,label,idx2tag[true]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
