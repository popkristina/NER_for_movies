{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "from textblob import Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    os.chdir('D:/TU_Graz/Thesis/Datasets/Reddit_features')\n",
    "    data = pd.read_csv(\"test_submissions_features_FINALL_NS.csv\")\n",
    "    submissions = pd.read_csv(\"../Reddit_preprocessed/test_submissions_simplified_NL.csv\")\n",
    "    submissions = submissions.fillna(\"\")\n",
    "    movies_matched = pd.read_csv(\"../IMDB_database/movies_matched.csv\")\n",
    "    \n",
    "    return data, submissions, movies_matched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some movies are written in their original language when the user is writing a request, to we need a dictionary of all the alternative names that a movie can have, so that we can check them all when tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_alt_movie_dict(movies_matched):\n",
    "    alt_names = dict()\n",
    "    for index, row in movies_matched.iterrows():\n",
    "        if row[\"original_title\"] not in alt_names.keys():\n",
    "            alt_names[row[\"original_title\"]] = list()\n",
    "        alt_names[row[\"original_title\"]].append(row[\"alternative\"])\n",
    "    return alt_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TAG GENRES, KEYWORDS, MOVIES AND ACTORS FOR ENTITY TAG AND SENTIMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Iterate through every token in the dataset. Keep track of which submission it belongs to. \n",
    "Compare it against every genre entity, then actor entity and so on, that the crowdowkers \n",
    "identified for the corresponding submission. Note: these are in two different dataframes. \n",
    "\n",
    "The word can be misspelled, so a spell correction is run on it, in case it does not match any\n",
    "of the identified entity words. If the spell-checked word matches a word in one of the entity \n",
    "columns, then a corresponding entity tag is added to it.\n",
    "\n",
    "The identified entities in \"submissions\" df are written in the form: \n",
    "Actor1Name Actor1LastName | Actor2Name Actor2LastName etc. Since our training data is in token\n",
    "form, we split the multi-token entities and compare token to token. \n",
    "\n",
    "The tags assigned are per entity, reagrdless of whether it's a first word of an entity or a \n",
    "continuation (second, third and so on). Checks are also made per sentiment. Whether the word is a part of a positive-entity or negative-\n",
    "sentiment. \n",
    "\n",
    "Special case for the movies, in the 'submissions' df the names of the movies identified are as\n",
    "written on IMDB. Sometimes, movies have alternative names in several languages, and the user\n",
    "could've written it in a different lanugage, so we also keep track of the corresonding names\n",
    "for every movie and match the current token against them as well.\n",
    "\n",
    "\"\"\"\n",
    "def tag_entity(data, submissions, alt_names):\n",
    "    genre_tags = []\n",
    "    actor_tags = []\n",
    "    keyword_tags = []\n",
    "    movie_tags = []\n",
    "    sents_movies = []\n",
    "    sents_gen = []\n",
    "    sents_actor = []\n",
    "    sents_keyword = []\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        current_sentence = int(row[\"Sentence\"].split(\" \")[1])\n",
    "        word = Word(row[\"Token\"].lower())\n",
    "        #token_spellcheck = word.spellcheck()[0][0]\n",
    "        token_spellcheck = [w[0] for w in word.spellcheck()] # it's a list of closest words\n",
    "    \n",
    "    \n",
    "        ### TAG GENRES ###\n",
    "    \n",
    "        # Assign entity tag (genre or other)\n",
    "        splits = submissions.iloc[current_sentence][\"genres\"].split(\"|\")\n",
    "        splits = [s.lower() for s in splits]\n",
    "        spellcheck = False\n",
    "        for t in token_spellcheck:\n",
    "            if t in splits:\n",
    "                spellcheck = True\n",
    "        if row[\"Token\"].lower() in splits:\n",
    "            genre_tags.append(\"genre\")\n",
    "        elif row[\"Token\"].lower() in ['scifi', 'fi', 'sci']:\n",
    "            genre_tags.append(\"genre\")\n",
    "        elif row[\"Token\"].lower() in ['thrillers', 'documentaries', 'comedies', 'dramas', 'horrors', 'musicals']:\n",
    "            genre_tags.append(\"genre\")\n",
    "        elif spellcheck:\n",
    "            genre_tags.append(\"genre\")\n",
    "        #elif token_spellcheck in splits:\n",
    "        #    genre_tags.append(\"genre\")\n",
    "        else:\n",
    "            genre_tags.append(\"O\")\n",
    "        \n",
    "        # Assign genre sentiment tag (positive, negative, neutral, or other)\n",
    "        pos_splits = submissions.iloc[current_sentence][\"pos_genres\"].split(\"|\")\n",
    "        pos_splits = [p.lower() for p in pos_splits]\n",
    "        neg_splits = submissions.iloc[current_sentence][\"neg_genres\"].split(\"|\")\n",
    "        neg_splits = [n.lower() for n in neg_splits]\n",
    "        spellcheck_pos = False\n",
    "        spellcheck_neg = False\n",
    "        for t in token_spellcheck:\n",
    "            if t in pos_splits:\n",
    "                spellcheck_pos = True\n",
    "            elif t in neg_splits:\n",
    "                spellcheck_neg = True\n",
    "        pos_genre = False\n",
    "        neg_genre = False\n",
    "        for genre in ['documentaries', 'thrillers', 'comedies', 'dramas', 'horrors', 'musicals']:\n",
    "            if genre in pos_splits:\n",
    "                pos_genre = True\n",
    "            elif genre in neg_splits:\n",
    "                neg_genre = True\n",
    "        if row[\"Token\"].lower() in pos_splits:\n",
    "            sents_gen.append(\"pos\")\n",
    "        elif spellcheck_pos:\n",
    "            sents_gen.append(\"pos\")\n",
    "        elif pos_genre:\n",
    "            sents_gen.append(\"pos\")\n",
    "        elif row[\"Token\"].lower() in neg_splits:\n",
    "            sents_gen.append(\"neg\")\n",
    "        elif spellcheck_neg:\n",
    "            sents_gen.append(\"neg\")\n",
    "        elif neg_genre:\n",
    "            sents_gen.append(\"neg\")\n",
    "        else:\n",
    "            sents_gen.append(\"O\")\n",
    "        \n",
    "        \n",
    "        ### TAG ACTORS ###\n",
    "    \n",
    "        # Assign entity tag\n",
    "        splits = submissions.iloc[current_sentence][\"actor\"].split(\"|\")\n",
    "        tmp = []\n",
    "        for s in splits:\n",
    "            names = s.split(\" \")\n",
    "            for name in names:\n",
    "                tmp.append(name)\n",
    "        tmp = [t.lower() for t in tmp]\n",
    "        spellcheck = False\n",
    "        for t in token_spellcheck:\n",
    "            if t in tmp:\n",
    "                spellcheck = True\n",
    "        if row[\"Token\"].lower() in tmp:\n",
    "            actor_tags.append(\"actor\")\n",
    "        elif spellcheck:\n",
    "            actor_tags.append(\"actor\")\n",
    "        else:\n",
    "            actor_tags.append(\"O\")\n",
    "        \n",
    "        # Assign sentiment tag\n",
    "        pos_splits = submissions.iloc[current_sentence][\"pos_actor\"].split(\"|\")\n",
    "        neg_splits = submissions.iloc[current_sentence][\"neg_actor\"].split(\"|\")\n",
    "        tmp_pos = []\n",
    "        for p in pos_splits:\n",
    "            names = p.split(\" \")\n",
    "            for name in names:\n",
    "                tmp_pos.append(name)\n",
    "        tmp_pos = [t.lower() for t in tmp_pos]\n",
    "        tmp_neg = []\n",
    "        for n in neg_splits:\n",
    "            names = n.split(\" \")\n",
    "            for name in names:\n",
    "                tmp_neg.append(name)\n",
    "        tmp_neg = [t.lower() for t in tmp_neg]\n",
    "        spellcheck_pos = False\n",
    "        spellcheck_neg = False\n",
    "        for t in token_spellcheck:\n",
    "            if t in tmp_pos:\n",
    "                spellcheck_pos = True\n",
    "            elif t in tmp_neg:\n",
    "                spellcheck_neg = True\n",
    "        if row[\"Token\"].lower() in tmp_pos:\n",
    "            sents_actor.append(\"pos\")\n",
    "        elif spellcheck_pos:\n",
    "            sents_actor.append(\"pos\")\n",
    "        elif row[\"Token\"].lower() in tmp_neg:\n",
    "            sents_actor.append(\"neg\")    \n",
    "        elif spellcheck_neg:\n",
    "            sents_actor.append(\"neg\")\n",
    "        else:\n",
    "            sents_actor.append(\"O\")\n",
    "        \n",
    "        \n",
    "        ### TAG KEYWORDS ###\n",
    "    \n",
    "        # Assign entity tag\n",
    "        splits = submissions.iloc[current_sentence][\"keywords\"].split(\"|\")\n",
    "        splits = [s.lower() for s in splits]\n",
    "        spellcheck = False\n",
    "        for t in token_spellcheck:\n",
    "            if t in splits:\n",
    "                spellcheck = True\n",
    "        if row[\"Token\"].lower() in splits:\n",
    "            keyword_tags.append(\"keyword\")\n",
    "        elif spellcheck:\n",
    "            keyword_tags.append(\"keyword\")\n",
    "        else:\n",
    "            keyword_tags.append(\"O\")\n",
    "        \n",
    "        # Assign sentiment tag\n",
    "        pos_splits = submissions.iloc[current_sentence][\"pos_keywords\"].split(\"|\")\n",
    "        pos_splits = [p.lower() for p in pos_splits]\n",
    "        neg_splits = submissions.iloc[current_sentence][\"neg_keywords\"].split(\"|\")\n",
    "        spellcheck_pos = False\n",
    "        spellcheck_neg = False\n",
    "        for t in token_spellcheck:\n",
    "            if t in pos_splits:\n",
    "                spellcheck_pos = True\n",
    "            elif t in neg_splits:\n",
    "                spellcheck_neg = True\n",
    "        if row[\"Token\"].lower() in pos_splits:\n",
    "            sents_keyword.append(\"pos\")\n",
    "        elif spellcheck_pos:\n",
    "            sents_keyword.append(\"pos\")\n",
    "        elif row[\"Token\"].lower() in neg_splits:\n",
    "            sents_keyword.append(\"neg\")\n",
    "        elif spellcheck_neg:\n",
    "            sents_keyword.append(\"neg\")\n",
    "        else:\n",
    "            sents_keyword.append(\"O\")\n",
    "        \n",
    "        ### TAG MOVIES ###\n",
    "    \n",
    "        # Assign entity tag\n",
    "        splits = submissions.iloc[current_sentence][\"movies\"].split(\"|\")\n",
    "        alt_splits = []\n",
    "        for s in splits:\n",
    "            if s in alt_names.keys():\n",
    "                alt_name_tmp = alt_names[s]\n",
    "                for name in alt_name_tmp:\n",
    "                    tmp = name.split(\" \")\n",
    "                    for t in tmp:\n",
    "                        alt_splits.append(t)\n",
    "        tmp = []\n",
    "        for s in splits:\n",
    "            names = s.split(\" \")\n",
    "            for name in names:\n",
    "                tmp.append(name)\n",
    "        tmp = [t.lower() for t in tmp]\n",
    "        for alt in alt_splits:\n",
    "            tmp.append(alt.lower())\n",
    "        spellcheck = False\n",
    "        for t in token_spellcheck:\n",
    "            if t in tmp:\n",
    "                spellcheck = True\n",
    "        if row[\"Token\"].lower() in tmp:\n",
    "            movie_tags.append(\"movie\")\n",
    "        elif spellcheck:\n",
    "            movie_tags.append(\"movie\")\n",
    "        elif row[\"Token\"].lower() in ['lotr', 'hp']:\n",
    "            movie_tags.append(\"movie\")\n",
    "        else:\n",
    "            movie_tags.append(\"O\")\n",
    "        \n",
    "        # Assign sentiment tag\n",
    "        pos_splits = submissions.iloc[current_sentence][\"pos_movies\"].split(\"|\")\n",
    "        neg_splits = submissions.iloc[current_sentence][\"neg_movies\"].split(\"|\")\n",
    "        tmp_pos = []\n",
    "        for p in pos_splits:\n",
    "            names = p.split(\" \")\n",
    "            for name in names:\n",
    "                tmp_pos.append(name)\n",
    "        tmp_pos = [t.lower() for t in tmp_pos]\n",
    "        for alt in alt_splits:\n",
    "            tmp_pos.append(alt.lower())\n",
    "        tmp_neg = []\n",
    "        for n in neg_splits:\n",
    "            names = n.split(\" \")\n",
    "            for name in names:\n",
    "                tmp_neg.append(name)\n",
    "        tmp_neg = [t.lower() for t in tmp_neg]\n",
    "        for alt in alt_splits:\n",
    "            tmp_neg.append(alt.lower())\n",
    "        spellcheck_pos = False\n",
    "        spellcheck_neg = False\n",
    "        for t in token_spellcheck:\n",
    "            if t in tmp_pos:\n",
    "                spellcheck_pos = True\n",
    "            elif t in tmp_neg:\n",
    "                spellcheck_neg = True\n",
    "        if row[\"Token\"].lower() in tmp_pos:\n",
    "            sents_movies.append(\"pos\")\n",
    "        elif spellcheck_pos:\n",
    "            sents_movies.append(\"pos\")\n",
    "        elif row[\"Token\"].lower() in tmp_neg:\n",
    "            sents_movies.append(\"neg\")    \n",
    "        elif spellcheck_neg:\n",
    "            sents_movies.append(\"neg\")\n",
    "        else:\n",
    "            sents_movies.append(\"O\")\n",
    "        \n",
    "    data['gen_tag'] = pd.Series(genre_tags)\n",
    "    data['gen_sentiment'] = pd.Series(sents_gen)\n",
    "    data['actor_tag'] = pd.Series(actor_tags)\n",
    "    data['actor_sentiment'] = pd.Series(sents_actor)\n",
    "    data['keyword_tag'] = pd.Series(keyword_tags)\n",
    "    data['keyword_sentiment'] = pd.Series(sents_keyword) \n",
    "    data['movie_tag'] = pd.Series(movie_tags)\n",
    "    data['movie_sentiment'] = pd.Series(sents_movies)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Assuming each token in the data has a corresponding entity tag, we check whether it's an opening\n",
    "tag 'B' or inner tag 'I' of the entity. For every token we check it's tag, if it's for instance\n",
    "a movie tag which follows a movie tag, means it's an inner tag, if it follws a 'O' tag, means\n",
    "it's a 'B' tag. \n",
    "\n",
    "\"\"\"\n",
    "def tag_bio(data):\n",
    "    bio_gens = []\n",
    "    bio_actors = []\n",
    "    bio_keywords = []\n",
    "    bio_movies = []\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        if index == 0:\n",
    "            if row[\"gen_tag\"] == \"genre\":\n",
    "                bio_gens.append(\"B-genre\")\n",
    "            else:\n",
    "                bio_gens.append(\"O\")\n",
    "            if row[\"actor_tag\"] == \"actor\":\n",
    "                bio_actors.append(\"B-actor\")\n",
    "            else:\n",
    "                bio_actors.append(\"O\")\n",
    "            if row[\"keyword_tag\"] == \"keyword\":\n",
    "                bio_keywords.append(\"B-keyword\")\n",
    "            else:\n",
    "                bio_keywords.append(\"O\")\n",
    "            if row[\"movie_tag\"] == 'movie':\n",
    "                bio_movies.append(\"B-movie\")\n",
    "            else:\n",
    "                bio_movies.append(\"O\")\n",
    "            \n",
    "        elif row[\"Token_index\"] == 0:\n",
    "                if row[\"gen_tag\"] == \"genre\":\n",
    "                    bio_gens.append(\"B-genre\")\n",
    "                else:\n",
    "                    bio_gens.append(\"O\")\n",
    "                if row[\"actor_tag\"] == \"actor\":\n",
    "                    bio_actors.append(\"B-actor\")\n",
    "                else:\n",
    "                    bio_actors.append(\"O\")\n",
    "                if row[\"keyword_tag\"] == \"keyword\":\n",
    "                    bio_keywords.append(\"B-keyword\")\n",
    "                else:\n",
    "                    bio_keywords.append(\"O\")\n",
    "                if row[\"movie_tag\"] == 'movie':\n",
    "                    bio_movies.append(\"B-movie\")\n",
    "                else:\n",
    "                    bio_movies.append(\"O\")\n",
    "        else:\n",
    "            if data.iloc[index-1][\"gen_tag\"] == 'genre' and row[\"gen_tag\"] == 'genre':\n",
    "                bio_gens.append(\"I-genre\")\n",
    "            elif data.iloc[index-1][\"gen_tag\"] == 'O' and row[\"gen_tag\"] == 'genre':\n",
    "                bio_gens.append(\"B-genre\")\n",
    "            else:\n",
    "                bio_gens.append(\"O\")\n",
    "            \n",
    "            if data.iloc[index-1][\"actor_tag\"] == 'actor' and row[\"actor_tag\"] == 'actor':\n",
    "                bio_actors.append(\"I-actor\")\n",
    "            elif data.iloc[index-1][\"actor_tag\"] == 'O' and row[\"actor_tag\"] == 'actor':\n",
    "                bio_actors.append(\"B-actor\")\n",
    "            else:\n",
    "                bio_actors.append(\"O\")\n",
    "        \n",
    "            if data.iloc[index-1][\"keyword_tag\"] == 'keyword' and row[\"keyword_tag\"] == 'keyword':\n",
    "                bio_keywords.append(\"I-keyword\")\n",
    "            elif data.iloc[index-1][\"keyword_tag\"] == 'O' and row[\"keyword_tag\"] == 'keyword':\n",
    "                bio_keywords.append(\"B-keyword\")\n",
    "            else:\n",
    "                bio_keywords.append(\"O\")\n",
    "        \n",
    "            if data.iloc[index-1][\"movie_tag\"] == 'movie' and row[\"movie_tag\"] == 'movie':\n",
    "                bio_movies.append(\"I-movie\")\n",
    "            elif data.iloc[index-1][\"movie_tag\"] == 'O' and row[\"movie_tag\"] == 'movie':\n",
    "                bio_movies.append(\"B-movie\")\n",
    "            else:\n",
    "                bio_movies.append(\"O\")\n",
    "            \n",
    "    data['bio_genre'] = pd.Series(bio_gens)\n",
    "    data['bio_actor'] = pd.Series(bio_actors)\n",
    "    data['bio_keyword'] = pd.Series(bio_keywords)\n",
    "    data['bio_movie'] = pd.Series(bio_movies)   \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Adding additional tag columns for every entity by combining their sentiment and their BIO tag\n",
    "into one column tag per entity.\n",
    "\n",
    "\"\"\"\n",
    "def tag_bio_sentiment(data):\n",
    "    bio_gens_sent = []\n",
    "    bio_actors_sent = []\n",
    "    bio_keywords_sent = []\n",
    "    bio_movies_sent = []\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        if row[\"bio_genre\"] == \"B-genre\" and row[\"gen_sentiment\"] == \"pos\":\n",
    "            bio_gens_sent.append(\"B-gen-pos\")\n",
    "        elif row[\"bio_genre\"] == \"B-genre\" and row[\"gen_sentiment\"] == \"neg\":\n",
    "            bio_gens_sent.append(\"B-gen-neg\")\n",
    "        elif row[\"bio_genre\"] == \"I-genre\" and row[\"gen_sentiment\"] == \"pos\":\n",
    "            bio_gens_sent.append(\"I-gen-pos\")\n",
    "        elif row[\"bio_genre\"] == \"I-genre\" and row[\"gen_sentiment\"] == \"neg\":\n",
    "            bio_gens_sent.append(\"I-gen-neg\")\n",
    "        else:\n",
    "            bio_gens_sent.append(\"O\")\n",
    "    \n",
    "        if row[\"bio_actor\"] == \"B-actor\" and row[\"actor_sentiment\"] == \"pos\":\n",
    "            bio_actors_sent.append(\"B-actor-pos\")\n",
    "        elif row[\"bio_actor\"] == \"B-actor\" and row[\"actor_sentiment\"] == \"neg\":\n",
    "            bio_actors_sent.append(\"B-actor-neg\")\n",
    "        elif row[\"bio_actor\"] == \"I-actor\" and row[\"actor_sentiment\"] == \"neg\":\n",
    "            bio_actors_sent.append(\"I-actor-neg\")\n",
    "        elif row[\"bio_actor\"] == \"I-actor\" and row[\"actor_sentiment\"] == \"pos\":\n",
    "            bio_actors_sent.append(\"I-actor-pos\")\n",
    "        else:\n",
    "            bio_actors_sent.append(\"O\")\n",
    "    \n",
    "        if row[\"bio_keyword\"] == \"B-keyword\" and row[\"keyword_sentiment\"] == \"pos\":\n",
    "            bio_keywords_sent.append(\"B-keyword-pos\")\n",
    "        elif row[\"bio_keyword\"] == \"B-keyword\" and row[\"keyword_sentiment\"] == \"neg\":\n",
    "            bio_keywords_sent.append(\"B-keyword-neg\")\n",
    "        elif row[\"bio_keyword\"] == \"I-keyword\" and row[\"keyword_sentiment\"] == \"neg\":\n",
    "            bio_keywords_sent.append(\"I-keyword-neg\")\n",
    "        elif row[\"bio_keyword\"] == \"I-keyword\" and row[\"keyword_sentiment\"] == \"pos\":\n",
    "            bio_keywords_sent.append(\"I-keyword-pos\")\n",
    "        else:\n",
    "            bio_keywords_sent.append(\"O\")\n",
    "        \n",
    "        if row[\"bio_movie\"] == \"B-movie\" and row[\"movie_sentiment\"] == \"neg\":\n",
    "            bio_movies_sent.append(\"B-movie-neg\")\n",
    "        elif row[\"bio_movie\"] == \"B-movie\" and row[\"movie_sentiment\"] == \"pos\":\n",
    "            bio_movies_sent.append(\"B-movie-pos\")\n",
    "        elif row[\"bio_movie\"] == \"I-movie\" and row[\"movie_sentiment\"] == \"pos\":\n",
    "            bio_movies_sent.append(\"I-movie-pos\")\n",
    "        elif row[\"bio_movie\"] == \"I-movie\" and row[\"movie_sentiment\"] == \"neg\":\n",
    "            bio_movies_sent.append(\"I-movie-neg\")\n",
    "        else:\n",
    "            bio_movies_sent.append(\"O\")\n",
    "\n",
    "    data['bio-genre-sent'] = pd.Series(bio_gens_sent)\n",
    "    data['bio-actor-sent'] = pd.Series(bio_actors_sent)\n",
    "    data['bio-keywords-sent'] = pd.Series(bio_keywords_sent)\n",
    "    data['bio-movies-sent'] = pd.Series(bio_movies_sent)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Very often stopwords and various punctuations are part of a name of a movie. By mathing each token\n",
    "in a sentence against the identified entities, very often a regular stopword/punctuation or \n",
    "other regular word will get tagged. The following function checks for these mistakes and fixes \n",
    "them.\n",
    "\n",
    "\"\"\"\n",
    "def corrections(data):\n",
    "    for i in range(1, len(data[\"Sentence\"])-1):\n",
    "        if data.iloc[i]['Token'] in ['you', 'or', 'is', 'it', 'plot', 'movie', 'movies', 'the', 'of', 'I', 'a', '-', 'and', 'in', 'The', 'A', 'to', 'no', 'for', 'i', 'my']:\n",
    "            if data.iloc[i]['bio_movie'] != 'O' and data.iloc[i+1]['movie_tag'] == 'O':\n",
    "                data.at[i, 'movie_tag'] = 'O'\n",
    "                data.at[i, 'movie_sentiment'] = 'O'\n",
    "                data.at[i, 'bio_movie'] = 'O'\n",
    "                data.at[i, 'bio-movies-sent'] = 'O'\n",
    "            if data.iloc[i]['bio_keyword'] == 'B-keyword' and data.iloc[i-1]['keyword_tag'] == 'O' and data.iloc[i+1]['keyword_tag'] == 'O':\n",
    "                data.at[i, 'keyword_tag'] = 'O'\n",
    "                data.at[i, 'keyword_sentiment'] = 'O'\n",
    "                data.at[i, 'bio_keyword'] = 'O'\n",
    "                data.at[i, 'bio-keywords-sent'] = 'O'\n",
    "    for i in range(1, len(data[\"Sentence\"])-1):\n",
    "        if data.iloc[i]['Token'] == \"'s\":\n",
    "            if data.iloc[i]['bio_movie'] == 'O' and data.iloc[i-1]['movie_tag'] != 'O' and data.iloc[i+1]['movie_tag'] != 'O':\n",
    "                data.at[i, 'movie_tag'] = 'movie'\n",
    "                data.at[i, 'movie_sentiment'] = data.iloc[i-1]['movie_sentiment']\n",
    "                data.at[i, 'bio_movie'] = 'I-movie'\n",
    "                if data.iloc[i-1]['movie_sentiment'] == 'pos':\n",
    "                    data.at[i, 'bio-movies-sent'] = 'I-movie-pos'\n",
    "                    data.at[i+1, 'bio-movies-sent'] = 'I-movie-pos'\n",
    "                elif data.iloc[i-1]['movie_sentiment'] == 'neg':\n",
    "                    data.at[i, 'bio-movies-sent'] = 'I-movie-neg'\n",
    "                    data.at[i+1, 'bio-movies-sent'] = 'I-movie-neg'\n",
    "                data.at[i+1, 'bio_movie'] = 'I-movie'\n",
    "        elif data.iloc[i]['Token'] == '-':\n",
    "            if data.iloc[i]['bio_movie'] != 'O' and data.iloc[i-1]['movie_tag'] == 'O':\n",
    "                data.at[i, 'movie_tag'] = 'O'\n",
    "                data.at[i, 'movie_sentiment'] = 'O'\n",
    "                data.at[i, 'bio_movie'] = 'O'\n",
    "                data.at[i, 'bio-movies-sent'] = 'O'\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "We merge separate columns per entities into one column where one token can be a:\n",
    "'B-movie-pos'\n",
    "'I-movie-pos'\n",
    "'B-movie-neg'\n",
    "'I-movie-neg'\n",
    "...\n",
    "'B-actor-pos'\n",
    "...\n",
    "'O'\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def merge(data):\n",
    "    entity_tag = []\n",
    "    sentiment_tag = []\n",
    "    bio_tag = []\n",
    "    bio_tag_sentiment = []\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        if row[\"gen_tag\"] != 'O':\n",
    "            entity_tag.append(\"entity\")\n",
    "        elif row[\"actor_tag\"] != 'O':\n",
    "            entity_tag.append(\"entity\")\n",
    "        elif row[\"keyword_tag\"] != 'O':\n",
    "            entity_tag.append(\"entity\")\n",
    "        elif row[\"movie_tag\"] != 'O':\n",
    "            entity_tag.append('entity')\n",
    "        else:\n",
    "            entity_tag.append('O')\n",
    "    \n",
    "        if row[\"gen_sentiment\"] == 'pos':\n",
    "            sentiment_tag.append('pos')\n",
    "        elif row[\"gen_sentiment\"] == 'neg':\n",
    "            sentiment_tag.append('neg')\n",
    "        elif row[\"actor_sentiment\"] == 'pos':\n",
    "            sentiment_tag.append('pos')\n",
    "        elif row[\"actor_sentiment\"] == 'neg':\n",
    "            sentiment_tag.append('neg')\n",
    "        elif row[\"keyword_sentiment\"] == 'pos':\n",
    "            sentiment_tag.append('pos')\n",
    "        elif row[\"keyword_sentiment\"] == 'neg':\n",
    "            sentiment_tag.append('neg')\n",
    "        elif row[\"movie_sentiment\"] == 'pos':\n",
    "            sentiment_tag.append('pos')\n",
    "        elif row[\"movie_sentiment\"] == 'neg':\n",
    "            sentiment_tag.append('neg')\n",
    "        else:\n",
    "            sentiment_tag.append('O')\n",
    "    \n",
    "        if row[\"bio_genre\"] == 'B-genre':\n",
    "            bio_tag.append('B-genre')\n",
    "        elif row[\"bio_genre\"] == 'I-genre':\n",
    "            bio_tag.append('I-genre')\n",
    "        elif row[\"bio_actor\"] == 'B-actor':\n",
    "            bio_tag.append('B-actor')\n",
    "        elif row[\"bio_actor\"] == 'I-actor':\n",
    "            bio_tag.append('I-actor')\n",
    "        elif row[\"bio_keyword\"] == 'B-keyword':\n",
    "            bio_tag.append('B-keyword')\n",
    "        elif row[\"bio_keyword\"] == 'I-keyword':\n",
    "            bio_tag.append('I-keyword')\n",
    "        elif row[\"bio_movie\"] == 'B-movie':\n",
    "            bio_tag.append('B-movie')\n",
    "        elif row[\"bio_movie\"] == 'I-movie':\n",
    "            bio_tag.append('I-movie')\n",
    "        else:\n",
    "            bio_tag.append('O')\n",
    "    \n",
    "        if row[\"bio-genre-sent\"] == 'B-gen-pos':\n",
    "            bio_tag_sentiment.append('B-gen-pos')\n",
    "        elif row[\"bio-genre-sent\"] == 'I-gen-pos':\n",
    "            bio_tag_sentiment.append('I-gen-pos')\n",
    "        elif row[\"bio-genre-sent\"] == 'B-gen-neg':\n",
    "            bio_tag_sentiment.append('B-gen-neg')\n",
    "        elif row[\"bio-genre-sent\"] == 'I-gen-neg':\n",
    "            bio_tag_sentiment.append('I-gen-neg')\n",
    "        elif row[\"bio-actor-sent\"] == 'B-actor-pos':\n",
    "            bio_tag_sentiment.append('B-actor-pos')\n",
    "        elif row[\"bio-actor-sent\"] == 'I-actor-pos':\n",
    "            bio_tag_sentiment.append('I-actor-pos')\n",
    "        elif row[\"bio-actor-sent\"] == 'B-actor-neg':\n",
    "            bio_tag_sentiment.append('B-actor-neg')\n",
    "        elif row[\"bio-actor-sent\"] == 'I-actor-neg':\n",
    "            bio_tag_sentiment.append('I-actor-neg')\n",
    "        elif row[\"bio-keywords-sent\"] == 'B-keyword-pos':\n",
    "            bio_tag_sentiment.append('B-keyword-pos')\n",
    "        elif row[\"bio-keywords-sent\"] == 'I-keyword-pos':\n",
    "            bio_tag_sentiment.append('I-keyword-pos')\n",
    "        elif row[\"bio-keywords-sent\"] == 'B-keyword-neg':\n",
    "            bio_tag_sentiment.append('B-keyword-neg')\n",
    "        elif row[\"bio-keywords-sent\"] == 'I-keyword-neg':\n",
    "            bio_tag_sentiment.append('I-keyword-neg')\n",
    "        elif row[\"bio-movies-sent\"] == 'B-movie-pos':\n",
    "            bio_tag_sentiment.append('B-movie-pos')\n",
    "        elif row[\"bio-movies-sent\"] == 'I-movie-pos':\n",
    "            bio_tag_sentiment.append('I-movie-pos')\n",
    "        elif row[\"bio-movies-sent\"] == 'B-movie-neg':\n",
    "            bio_tag_sentiment.append('B-movie-neg')\n",
    "        elif row[\"bio-movies-sent\"] == 'I-movie-neg':\n",
    "            bio_tag_sentiment.append('I-movie-neg')\n",
    "        else:\n",
    "            bio_tag_sentiment.append('O')\n",
    "        \n",
    "    data[\"entity_tag\"] = pd.Series(entity_tag)\n",
    "    data[\"sentiment\"] = pd.Series(sentiment_tag)\n",
    "    data[\"bio_indic\"] = pd.Series(bio_tag)\n",
    "    data[\"BIO_sent\"] = pd.Series(bio_tag_sentiment)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "input_dir = 'D:/TU_Graz/Thesis/Datasets/Reddit_features/'\n",
    "output_dir = input_dir\n",
    "\n",
    "# 'data' contains the tokenized submissions along with their features, so that we can add the \n",
    "# tags to them to complete the table. In order to tag every token we match them in the columns\n",
    "# from 'submissions' file where the user identified an entity\n",
    "data, submissions, movies_matched = read_data()\n",
    "\n",
    "# Create alternative movie names dict\n",
    "alt_names = create_alt_movie_dict(movies_matched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create first round of tags for each entity. Per column we have 'entity'-'other'\n",
    "data = tag_entity(data, submissions, alt_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sent_id</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Token_index</th>\n",
       "      <th>Token</th>\n",
       "      <th>POS_tag</th>\n",
       "      <th>POS_universal</th>\n",
       "      <th>NER_tag</th>\n",
       "      <th>NER_iob</th>\n",
       "      <th>lemma</th>\n",
       "      <th>norm</th>\n",
       "      <th>...</th>\n",
       "      <th>norm_freq</th>\n",
       "      <th>abs_freq</th>\n",
       "      <th>gen_tag</th>\n",
       "      <th>gen_sentiment</th>\n",
       "      <th>actor_tag</th>\n",
       "      <th>actor_sentiment</th>\n",
       "      <th>keyword_tag</th>\n",
       "      <th>keyword_sentiment</th>\n",
       "      <th>movie_tag</th>\n",
       "      <th>movie_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25965</th>\n",
       "      <td>5v38hx</td>\n",
       "      <td>Sentence 294</td>\n",
       "      <td>25</td>\n",
       "      <td>Prisoners</td>\n",
       "      <td>1.579455e+19</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.345215e+19</td>\n",
       "      <td>4.935277e+18</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>4</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25966</th>\n",
       "      <td>5v38hx</td>\n",
       "      <td>Sentence 294</td>\n",
       "      <td>26</td>\n",
       "      <td>,</td>\n",
       "      <td>2.593209e+18</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.593209e+18</td>\n",
       "      <td>2.593209e+18</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041163</td>\n",
       "      <td>1069</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25967</th>\n",
       "      <td>5v38hx</td>\n",
       "      <td>Sentence 294</td>\n",
       "      <td>27</td>\n",
       "      <td>Sicario</td>\n",
       "      <td>1.579455e+19</td>\n",
       "      <td>96</td>\n",
       "      <td>380</td>\n",
       "      <td>3</td>\n",
       "      <td>1.741044e+19</td>\n",
       "      <td>5.471049e+18</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>2</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25968</th>\n",
       "      <td>5v38hx</td>\n",
       "      <td>Sentence 294</td>\n",
       "      <td>28</td>\n",
       "      <td>etc</td>\n",
       "      <td>1.540191e+19</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.775260e+18</td>\n",
       "      <td>5.775260e+18</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001810</td>\n",
       "      <td>47</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25969</th>\n",
       "      <td>5v38hx</td>\n",
       "      <td>Sentence 294</td>\n",
       "      <td>29</td>\n",
       "      <td>.</td>\n",
       "      <td>1.264607e+19</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.264607e+19</td>\n",
       "      <td>1.264607e+19</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031613</td>\n",
       "      <td>821</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 158 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sent_id      Sentence  Token_index      Token       POS_tag  \\\n",
       "25965  5v38hx  Sentence 294           25  Prisoners  1.579455e+19   \n",
       "25966  5v38hx  Sentence 294           26          ,  2.593209e+18   \n",
       "25967  5v38hx  Sentence 294           27    Sicario  1.579455e+19   \n",
       "25968  5v38hx  Sentence 294           28        etc  1.540191e+19   \n",
       "25969  5v38hx  Sentence 294           29          .  1.264607e+19   \n",
       "\n",
       "       POS_universal  NER_tag  NER_iob         lemma          norm  ...  \\\n",
       "25965             96        0        2  1.345215e+19  4.935277e+18  ...   \n",
       "25966             97        0        2  2.593209e+18  2.593209e+18  ...   \n",
       "25967             96      380        3  1.741044e+19  5.471049e+18  ...   \n",
       "25968            101        0        2  5.775260e+18  5.775260e+18  ...   \n",
       "25969             97        0        2  1.264607e+19  1.264607e+19  ...   \n",
       "\n",
       "       norm_freq  abs_freq  gen_tag  gen_sentiment  actor_tag  \\\n",
       "25965   0.000154         4        O              O          O   \n",
       "25966   0.041163      1069        O              O          O   \n",
       "25967   0.000077         2        O              O          O   \n",
       "25968   0.001810        47        O              O          O   \n",
       "25969   0.031613       821        O              O          O   \n",
       "\n",
       "       actor_sentiment  keyword_tag  keyword_sentiment  movie_tag  \\\n",
       "25965                O            O                  O          O   \n",
       "25966                O            O                  O          O   \n",
       "25967                O            O                  O          O   \n",
       "25968                O            O                  O          O   \n",
       "25969                O            O                  O          O   \n",
       "\n",
       "       movie_sentiment  \n",
       "25965                O  \n",
       "25966                O  \n",
       "25967                O  \n",
       "25968                O  \n",
       "25969                O  \n",
       "\n",
       "[5 rows x 158 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create second round of tags for each entity\n",
    "data = tag_bio(data)\n",
    "\n",
    "# Extend the BIO-tags with sentiment tags\n",
    "data = tag_bio_sentiment(data)\n",
    "\n",
    "# Fix the most common tags that were likely tagged wrong\n",
    "data = corrections(data)\n",
    "\n",
    "# Mix the separate entity columns into columns that contain all tags\n",
    "data = merge(data)\n",
    "\n",
    "# And save them\n",
    "data.to_csv(\"tmp_test.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
