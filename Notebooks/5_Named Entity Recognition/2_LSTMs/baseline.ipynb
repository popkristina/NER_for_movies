{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    os.chdir('D:/TU_Graz/Thesis/Datasets/Reddit_features')\n",
    "    train = pd.read_csv(\"train_final_all.csv\")\n",
    "    test = pd.read_csv(\"test_final_all.csv\")\n",
    "    data = train.append(test)\n",
    "    \n",
    "    return train, test, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lists(data, category):\n",
    "    words = list(set(data[\"Token\"].values))\n",
    "    words.append(\"ENDPAD\")\n",
    "    n_words = len(words)\n",
    "    tags = list(set(data[category].values))\n",
    "    n_tags = len(tags)\n",
    "    \n",
    "    return words, n_words, tags, n_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_sentences(data, category):\n",
    "    all_sents = []\n",
    "    sent_ids = data['Sent_id'].unique()\n",
    "    for curr_id in sent_ids:\n",
    "        tmp_df = data[data['Sent_id'] == curr_id]\n",
    "        tmp_df = pd.concat([tmp_df['Token'], tmp_df[\"Token_index\"], tmp_df.iloc[:,4:149], tmp_df[category]], axis = 1)\n",
    "        records = tmp_df.to_records(index=False)\n",
    "        all_sents.append(records)\n",
    "    return all_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_sents_over_threshold(sents, threshold):\n",
    "    sentences = list()\n",
    "    for s in sents:\n",
    "        if len(s) < threshold:\n",
    "            sentences.append(s)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_and_pad(sentences, max_len, word2idx, tag2idx):\n",
    "    X = [[word2idx[w[0]] for w in s] for s in sentences]\n",
    "    X = pad_sequences(maxlen = max_len, sequences = X, padding = \"post\", value = n_words - 1)\n",
    "\n",
    "    y = [[tag2idx[w[len(w)-1]] for w in s] for s in sentences]\n",
    "    y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tag2idx[\"O\"])\n",
    "    y = [to_categorical(i, num_classes=n_tags) for i in y]\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(max_len, n_words, n_tags):\n",
    "    input = Input(shape=(max_len,))\n",
    "    model = Embedding(input_dim=n_words, output_dim=max_len, input_length=max_len)(input)\n",
    "    model = Dropout(0.1)(model)\n",
    "    model = Bidirectional(LSTM(units=150, return_sequences=True, recurrent_dropout=0.3))(model)  # variational biLSTM\n",
    "    out = Dense(n_tags, activation=\"softmax\")(model) \n",
    "    model = Model(input, out)\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(hist, curve1, curve2):\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.plot(hist[curve1])\n",
    "    plt.plot(hist[curve2])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_test_data(model, X_test, y_test):\n",
    "    p = model.predict(np.array(X_test))\n",
    "    p = np.argmax(p, axis=-1)\n",
    "    y_test = np.array(y_test)\n",
    "    y_test = np.argmax(y_test, axis=-1)\n",
    "\n",
    "    y_orig = []\n",
    "    for sent in y_test:\n",
    "        for tag in sent:\n",
    "            y_orig.append(tag)\n",
    "        \n",
    "    y_preds = []\n",
    "    for sent in p:\n",
    "        for tag in sent:\n",
    "            y_preds.append(tag)    \n",
    "\n",
    "    report = classification_report(y_orig, y_preds)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the data...\n",
      "Creating sets of words and tags...\n",
      "Creating sentence list...\n",
      "Removing submissions longer than threshold...\n",
      "Creating word and tag maps...\n",
      "Padding data...\n",
      "Building the model...\n",
      "{}\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 300)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 300, 300)          2922900   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 300, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 300, 300)          541200    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300, 17)           5117      \n",
      "=================================================================\n",
      "Total params: 3,469,217\n",
      "Trainable params: 3,469,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Fitting the model....\n",
      "Epoch 1/15\n",
      "28/28 - 74s - loss: 0.3819 - accuracy: 0.9226 - val_loss: 0.2797 - val_accuracy: 0.9551\n",
      "Epoch 2/15\n",
      "28/28 - 69s - loss: 0.1668 - accuracy: 0.9568 - val_loss: 0.1589 - val_accuracy: 0.9556\n",
      "Epoch 3/15\n",
      "28/28 - 68s - loss: 0.1306 - accuracy: 0.9630 - val_loss: 0.1266 - val_accuracy: 0.9651\n",
      "Epoch 4/15\n",
      "28/28 - 69s - loss: 0.0972 - accuracy: 0.9721 - val_loss: 0.1078 - val_accuracy: 0.9703\n",
      "Epoch 5/15\n",
      "28/28 - 65s - loss: 0.0767 - accuracy: 0.9782 - val_loss: 0.1055 - val_accuracy: 0.9715\n",
      "Epoch 6/15\n",
      "28/28 - 70s - loss: 0.0634 - accuracy: 0.9821 - val_loss: 0.0931 - val_accuracy: 0.9752\n",
      "Epoch 7/15\n",
      "28/28 - 69s - loss: 0.0551 - accuracy: 0.9844 - val_loss: 0.1010 - val_accuracy: 0.9756\n",
      "Epoch 8/15\n",
      "28/28 - 72s - loss: 0.0480 - accuracy: 0.9862 - val_loss: 0.1020 - val_accuracy: 0.9755\n",
      "Epoch 9/15\n",
      "28/28 - 67s - loss: 0.0432 - accuracy: 0.9874 - val_loss: 0.0943 - val_accuracy: 0.9762\n",
      "Epoch 10/15\n",
      "28/28 - 68s - loss: 0.0391 - accuracy: 0.9887 - val_loss: 0.0916 - val_accuracy: 0.9754\n",
      "Epoch 11/15\n",
      "28/28 - 68s - loss: 0.0356 - accuracy: 0.9897 - val_loss: 0.0930 - val_accuracy: 0.9771\n",
      "Epoch 12/15\n",
      "28/28 - 70s - loss: 0.0318 - accuracy: 0.9907 - val_loss: 0.1153 - val_accuracy: 0.9753\n",
      "Epoch 13/15\n",
      "28/28 - 64s - loss: 0.0293 - accuracy: 0.9915 - val_loss: 0.0998 - val_accuracy: 0.9775\n",
      "Epoch 14/15\n",
      "28/28 - 68s - loss: 0.0263 - accuracy: 0.9920 - val_loss: 0.0967 - val_accuracy: 0.9764\n",
      "Epoch 15/15\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading the data...\")\n",
    "train, test, data = read_data()\n",
    "\n",
    "print(\"Creating sets of words and tags...\")\n",
    "words, n_words, tags, n_tags = create_lists(data, \"BIO\")\n",
    "\n",
    "print(\"Creating sentence list...\")\n",
    "sents = group_sentences(data, 'BIO')\n",
    "\n",
    "print(\"Removing submissions longer than threshold...\")\n",
    "sentences = remove_sents_over_threshold(sents, 300)\n",
    "\n",
    "print(\"Creating word and tag maps...\")\n",
    "max_len = 300\n",
    "word2idx = {w: i for i, w in enumerate(words)}\n",
    "tag2idx = {t: i for i, t in enumerate(tags)}\n",
    "\n",
    "print(\"Padding data...\")\n",
    "X, y = prepare_and_pad(sentences, max_len, word2idx, tag2idx)\n",
    "\n",
    "# Train and test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, shuffle=False)\n",
    "#X_tr = X[0:1185], y_tr = y[0:1185], X_te = X[1186:], y_te = y[1186:]\n",
    "\n",
    "print(\"Building the model...\")\n",
    "model = build_model(max_len, n_words, n_tags)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "print(\"Fitting the model....\")\n",
    "history = model.fit(X_train, np.array(y_train), batch_size=32, epochs=15, validation_split=0.2, verbose=2)\n",
    "hist = pd.DataFrame(history.history)\n",
    "\n",
    "print(\"Plotting learning curves...\")\n",
    "plot_learning_curves(hist, \"accuracy\", \"val_accuracy\")\n",
    "plot_learning_curves(hist, \"loss\", \"val_loss\")\n",
    "\n",
    "print(\"Test the model...\")\n",
    "evaluate_test_data(model, X_test, y_test) \n",
    "\n",
    "print(\"Output predictions for a random example...\")\n",
    "i = 0\n",
    "p = model.predict(np.array([X_test[i]]))\n",
    "p = np.argmax(p, axis=-1)\n",
    "print(\"{:15} ({:5}): {}\".format(\"Word\", \"True\", \"Pred\"))\n",
    "for w, pred in zip(X_test[i], p[0]):\n",
    "    if(words[w] != 'ENDPAD'):\n",
    "        print(\"{:15}: {}\".format(words[w], tags[pred]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
