{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T14:03:47.435027Z",
     "iopub.status.busy": "2022-10-10T14:03:47.434792Z",
     "iopub.status.idle": "2022-10-10T14:03:55.178657Z",
     "shell.execute_reply": "2022-10-10T14:03:55.177788Z",
     "shell.execute_reply.started": "2022-10-10T14:03:47.434997Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import string\n",
    "import json\n",
    "import os\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from torch import cuda, tensor, no_grad\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import XLMRobertaForTokenClassification, XLMRobertaTokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from seqeval.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T14:04:54.218719Z",
     "iopub.status.busy": "2022-10-10T14:04:54.218388Z",
     "iopub.status.idle": "2022-10-10T14:04:54.234577Z",
     "shell.execute_reply": "2022-10-10T14:04:54.233815Z",
     "shell.execute_reply.started": "2022-10-10T14:04:54.218669Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    #os.chdir('D:/TU_Graz/Thesis/Datasets/Reddit_features')\n",
    "    train = pd.read_csv(\"../input/bio-tagged/train_final_all.csv\")\n",
    "    test = pd.read_csv(\"../input/bio-tagged/test_final_all.csv\")\n",
    "    data = train.append(test)\n",
    "\n",
    "    return train, test, data\n",
    "\n",
    "def group_sentences(data, category):\n",
    "    all_sents = []\n",
    "    sent_ids = data['Sent_id'].unique()\n",
    "    for curr_id in sent_ids:\n",
    "        tmp_df = data[data['Sent_id'] == curr_id]\n",
    "        tmp_df = pd.concat([tmp_df['Sent_id'], tmp_df['Token'], tmp_df[\"Token_index\"], tmp_df.iloc[:,4:147], tmp_df[category]], axis = 1)\n",
    "        records = tmp_df.to_records(index=False)\n",
    "        all_sents.append(records)\n",
    "    return all_sents\n",
    "\n",
    "def remove_sents_over_threshold(sents, threshold):\n",
    "    sentences = list()\n",
    "    for s in sents:\n",
    "        if len(s) < threshold:\n",
    "            sentences.append(s)\n",
    "    return sentences\n",
    "\n",
    "def set_processor_params():\n",
    "    device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "    n_gpu = cuda.device_count()\n",
    "    cuda.get_device_name(0)\n",
    "    return device, n_gpu\n",
    "\n",
    "def tokenize(sentence, sentence_labels):\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "    for word, label in zip(sentence, sentence_labels):\n",
    "        str_word = str(word)\n",
    "        tokenized_word = tokenizer.tokenize(str_word) # Tokenize the word\n",
    "        n_subwords = len(tokenized_word) # Count subwords\n",
    "        tokenized_sentence.extend(tokenized_word) # Add to the final tokenized list\n",
    "        labels.extend([label] * n_subwords) # Add the same label of the original word to all of its subwords\n",
    "    return tokenized_sentence, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T14:05:03.889134Z",
     "iopub.status.busy": "2022-10-10T14:05:03.888755Z",
     "iopub.status.idle": "2022-10-10T14:05:39.082519Z",
     "shell.execute_reply": "2022-10-10T14:05:39.081787Z",
     "shell.execute_reply.started": "2022-10-10T14:05:03.889086Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare sent\n",
      "1185\n",
      "1185\n",
      "tokenize\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81058e3305234db28c9c509138cb6be3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed26d825474d4376871ac280ccb575fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad\n"
     ]
    }
   ],
   "source": [
    "train, test, data = read_data()\n",
    "device, n_gpu = set_processor_params()\n",
    "\n",
    "tag_values = list(set(train[\"BIO\"].values))\n",
    "tag_values.append(\"PAD\")\n",
    "tag2idx = {t: i for i, t in enumerate(tag_values)}\n",
    "idx2tag = {v: k for k, v in tag2idx.items()}\n",
    "\n",
    "print(\"prepare sent\")\n",
    "sents = group_sentences(train, 'BIO')\n",
    "for i in range(0, len(sents)):\n",
    "    sents[i] = sents[i][0:300]\n",
    "sentences = [[word[1] for word in sentence] for sentence in sents]\n",
    "labels = [[tag2idx[w[len(w)-1]] for w in s] for s in sents]\n",
    "\n",
    "MAX_LEN = 350\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 3e-5\n",
    "MAX_GRAD_NORM = 1.0\n",
    "\n",
    "print(\"tokenize\")\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-large\", do_lower_case=False)\n",
    "tokenized_texts_and_labels = [tokenize(sentence, sentence_labels) for sentence, sentence_labels in zip(sentences, labels)]\n",
    "\n",
    "tokenized_texts = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels]\n",
    "labels_subwords = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels]\n",
    "\n",
    "print(\"pad\")\n",
    "# Cut the token and label sequences to the max length\n",
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts], maxlen = MAX_LEN, dtype=\"long\", value=0.0, \n",
    "                          truncating=\"post\", padding=\"post\")\n",
    "input_tags = pad_sequences([[l for l in lab] for lab in labels_subwords], maxlen = MAX_LEN, value = tag2idx[\"PAD\"], \n",
    "                           padding=\"post\", dtype=\"long\", truncating=\"post\")\n",
    "attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]\n",
    "\n",
    "tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, input_tags, test_size=0.1, random_state=2021)\n",
    "tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids, test_size=0.1, random_state=2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T14:06:01.601290Z",
     "iopub.status.busy": "2022-10-10T14:06:01.600996Z",
     "iopub.status.idle": "2022-10-10T14:07:09.638974Z",
     "shell.execute_reply": "2022-10-10T14:07:09.638192Z",
     "shell.execute_reply.started": "2022-10-10T14:06:01.601257Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95bf394ecc6048b8983d816176bdd6c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/616 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7a663a354614244b77bf7eb3f69dbbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-large and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tr_inputs = tensor(tr_inputs)\n",
    "val_inputs = tensor(val_inputs)\n",
    "tr_tags = tensor(tr_tags)\n",
    "val_tags = tensor(val_tags)\n",
    "tr_masks = tensor(tr_masks)\n",
    "val_masks = tensor(val_masks)\n",
    "\n",
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Pretrained model params\n",
    "model = XLMRobertaForTokenClassification.from_pretrained(\"xlm-roberta-large\", num_labels = len(tag2idx), output_attentions = False, output_hidden_states=False)\n",
    "model.cuda(); # Pass the model parameters to gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T14:09:18.777818Z",
     "iopub.status.busy": "2022-10-10T14:09:18.777526Z",
     "iopub.status.idle": "2022-10-10T14:09:18.786983Z",
     "shell.execute_reply": "2022-10-10T14:09:18.786262Z",
     "shell.execute_reply.started": "2022-10-10T14:09:18.777786Z"
    }
   },
   "outputs": [],
   "source": [
    "FULL_FINETUNING = True\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters())\n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "    \n",
    "optimizer = AdamW(\n",
    "    optimizer_grouped_parameters,\n",
    "    lr=LEARNING_RATE,\n",
    "    eps=1e-8\n",
    ")\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * EPOCHS\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T14:09:23.458390Z",
     "iopub.status.busy": "2022-10-10T14:09:23.458102Z",
     "iopub.status.idle": "2022-10-10T15:02:04.572967Z",
     "shell.execute_reply": "2022-10-10T15:02:04.572249Z",
     "shell.execute_reply.started": "2022-10-10T14:09:23.458357Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.3614864827653442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   5%|▌         | 1/20 [02:39<50:38, 159.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.23296877443790437\n",
      "Validation Accuracy: 0.934170104450101\n",
      "Average train loss: 0.2021774271649591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  10%|█         | 2/20 [05:18<47:48, 159.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.2215116299688816\n",
      "Validation Accuracy: 0.9383832177652944\n",
      "Average train loss: 0.15368054970429185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  15%|█▌        | 3/20 [07:57<45:05, 159.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.19733600007990995\n",
      "Validation Accuracy: 0.9466338980075485\n",
      "Average train loss: 0.11464848398311196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  20%|██        | 4/20 [10:36<42:25, 159.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.20815670775870482\n",
      "Validation Accuracy: 0.9466338980075485\n",
      "Average train loss: 0.08217178747905524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  25%|██▌       | 5/20 [13:15<39:45, 159.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.24258441006143888\n",
      "Validation Accuracy: 0.9447028877380848\n",
      "Average train loss: 0.06148220700201359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  30%|███       | 6/20 [15:54<37:05, 158.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.2338101178407669\n",
      "Validation Accuracy: 0.9487404546651452\n",
      "Average train loss: 0.04652103779694477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  35%|███▌      | 7/20 [18:33<34:25, 158.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.29998933201034866\n",
      "Validation Accuracy: 0.9487404546651452\n",
      "Average train loss: 0.03709957897593354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  40%|████      | 8/20 [21:11<31:45, 158.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.27452956984440485\n",
      "Validation Accuracy: 0.9468094443956816\n",
      "Average train loss: 0.025844199915393867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  45%|████▌     | 9/20 [23:50<29:06, 158.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.30547475231190524\n",
      "Validation Accuracy: 0.9494426402176775\n",
      "Average train loss: 0.018392425092601666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|█████     | 10/20 [26:29<26:26, 158.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.3243030616392692\n",
      "Validation Accuracy: 0.9484771350829456\n",
      "Average train loss: 0.015611621671609585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  55%|█████▌    | 11/20 [29:07<23:46, 158.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.32566913465658825\n",
      "Validation Accuracy: 0.9488282278592118\n",
      "Average train loss: 0.012784849170400073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  60%|██████    | 12/20 [31:45<21:06, 158.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.3572376176714897\n",
      "Validation Accuracy: 0.9485649082770122\n",
      "Average train loss: 0.009104769756659527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  65%|██████▌   | 13/20 [34:22<18:26, 158.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.3553126295407613\n",
      "Validation Accuracy: 0.9508470113227421\n",
      "Average train loss: 0.00750851441770284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  70%|███████   | 14/20 [37:00<15:47, 157.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.3646807567526897\n",
      "Validation Accuracy: 0.9494426402176775\n",
      "Average train loss: 0.005239606875251963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  75%|███████▌  | 15/20 [39:37<13:08, 157.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.39547887903948625\n",
      "Validation Accuracy: 0.9497937329939437\n",
      "Average train loss: 0.004076344563377171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  80%|████████  | 16/20 [42:14<10:29, 157.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.39191478515664735\n",
      "Validation Accuracy: 0.9510225577108751\n",
      "Average train loss: 0.003095412393554994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  85%|████████▌ | 17/20 [44:51<07:51, 157.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.4033694473405679\n",
      "Validation Accuracy: 0.9505836917405425\n",
      "Average train loss: 0.0028177620559917283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  90%|█████████ | 18/20 [47:27<05:14, 157.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.4054140463471413\n",
      "Validation Accuracy: 0.9504959185464759\n",
      "Average train loss: 0.0025637121457787607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  95%|█████████▌| 19/20 [50:04<02:36, 156.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.4106761823097865\n",
      "Validation Accuracy: 0.9504959185464759\n",
      "Average train loss: 0.0018704419553784888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 20/20 [52:41<00:00, 158.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.4191238063077132\n",
      "Validation Accuracy: 0.9507592381286755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss_values, validation_loss_values = [], []\n",
    "accuracy_values, validation_accuracy_values = [], []\n",
    "\n",
    "for i in trange(EPOCHS, desc=\"Epoch\"):\n",
    "    \n",
    "    # TRAINING\n",
    "    # Perform one full pass over the training set\n",
    "    model.train() # Put the model into training mode\n",
    "    total_loss, total_accuracy = 0, 0 # Reset the total loss and acc. for current epoch\n",
    "    \n",
    "    # Training loop\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch) # add batch to gpu\n",
    "        b_input_ids, b_input_mask, b_labels = batch # Input ids, mask and labels of the current batch\n",
    "        model.zero_grad() # Always clear any previously calculated gradients before performing a backward pass\n",
    "        #cuda.empty_cache() \n",
    "        # Forward pass\n",
    "        # This will return the loss (rather than the model output) because we have provided the `labels`.\n",
    "        outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs[0]\n",
    "        \n",
    "        # Perform a backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "        total_loss += loss.item() # track train loss\n",
    "        \n",
    "        # Clip the norm of the gradient to help prevent the exploding gradients problem\n",
    "        from torch.nn.utils import clip_grad_norm_\n",
    "        clip_grad_norm_(parameters=model.parameters(), max_norm=MAX_GRAD_NORM)\n",
    "        \n",
    "        optimizer.step() # update parameters\n",
    "        scheduler.step() # Update the learning rate\n",
    "        \n",
    "    avg_train_loss = total_loss / len(train_dataloader) # Calc. avg loss over training data\n",
    "    print(\"Average train loss: {}\".format(avg_train_loss))\n",
    "    loss_values.append(avg_train_loss) # Store the loss value for plotting the learning curve\n",
    "\n",
    "    # VALIDATION\n",
    "    # After the completion of each training epoch, measure performance on validation set  \n",
    "    model.eval() # Put the model into evaluation mode\n",
    "    eval_loss, eval_accuracy = 0, 0 # Reset the validation loss for current epoch\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    predictions , true_labels = [], []\n",
    "    \n",
    "    # Validation loop\n",
    "    for batch in valid_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        #cuda.empty_cache() \n",
    "        # Telling the model not to compute or store gradients, to save memory and speed up validation\n",
    "        with no_grad():\n",
    "            cuda.empty_cache() \n",
    "            # Forward pass, calculate logit predictions\n",
    "            # This will return the logits rather than the loss because we have not provided labels\n",
    "            outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "            \n",
    "            #encoded_input = tokenizer(text, return_tensors='pt')\n",
    "            #output = model(**encoded_input)\n",
    "        \n",
    "        logits = outputs[1].detach().cpu().numpy() # Move logits to cpu\n",
    "        label_ids = b_labels.to('cpu').numpy() # Move labels to cpu\n",
    "        eval_loss += outputs[0].mean().item() # Valid. loss for current batch\n",
    "        \n",
    "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        true_labels.extend(label_ids)\n",
    "        \n",
    "    eval_loss = eval_loss / len(valid_dataloader)\n",
    "    validation_loss_values.append(eval_loss)\n",
    "    print(\"Validation loss: {}\".format(eval_loss))\n",
    "    \n",
    "    # Calculate the accuracy for this batch of test sentences\n",
    "    pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)\n",
    "                                 for p_i, l_i in zip(p, l) if tag_values[l_i] != \"PAD\"]\n",
    "    valid_tags = [tag_values[l_i] for l in true_labels\n",
    "                                  for l_i in l if tag_values[l_i] != \"PAD\"]\n",
    "    validation_accuracy_values.append(accuracy_score(pred_tags, valid_tags))\n",
    "    print(\"Validation Accuracy: {}\".format(accuracy_score(pred_tags, valid_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T15:02:04.575169Z",
     "iopub.status.busy": "2022-10-10T15:02:04.574732Z",
     "iopub.status.idle": "2022-10-10T15:02:16.597993Z",
     "shell.execute_reply": "2022-10-10T15:02:16.597234Z",
     "shell.execute_reply.started": "2022-10-10T15:02:04.575128Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "################ CALCULATE INTEREMEDIATE RESULTS ################\n",
    "test_sents = group_sentences(test, 'BIO')\n",
    "for i in range(0,len(test_sents)):\n",
    "    test_sents[i] = test_sents[i][0:300]\n",
    "    \n",
    "test_sentences = [[word[1] for word in sentence] for sentence in test_sents]  \n",
    "\n",
    "test_labels = [[tag2idx[w[len(w)-1]] for w in s] for s in test_sents]\n",
    "test_labels_str = [[w[len(w)-1] for w in s] for s in test_sents]\n",
    "\n",
    "reports = dict()\n",
    "i = 0\n",
    "for test_sentence in test_sentences:\n",
    "    tokenized_sentence = tokenizer.encode(test_sentence)\n",
    "    input_ids = tensor([tokenized_sentence]).cuda()\n",
    "    input_masks = tensor([[float(i != 0.0) for i in ii] for ii in input_ids]).cuda()\n",
    "    with no_grad():\n",
    "        output = model(input_ids, attention_mask = input_masks)\n",
    "    label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)\n",
    "    label_indices = label_indices[0]\n",
    "    label_indices = label_indices[1:-1]\n",
    "    \n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
    "    new_tokens, new_labels = [], []\n",
    "    for token, label_idx in zip(tokens, label_indices):\n",
    "        if token.startswith(\"##\"):\n",
    "            new_tokens[-1] = new_tokens[-1] + token[2:]\n",
    "        else:\n",
    "            new_labels.append(tag_values[label_idx])\n",
    "            new_tokens.append(token)\n",
    "    report = classification_report(test_labels_str[i], new_labels, output_dict=True)\n",
    "    reports[test_sents[i][0][0]] = report\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T15:02:16.599358Z",
     "iopub.status.busy": "2022-10-10T15:02:16.599114Z",
     "iopub.status.idle": "2022-10-10T15:02:16.632783Z",
     "shell.execute_reply": "2022-10-10T15:02:16.632139Z",
     "shell.execute_reply.started": "2022-10-10T15:02:16.599325Z"
    }
   },
   "outputs": [],
   "source": [
    "import json    \n",
    "with open(\"xlmroberta_large_per_submission_tag_evaluation.json\", \"w\") as outfile:\n",
    "    json.dump(reports, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T15:09:38.461942Z",
     "iopub.status.busy": "2022-10-10T15:09:38.461380Z",
     "iopub.status.idle": "2022-10-10T15:10:01.018467Z",
     "shell.execute_reply": "2022-10-10T15:10:01.017580Z",
     "shell.execute_reply.started": "2022-10-10T15:09:38.461896Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare test set...\n",
      "Tokenize and predict...\n"
     ]
    }
   ],
   "source": [
    "########## CALCULATE ALL RESULTS #############\n",
    "\n",
    "print(\"Prepare test set...\")\n",
    "test_sents = group_sentences(test, 'BIO')\n",
    "for i in range(0,len(test_sents)):\n",
    "    test_sents[i] = test_sents[i][0:300]\n",
    "    \n",
    "test_sentences = [[word[1] for word in sentence] for sentence in test_sents]\n",
    "test_labels = [[tag2idx[w[len(w)-1]] for w in s] for s in test_sents]\n",
    "test_labels_str = [[w[len(w)-1] for w in s] for s in test_sents]\n",
    "\n",
    "print(\"Tokenize and predict...\")\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "all_true_labels_str = []\n",
    "\n",
    "all_predictions_list = []\n",
    "\n",
    "for lab in test_labels:\n",
    "    all_true_labels.extend(lab)\n",
    "    \n",
    "for lab in test_labels_str:\n",
    "    all_true_labels_str.extend(lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T15:10:01.020417Z",
     "iopub.status.busy": "2022-10-10T15:10:01.020176Z",
     "iopub.status.idle": "2022-10-10T15:10:25.849149Z",
     "shell.execute_reply": "2022-10-10T15:10:25.847180Z",
     "shell.execute_reply.started": "2022-10-10T15:10:01.020384Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "  B-actor-neg       0.00      0.00      0.00         7\n",
      "  B-actor-pos       0.00      0.00      0.00       131\n",
      "    B-gen-neg       0.75      0.06      0.11        53\n",
      "    B-gen-pos       0.56      0.11      0.18       637\n",
      "B-keyword-neg       0.29      0.03      0.05       199\n",
      "B-keyword-pos       0.41      0.08      0.13      2472\n",
      "  B-movie-neg       0.60      0.05      0.09        59\n",
      "  B-movie-pos       0.45      0.20      0.28      4597\n",
      "  I-actor-neg       0.00      0.00      0.00         6\n",
      "  I-actor-pos       0.00      0.00      0.00       124\n",
      "    I-gen-neg       0.00      0.00      0.00         5\n",
      "    I-gen-pos       0.97      0.28      0.43       109\n",
      "I-keyword-neg       0.00      0.00      0.00        93\n",
      "I-keyword-pos       0.48      0.04      0.07      1694\n",
      "  I-movie-neg       0.33      0.01      0.03        72\n",
      "  I-movie-pos       0.80      0.16      0.27      6019\n",
      "            O       0.85      0.99      0.91     72944\n",
      "\n",
      "     accuracy                           0.83     89221\n",
      "    macro avg       0.38      0.12      0.15     89221\n",
      " weighted avg       0.80      0.83      0.79     89221\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for test_sentence in test_sentences:\n",
    "    tokenized_sentence = tokenizer.encode(test_sentence)\n",
    "    input_ids = tensor([tokenized_sentence]).cuda()\n",
    "    input_masks = tensor([[float(i != 0.0) for i in ii] for ii in input_ids]).cuda()\n",
    "    with no_grad():\n",
    "        output = model(input_ids, attention_mask = input_masks)\n",
    "    label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)\n",
    "    label_indices = label_indices[0]\n",
    "    label_indices = label_indices[1:-1]\n",
    "    \n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
    "    new_tokens, new_labels = [], []\n",
    "    for token, label_idx in zip(tokens, label_indices):\n",
    "        if token.startswith(\"##\"):\n",
    "            new_tokens[-1] = new_tokens[-1] + token[2:]\n",
    "        else:\n",
    "            new_labels.append(tag_values[label_idx])\n",
    "            new_tokens.append(token)\n",
    "    all_predictions.extend(new_labels)\n",
    "    all_predictions_list.append(new_labels)\n",
    "    \n",
    "all_preds = [tag2idx[label] for label in all_predictions]\n",
    "report = classification_report(all_true_labels_str, all_predictions)\n",
    "print(report)\n",
    "\n",
    "#with open(\"roberta_large_all_submission_tag_evaluation.json\", \"w\") as outfile:\n",
    "#    json.dump(report, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T15:10:25.850840Z",
     "iopub.status.busy": "2022-10-10T15:10:25.850576Z",
     "iopub.status.idle": "2022-10-10T15:10:31.749467Z",
     "shell.execute_reply": "2022-10-10T15:10:31.748613Z",
     "shell.execute_reply.started": "2022-10-10T15:10:25.850804Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(model, 'xlm-roberta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T15:10:31.751549Z",
     "iopub.status.busy": "2022-10-10T15:10:31.751279Z",
     "iopub.status.idle": "2022-10-10T15:10:31.861700Z",
     "shell.execute_reply": "2022-10-10T15:10:31.861022Z",
     "shell.execute_reply.started": "2022-10-10T15:10:31.751506Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = all_predictions_list\n",
    "all_outputs = dict()   # Stores all outputs from the test dataset per entity\n",
    "all_outputs_per_sentence = dict()  # Stores separate dictionaries per entity for every sentence in the dataset\n",
    "\n",
    "for i in range(0, len(predictions)): # Sentences iteration\n",
    "    tmp_dict = dict()\n",
    "    max_len = len(predictions[i])\n",
    "    for j in range(0, len(predictions[i])-3): # Word iteration\n",
    "        if predictions[i][j] == 'B-movie-pos':\n",
    "            if not 'positive_movies' in all_outputs.keys():\n",
    "                all_outputs['positive_movies'] = []\n",
    "            if not 'positive_movies' in tmp_dict.keys():\n",
    "                tmp_dict['positive_movies'] = []\n",
    "            tmp_entity = test_sentences[i][j]\n",
    "            k = j+1\n",
    "            while(predictions[i][k] == 'I-movie-pos' and k < max_len-1):\n",
    "                tmp_entity += ' ' + test_sentences[i][k]\n",
    "                if k < len(predictions):\n",
    "                    k +=1 \n",
    "            all_outputs['positive_movies'].append(tmp_entity)\n",
    "            if tmp_entity not in tmp_dict['positive_movies']:\n",
    "                tmp_dict['positive_movies'].append(tmp_entity)\n",
    "        \n",
    "        if predictions[i][j] == 'B-movie-neg':\n",
    "            if not 'negative_movies' in all_outputs.keys():\n",
    "                all_outputs['negative_movies'] = []\n",
    "            if not 'negative_movies' in tmp_dict.keys():\n",
    "                tmp_dict['negative_movies'] = []\n",
    "            tmp_entity = test_sentences[i][j]\n",
    "            k = j+1\n",
    "            while(predictions[i][k] == 'I-movie-neg'):\n",
    "                tmp_entity += ' ' + test_sentences[i][k]\n",
    "                k +=1 \n",
    "            all_outputs['negative_movies'].append(tmp_entity)\n",
    "            if tmp_entity not in tmp_dict['negative_movies']:\n",
    "                tmp_dict['negative_movies'].append(tmp_entity)\n",
    "            \n",
    "        if predictions[i][j] == 'B-keyword-pos':\n",
    "            if not 'positive_keywords' in all_outputs.keys():\n",
    "                all_outputs['positive_keywords'] = []\n",
    "            if not 'positive_keywords' in tmp_dict.keys():\n",
    "                tmp_dict['positive_keywords'] = []\n",
    "            tmp_entity = test_sentences[i][j]\n",
    "            k = j+1\n",
    "            while(predictions[i][k] == 'I-keyword-pos'):\n",
    "                tmp_entity += ' ' + test_sentences[i][k]\n",
    "                #tmp_entity test_sentences[i][k]\n",
    "                k +=1 \n",
    "            all_outputs['positive_keywords'].append(tmp_entity)\n",
    "            if tmp_entity not in tmp_dict['positive_keywords']:\n",
    "                tmp_dict['positive_keywords'].append(tmp_entity)\n",
    "\n",
    "                    \n",
    "        if predictions[i][j] == 'B-keyword-neg':\n",
    "            if not 'negative_keywords' in all_outputs.keys():\n",
    "                all_outputs['negative_keywords'] = []\n",
    "            if not 'negative_keywords' in tmp_dict.keys():\n",
    "                tmp_dict['negative_keywords'] = []\n",
    "            tmp_entity = test_sentences[i][j]\n",
    "            k = j+1\n",
    "            while(predictions[i][k] == 'I-keyword-neg'):\n",
    "                tmp_entity += ' ' + test_sentences[i][k]\n",
    "                k +=1 \n",
    "            all_outputs['negative_keywords'].append(tmp_entity)\n",
    "            if tmp_entity not in tmp_dict['negative_keywords']:\n",
    "                tmp_dict['negative_keywords'].append(tmp_entity)\n",
    "                    \n",
    "                    \n",
    "        if predictions[i][j] == 'B-actor-pos':\n",
    "            if not 'positive_actors' in all_outputs.keys():\n",
    "                all_outputs['positive_actors'] = []\n",
    "            if not 'positive_actors' in tmp_dict.keys():\n",
    "                tmp_dict['positive_actors'] = []\n",
    "            tmp_entity = test_sentences[i][j]\n",
    "            k = j+1\n",
    "            while(predictions[i][k] == 'I-actor-pos'):\n",
    "                tmp_entity += ' ' + test_sentences[i][k]\n",
    "                k +=1 \n",
    "            all_outputs['positive_actors'].append(tmp_entity)\n",
    "            if tmp_entity not in tmp_dict['positive_actors']:\n",
    "                tmp_dict['positive_actors'].append(tmp_entity)\n",
    "            \n",
    "        if predictions[i][j] == 'B-actor-neg':\n",
    "            if not 'negative_actors' in all_outputs.keys():\n",
    "                all_outputs['negative_actors'] = []\n",
    "            if not 'negative_actors' in tmp_dict.keys():\n",
    "                tmp_dict['negative_actors'] = []\n",
    "            tmp_entity = test_sentences[i][j]\n",
    "            k = j+1\n",
    "            while(predictions[i][k] == 'I-actor-neg'):\n",
    "                tmp_entity += ' ' + test_sentences[i][k]\n",
    "                k +=1 \n",
    "            all_outputs['negative_actors'].append(tmp_entity)\n",
    "            if tmp_entity not in tmp_dict['negative_actors']:\n",
    "                tmp_dict['negative_actors'].append(tmp_entity)\n",
    "            \n",
    "        if predictions[i][j] == 'B-gen-pos':\n",
    "            if not 'positive_genres' in all_outputs.keys():\n",
    "                all_outputs['positive_genres'] = []\n",
    "            if not 'positive_genres' in tmp_dict.keys():\n",
    "                tmp_dict['positive_genres'] = []\n",
    "            tmp_entity = test_sentences[i][j]\n",
    "            k = j+1\n",
    "            while(predictions[i][k] == 'I-gen-pos'):\n",
    "                tmp_entity += ' ' + test_sentences[i][k]\n",
    "                k +=1 \n",
    "            all_outputs['positive_genres'].append(tmp_entity)\n",
    "            if tmp_entity not in tmp_dict['positive_genres']:\n",
    "                tmp_dict['positive_genres'].append(tmp_entity)\n",
    "            \n",
    "        if predictions[i][j] == 'B-gen-neg':\n",
    "            if not 'negative_genres' in all_outputs.keys():\n",
    "                all_outputs['negative_genres'] = []\n",
    "            if not 'negative_genres' in tmp_dict.keys():\n",
    "                tmp_dict['negative_genres'] = []\n",
    "            tmp_entity = test_sentences[i][j]\n",
    "            k = j+1\n",
    "            while(predictions[i][k] == 'I-gen-neg'):\n",
    "                tmp_entity += ' ' + test_sentences[i][k]\n",
    "                k +=1 \n",
    "            all_outputs['negative_genres'].append(tmp_entity)\n",
    "            if tmp_entity not in tmp_dict['negative_genres']:\n",
    "                tmp_dict['negative_genres'].append(tmp_entity)\n",
    "\n",
    "    if i < len(test_sents):\n",
    "        all_outputs_per_sentence[test_sents[i][0][0]] = tmp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T15:10:51.370102Z",
     "iopub.status.busy": "2022-10-10T15:10:51.369262Z",
     "iopub.status.idle": "2022-10-10T15:10:51.376393Z",
     "shell.execute_reply": "2022-10-10T15:10:51.375432Z",
     "shell.execute_reply.started": "2022-10-10T15:10:51.370064Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_keyphrases(dict):\n",
    "    for key in dict.keys():\n",
    "        if \"positive_keywords\" in dict[key].keys():\n",
    "            tmp_pos_keys = []\n",
    "            for keyphrase in dict[key][\"positive_keywords\"]:\n",
    "                keywords = keyphrase.split(\" \")\n",
    "                tmp_pos_keys.extend(keywords)\n",
    "            dict[key][\"positive_keywords\"] = list(set(tmp_pos_keys))\n",
    "        if \"negative_keywords\" in dict[key].keys():\n",
    "            tmp_neg_keys = []\n",
    "            for keyphrase in dict[key][\"negative_keywords\"]:\n",
    "                keywords = keyphrase.split(\" \")\n",
    "                tmp_neg_keys.extend(keywords)\n",
    "            dict[key][\"negative_keywords\"] = list(set(tmp_neg_keys))\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T15:10:51.793092Z",
     "iopub.status.busy": "2022-10-10T15:10:51.792433Z",
     "iopub.status.idle": "2022-10-10T15:10:51.797295Z",
     "shell.execute_reply": "2022-10-10T15:10:51.796597Z",
     "shell.execute_reply.started": "2022-10-10T15:10:51.793059Z"
    }
   },
   "outputs": [],
   "source": [
    "all_outputs_per_sentence = split_keyphrases(all_outputs_per_sentence)\n",
    "    \n",
    "with open(\"xlm-roberta_large_best_umatched_format_1.json\", \"w\") as outfile:\n",
    "    json.dump(all_outputs_per_sentence, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T15:11:52.452668Z",
     "iopub.status.busy": "2022-10-10T15:11:52.452384Z",
     "iopub.status.idle": "2022-10-10T15:11:53.836718Z",
     "shell.execute_reply": "2022-10-10T15:11:53.835890Z",
     "shell.execute_reply.started": "2022-10-10T15:11:52.452636Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   actor-neg       0.00      0.00      0.00         7\n",
      "   actor-pos       0.00      0.00      0.00       131\n",
      "     gen-neg       0.75      0.06      0.11        53\n",
      "     gen-pos       0.33      0.07      0.11       639\n",
      " keyword-neg       0.10      0.01      0.02       200\n",
      " keyword-pos       0.19      0.04      0.06      2475\n",
      "   movie-neg       0.29      0.03      0.06        64\n",
      "   movie-pos       0.11      0.05      0.07      4599\n",
      "\n",
      "   micro avg       0.13      0.04      0.07      8168\n",
      "   macro avg       0.22      0.03      0.05      8168\n",
      "weighted avg       0.15      0.04      0.07      8168\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(test_labels_str, all_predictions_list)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T15:11:58.249726Z",
     "iopub.status.busy": "2022-10-10T15:11:58.249117Z",
     "iopub.status.idle": "2022-10-10T15:11:58.271286Z",
     "shell.execute_reply": "2022-10-10T15:11:58.270337Z",
     "shell.execute_reply.started": "2022-10-10T15:11:58.249668Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies\tO\tO\n",
      "surrounding\tO\tO\n",
      "characters\tO\tO\n",
      "who\tO\tO\n",
      "suddenly\tO\tO\n",
      "stop\tO\tO\n",
      "giving\tO\tO\n",
      "a\tO\tO\n",
      "fuck\tO\tO\n",
      "Examples\tO\tO\n",
      ";\tO\tO\n",
      "NEW_LINE\tO\tO\n",
      "NEW_LINE\tO\tO\n",
      "*\tO\tO\n",
      "[\tO\tO\n",
      "Office\tB-movie-pos\tB-movie-pos\n",
      "Space\tB-movie-pos\tI-movie-pos\n",
      "]\tO\tO\n",
      "(\tO\tO\n",
      "NEW_LINE\tO\tO\n",
      "NEW_LINE\tO\tO\n",
      "*\tO\tO\n",
      "[\tO\tO\n",
      "Fight\tB-movie-pos\tB-movie-pos\n",
      "Club\tB-movie-pos\tI-movie-pos\n",
      "]\tO\tO\n",
      "(\tO\tO\n",
      "NEW_LINE\tO\tO\n",
      "NEW_LINE\tO\tO\n",
      "*\tO\tO\n",
      "[\tO\tO\n",
      "Falling\tO\tB-movie-pos\n",
      "Down\tO\tI-movie-pos\n",
      "]\tO\tO\n",
      "(\tO\tO\n",
      "NEW_LINE\tO\tO\n",
      "NEW_LINE\tO\tO\n",
      "*\tO\tO\n",
      "God\tB-movie-pos\tB-movie-pos\n",
      "Bless\tB-movie-pos\tI-movie-pos\n",
      "America\tO\tI-movie-pos\n"
     ]
    }
   ],
   "source": [
    "all_true_labels_bukvi = [idx2tag[tag] for tag in all_true_labels]\n",
    "for token, pred_label, true_label in zip(test_sentences[5], all_predictions_list[5], test_labels[5]):\n",
    "    print(\"{}\\t{}\\t{}\".format(token, pred_label, idx2tag[true_label]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T15:12:14.830280Z",
     "iopub.status.busy": "2022-10-10T15:12:14.830006Z",
     "iopub.status.idle": "2022-10-10T15:12:14.836392Z",
     "shell.execute_reply": "2022-10-10T15:12:14.835593Z",
     "shell.execute_reply.started": "2022-10-10T15:12:14.830252Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = 'xlm_roberta_best'\n",
    "\n",
    "dict_save = open(\"t2idx_xlmroberta.json\", \"w\")\n",
    "json.dump(tag2idx, dict_save)\n",
    "dict_save.close()\n",
    "\n",
    "dict_save = open(\"idx2t_xlmroberta.json\", \"w\")\n",
    "json.dump(idx2tag, dict_save)\n",
    "dict_save.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
