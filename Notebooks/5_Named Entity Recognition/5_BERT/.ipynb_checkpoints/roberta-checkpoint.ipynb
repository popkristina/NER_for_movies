{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T11:29:41.997779Z",
     "iopub.status.busy": "2022-10-10T11:29:41.997455Z",
     "iopub.status.idle": "2022-10-10T11:29:42.013120Z",
     "shell.execute_reply": "2022-10-10T11:29:42.012434Z",
     "shell.execute_reply.started": "2022-10-10T11:29:41.997743Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm, trange\n",
    "import string\n",
    "import os\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from torch import cuda, no_grad, tensor\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import RobertaTokenizer, RobertaForTokenClassification\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from seqeval.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T11:29:46.982320Z",
     "iopub.status.busy": "2022-10-10T11:29:46.981975Z",
     "iopub.status.idle": "2022-10-10T11:29:46.992302Z",
     "shell.execute_reply": "2022-10-10T11:29:46.991371Z",
     "shell.execute_reply.started": "2022-10-10T11:29:46.982284Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    #os.chdir('D:/TU_Graz/Thesis/Datasets/Reddit_features')\n",
    "    train = pd.read_csv(\"../input/bio-tagged/train_final_all.csv\")\n",
    "    test = pd.read_csv(\"../input/bio-tagged/test_final_all.csv\")\n",
    "    data = train.append(test)\n",
    "\n",
    "    return train, test, data\n",
    "\n",
    "def group_sentences(data, category):\n",
    "    all_sents = []\n",
    "    sent_ids = data['Sent_id'].unique()\n",
    "    for curr_id in sent_ids:\n",
    "        tmp_df = data[data['Sent_id'] == curr_id]\n",
    "        tmp_df = pd.concat([tmp_df['Sent_id'], tmp_df['Token'], tmp_df[\"Token_index\"], tmp_df.iloc[:,4:147], tmp_df[category]], axis = 1)\n",
    "        records = tmp_df.to_records(index=False)\n",
    "        all_sents.append(records)\n",
    "    return all_sents\n",
    "\n",
    "def remove_sents_over_threshold(sents, threshold):\n",
    "    sentences = list()\n",
    "    for s in sents:\n",
    "        if len(s) < threshold:\n",
    "            sentences.append(s)\n",
    "    return sentences\n",
    "\n",
    "def set_processor_params():\n",
    "    device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "    n_gpu = cuda.device_count()\n",
    "    cuda.get_device_name(0)\n",
    "    return device, n_gpu\n",
    "\n",
    "def tokenize(sentence, sentence_labels):\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "    for word, label in zip(sentence, sentence_labels):\n",
    "        str_word = str(word)\n",
    "        tokenized_word = tokenizer.tokenize(str_word) # Tokenize the word\n",
    "        n_subwords = len(tokenized_word) # Count subwords\n",
    "        tokenized_sentence.extend(tokenized_word) # Add to the final tokenized list\n",
    "        labels.extend([label] * n_subwords) # Add the same label of the original word to all of its subwords\n",
    "    return tokenized_sentence, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T11:29:48.145265Z",
     "iopub.status.busy": "2022-10-10T11:29:48.145001Z",
     "iopub.status.idle": "2022-10-10T11:30:23.639223Z",
     "shell.execute_reply": "2022-10-10T11:30:23.638462Z",
     "shell.execute_reply.started": "2022-10-10T11:29:48.145234Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare sent\n",
      "1185\n",
      "1185\n",
      "tokenize\n",
      "pad\n"
     ]
    }
   ],
   "source": [
    "print(\"Read data\")\n",
    "train, test, data = read_data()\n",
    "device, n_gpu = set_processor_params()\n",
    "tag_values = list(set(train[\"BIO\"].values))\n",
    "tag_values.append(\"PAD\")\n",
    "tag2idx = {t: i for i, t in enumerate(tag_values)}\n",
    "idx2tag = {v: k for k, v in tag2idx.items()}\n",
    "\n",
    "print(\"Prepare sentences\")\n",
    "sents = group_sentences(train, 'BIO')\n",
    "for i in range(0, len(sents)):\n",
    "    sents[i] = sents[i][0:300]\n",
    "sentences = [[word[1] for word in sentence] for sentence in sents]\n",
    "labels = [[tag2idx[w[len(w)-1]] for w in s] for s in sents]\n",
    "\n",
    "MAX_LEN = 350\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 3e-5\n",
    "MAX_GRAD_NORM = 1.0\n",
    "\n",
    "print(\"Tokenize\")\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-large\", do_lower_case=False)\n",
    "tokenized_texts_and_labels = [tokenize(sentence, sentence_labels) for sentence, sentence_labels in zip(sentences, labels)]\n",
    "\n",
    "tokenized_texts = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels]\n",
    "labels_subwords = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels]\n",
    "\n",
    "print(\"Pad\")\n",
    "# Cut the token and label sequences to the max length\n",
    "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts], maxlen = MAX_LEN, dtype=\"long\", value=0.0, \n",
    "                          truncating=\"post\", padding=\"post\")\n",
    "input_tags = pad_sequences([[l for l in lab] for lab in labels_subwords], maxlen = MAX_LEN, value = tag2idx[\"PAD\"], \n",
    "                           padding=\"post\", dtype=\"long\", truncating=\"post\")\n",
    "attention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]\n",
    "\n",
    "tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, input_tags, test_size=0.1, random_state=2021)\n",
    "tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids, test_size=0.1, random_state=2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T11:30:23.641576Z",
     "iopub.status.busy": "2022-10-10T11:30:23.640767Z",
     "iopub.status.idle": "2022-10-10T11:32:03.844838Z",
     "shell.execute_reply": "2022-10-10T11:32:03.843564Z",
     "shell.execute_reply.started": "2022-10-10T11:30:23.641536Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ff34ca13f05473bbc872c7721d1400a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForTokenClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tr_inputs = tensor(tr_inputs)\n",
    "val_inputs = tensor(val_inputs)\n",
    "tr_tags = tensor(tr_tags)\n",
    "val_tags = tensor(val_tags)\n",
    "tr_masks = tensor(tr_masks)\n",
    "val_masks = tensor(val_masks)\n",
    "\n",
    "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "model = RobertaForTokenClassification.from_pretrained(\"roberta-large\", num_labels = len(tag2idx), output_attentions = False, output_hidden_states=False)\n",
    "model.cuda(); # Pass the model parameters to gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T11:32:03.847115Z",
     "iopub.status.busy": "2022-10-10T11:32:03.846728Z",
     "iopub.status.idle": "2022-10-10T11:32:03.870544Z",
     "shell.execute_reply": "2022-10-10T11:32:03.869624Z",
     "shell.execute_reply.started": "2022-10-10T11:32:03.847062Z"
    }
   },
   "outputs": [],
   "source": [
    "FULL_FINETUNING = True\n",
    "if FULL_FINETUNING:\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    param_optimizer = list(model.classifier.named_parameters())\n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "    \n",
    "optimizer = AdamW(\n",
    "    optimizer_grouped_parameters,\n",
    "    lr=LEARNING_RATE,\n",
    "    eps=1e-8\n",
    ")\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T11:32:04.460929Z",
     "iopub.status.busy": "2022-10-10T11:32:04.459912Z",
     "iopub.status.idle": "2022-10-10T12:21:38.187234Z",
     "shell.execute_reply": "2022-10-10T12:21:38.186523Z",
     "shell.execute_reply.started": "2022-10-10T11:32:04.460897Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:   0%|          | 0/20 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 0.36536249632804135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:   5%|▌         | 1/20 [02:30<47:38, 150.44s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.23343931858738262\n",
      "Validation Accuracy: 0.9366966015143511\n",
      "Average train loss: 0.21230169787500683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  10%|█         | 2/20 [04:59<44:55, 149.77s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.2128193696339925\n",
      "Validation Accuracy: 0.9399542172917768\n",
      "Average train loss: 0.15909733952542815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  15%|█▌        | 3/20 [07:29<42:22, 149.57s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.2068712600817283\n",
      "Validation Accuracy: 0.9419792216939602\n",
      "Average train loss: 0.11608780913180515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  20%|██        | 4/20 [09:58<39:51, 149.45s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.26448247296114763\n",
      "Validation Accuracy: 0.9418911780243\n",
      "Average train loss: 0.08188831346800153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  25%|██▌       | 5/20 [12:27<37:20, 149.38s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.2784815696378549\n",
      "Validation Accuracy: 0.9328226800493045\n",
      "Average train loss: 0.05838462919666526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  30%|███       | 6/20 [14:56<34:50, 149.29s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.2818282932043076\n",
      "Validation Accuracy: 0.9423313963726008\n",
      "Average train loss: 0.044996824905133286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  35%|███▌      | 7/20 [17:25<32:19, 149.21s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.31695215180516245\n",
      "Validation Accuracy: 0.9399542172917768\n",
      "Average train loss: 0.0338567865978445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  40%|████      | 8/20 [19:54<29:48, 149.05s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.30991083023448784\n",
      "Validation Accuracy: 0.9433879204085226\n",
      "Average train loss: 0.02429158560132036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  45%|████▌     | 9/20 [22:23<27:18, 148.94s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.3684974133657912\n",
      "Validation Accuracy: 0.9431237893995422\n",
      "Average train loss: 0.0187115133148121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  50%|█████     | 10/20 [24:51<24:48, 148.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.39170944100866717\n",
      "Validation Accuracy: 0.9404824793097376\n",
      "Average train loss: 0.012998968250644028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  55%|█████▌    | 11/20 [27:20<22:19, 148.79s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.43156511429697275\n",
      "Validation Accuracy: 0.9445324881141046\n",
      "Average train loss: 0.010393377688392031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  60%|██████    | 12/20 [29:49<19:49, 148.73s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.44137560079495114\n",
      "Validation Accuracy: 0.9434759640781828\n",
      "Average train loss: 0.0079645021581878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  65%|██████▌   | 13/20 [32:17<17:20, 148.64s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.44836623123846947\n",
      "Validation Accuracy: 0.9397781299524565\n",
      "Average train loss: 0.006442571536384737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  70%|███████   | 14/20 [34:45<14:51, 148.57s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.4557272681966424\n",
      "Validation Accuracy: 0.9408346539883783\n",
      "Average train loss: 0.005740462038402222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  75%|███████▌  | 15/20 [37:14<12:22, 148.48s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.4653626590967178\n",
      "Validation Accuracy: 0.9392498679344955\n",
      "Average train loss: 0.004021632523324528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  80%|████████  | 16/20 [39:41<09:52, 148.25s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.490420013324668\n",
      "Validation Accuracy: 0.9388096495861947\n",
      "Average train loss: 0.0035833916255851297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  85%|████████▌ | 17/20 [42:09<07:24, 148.15s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.49658303006241716\n",
      "Validation Accuracy: 0.9399542172917768\n",
      "Average train loss: 0.0029272451640335864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  90%|█████████ | 18/20 [44:37<04:56, 148.10s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.50319081582129\n",
      "Validation Accuracy: 0.9433879204085226\n",
      "Average train loss: 0.002705734176984986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  95%|█████████▌| 19/20 [47:05<02:28, 148.05s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.5118562967826923\n",
      "Validation Accuracy: 0.9426835710512415\n",
      "Average train loss: 0.002362536035775094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 100%|██████████| 20/20 [49:33<00:00, 148.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 0.5137320080151161\n",
      "Validation Accuracy: 0.9418911780243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss_values, validation_loss_values = [], []\n",
    "accuracy_values, validation_accuracy_values = [], []\n",
    "\n",
    "for i in trange(EPOCHS, desc=\"Epoch\"):\n",
    "    \n",
    "    # TRAINING\n",
    "    # Perform one full pass over the training set\n",
    "    model.train() # Put the model into training mode\n",
    "    total_loss, total_accuracy = 0, 0 # Reset the total loss and acc. for current epoch\n",
    "    \n",
    "    # Training loop\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch) # add batch to gpu\n",
    "        b_input_ids, b_input_mask, b_labels = batch # Input ids, mask and labels of the current batch\n",
    "        model.zero_grad() # Always clear any previously calculated gradients before performing a backward pass\n",
    "        #cuda.empty_cache() \n",
    "        # Forward pass\n",
    "        # This will return the loss (rather than the model output) because we have provided the `labels`.\n",
    "        outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs[0]\n",
    "        \n",
    "        # Perform a backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "        total_loss += loss.item() # track train loss\n",
    "        \n",
    "        # Clip the norm of the gradient to help prevent the exploding gradients problem\n",
    "        from torch.nn.utils import clip_grad_norm_\n",
    "        clip_grad_norm_(parameters=model.parameters(), max_norm=MAX_GRAD_NORM)\n",
    "        \n",
    "        optimizer.step() # update parameters\n",
    "        scheduler.step() # Update the learning rate\n",
    "        \n",
    "    avg_train_loss = total_loss / len(train_dataloader) # Calc. avg loss over training data\n",
    "    print(\"Average train loss: {}\".format(avg_train_loss))\n",
    "    loss_values.append(avg_train_loss) # Store the loss value for plotting the learning curve\n",
    "\n",
    "    # VALIDATION\n",
    "    # After the completion of each training epoch, measure performance on validation set  \n",
    "    model.eval() # Put the model into evaluation mode\n",
    "    eval_loss, eval_accuracy = 0, 0 # Reset the validation loss for current epoch\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    predictions , true_labels = [], []\n",
    "    \n",
    "    # Validation loop\n",
    "    for batch in valid_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        #cuda.empty_cache() \n",
    "        # Telling the model not to compute or store gradients, to save memory and speed up validation\n",
    "        with no_grad():\n",
    "            cuda.empty_cache() \n",
    "            # Forward pass, calculate logit predictions\n",
    "            # This will return the logits rather than the loss because we have not provided labels\n",
    "            outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "            \n",
    "            #encoded_input = tokenizer(text, return_tensors='pt')\n",
    "            #output = model(**encoded_input)\n",
    "        \n",
    "        logits = outputs[1].detach().cpu().numpy() # Move logits to cpu\n",
    "        label_ids = b_labels.to('cpu').numpy() # Move labels to cpu\n",
    "        eval_loss += outputs[0].mean().item() # Valid. loss for current batch\n",
    "        \n",
    "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
    "        true_labels.extend(label_ids)\n",
    "        \n",
    "    eval_loss = eval_loss / len(valid_dataloader)\n",
    "    validation_loss_values.append(eval_loss)\n",
    "    print(\"Validation loss: {}\".format(eval_loss))\n",
    "    \n",
    "    # Calculate the accuracy for this batch of test sentences\n",
    "    pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)\n",
    "                                 for p_i, l_i in zip(p, l) if tag_values[l_i] != \"PAD\"]\n",
    "    valid_tags = [tag_values[l_i] for l in true_labels\n",
    "                                  for l_i in l if tag_values[l_i] != \"PAD\"]\n",
    "    validation_accuracy_values.append(accuracy_score(pred_tags, valid_tags))\n",
    "    print(\"Validation Accuracy: {}\".format(accuracy_score(pred_tags, valid_tags)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T12:21:38.189042Z",
     "iopub.status.busy": "2022-10-10T12:21:38.188626Z",
     "iopub.status.idle": "2022-10-10T12:21:49.392498Z",
     "shell.execute_reply": "2022-10-10T12:21:49.391731Z",
     "shell.execute_reply.started": "2022-10-10T12:21:38.189006Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "################ CALCULATE INTEREMEDIATE RESULTS ################\n",
    "test_sents = group_sentences(test, 'BIO')\n",
    "for i in range(0,len(test_sents)):\n",
    "    test_sents[i] = test_sents[i][0:300]\n",
    "    \n",
    "test_sentences = [[word[1] for word in sentence] for sentence in test_sents]  \n",
    "test_labels = [[tag2idx[w[len(w)-1]] for w in s] for s in test_sents]\n",
    "test_labels_str = [[w[len(w)-1] for w in s] for s in test_sents]\n",
    "\n",
    "reports = dict()\n",
    "i = 0\n",
    "for test_sentence in test_sentences:\n",
    "    tokenized_sentence = tokenizer.encode(test_sentence)\n",
    "    input_ids = tensor([tokenized_sentence]).cuda()\n",
    "    input_masks = tensor([[float(i != 0.0) for i in ii] for ii in input_ids]).cuda()\n",
    "    with no_grad():\n",
    "        output = model(input_ids, attention_mask = input_masks)\n",
    "    label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)\n",
    "    label_indices = label_indices[0]\n",
    "    label_indices = label_indices[1:-1]\n",
    "    \n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
    "    new_tokens, new_labels = [], []\n",
    "    for token, label_idx in zip(tokens, label_indices):\n",
    "        if token.startswith(\"##\"):\n",
    "            new_tokens[-1] = new_tokens[-1] + token[2:]\n",
    "        else:\n",
    "            new_labels.append(tag_values[label_idx])\n",
    "            new_tokens.append(token)\n",
    "    report = classification_report(test_labels_str[i], new_labels, output_dict=True)\n",
    "    reports[test_sents[i][0][0]] = report\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T12:21:49.394152Z",
     "iopub.status.busy": "2022-10-10T12:21:49.393893Z",
     "iopub.status.idle": "2022-10-10T12:21:49.428305Z",
     "shell.execute_reply": "2022-10-10T12:21:49.427696Z",
     "shell.execute_reply.started": "2022-10-10T12:21:49.394115Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"roberta_large_per_submission_tag_evaluation.json\", \"w\") as outfile:\n",
    "    json.dump(reports, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T12:21:49.429898Z",
     "iopub.status.busy": "2022-10-10T12:21:49.429646Z",
     "iopub.status.idle": "2022-10-10T12:22:11.669743Z",
     "shell.execute_reply": "2022-10-10T12:22:11.668965Z",
     "shell.execute_reply.started": "2022-10-10T12:21:49.429864Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare test set...\n",
      "Tokenize and predict...\n"
     ]
    }
   ],
   "source": [
    "########## CALCULATE ALL RESULTS #############\n",
    "\n",
    "print(\"Prepare test set...\")\n",
    "test_sents = group_sentences(train, 'BIO')\n",
    "for i in range(0,len(test_sents)):\n",
    "    test_sents[i] = test_sents[i][0:300]\n",
    "    \n",
    "test_sentences = [[word[1] for word in sentence] for sentence in test_sents]\n",
    "test_labels = [[tag2idx[w[len(w)-1]] for w in s] for s in test_sents]\n",
    "test_labels_str = [[w[len(w)-1] for w in s] for s in test_sents]\n",
    "\n",
    "print(\"Tokenize and predict...\")\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "all_true_labels_str = []\n",
    "\n",
    "all_predictions_list = []\n",
    "\n",
    "for lab in test_labels:\n",
    "    all_true_labels.extend(lab)\n",
    "    \n",
    "for lab in test_labels_str:\n",
    "    all_true_labels_str.extend(lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T12:27:39.421643Z",
     "iopub.status.busy": "2022-10-10T12:27:39.421341Z",
     "iopub.status.idle": "2022-10-10T12:28:03.995814Z",
     "shell.execute_reply": "2022-10-10T12:28:03.994294Z",
     "shell.execute_reply.started": "2022-10-10T12:27:39.421601Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "  B-actor-neg       0.67      0.29      0.40         7\n",
      "  B-actor-pos       0.94      0.45      0.61       131\n",
      "    B-gen-neg       1.00      0.17      0.29        53\n",
      "    B-gen-pos       0.91      0.26      0.41       637\n",
      "B-keyword-neg       0.68      0.54      0.60       199\n",
      "B-keyword-pos       0.72      0.56      0.63      2472\n",
      "  B-movie-neg       0.56      0.47      0.51        59\n",
      "  B-movie-pos       0.84      0.74      0.79      4597\n",
      "  I-actor-neg       1.00      0.33      0.50         6\n",
      "  I-actor-pos       0.95      0.57      0.71       124\n",
      "    I-gen-neg       0.00      0.00      0.00         5\n",
      "    I-gen-pos       0.95      0.83      0.89       109\n",
      "I-keyword-neg       0.76      0.54      0.63        93\n",
      "I-keyword-pos       0.74      0.63      0.68      1694\n",
      "  I-movie-neg       0.47      0.31      0.37        72\n",
      "  I-movie-pos       0.87      0.85      0.86      6019\n",
      "            O       0.95      0.98      0.96     72944\n",
      "\n",
      "     accuracy                           0.93     89221\n",
      "    macro avg       0.77      0.50      0.58     89221\n",
      " weighted avg       0.93      0.93      0.93     89221\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for test_sentence in test_sentences:\n",
    "    tokenized_sentence = tokenizer.encode(test_sentence)\n",
    "    input_ids = tensor([tokenized_sentence]).cuda()\n",
    "    input_masks = tensor([[float(i != 0.0) for i in ii] for ii in input_ids]).cuda()\n",
    "    with no_grad():\n",
    "        output = model(input_ids, attention_mask = input_masks)\n",
    "    label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)\n",
    "    label_indices = label_indices[0]\n",
    "    label_indices = label_indices[1:-1]\n",
    "    \n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
    "    new_tokens, new_labels = [], []\n",
    "    for token, label_idx in zip(tokens, label_indices):\n",
    "        if token.startswith(\"##\"):\n",
    "            new_tokens[-1] = new_tokens[-1] + token[2:]\n",
    "        else:\n",
    "            new_labels.append(tag_values[label_idx])\n",
    "            new_tokens.append(token)\n",
    "    all_predictions.extend(new_labels)\n",
    "    all_predictions_list.append(new_labels)\n",
    "    \n",
    "all_preds = [tag2idx[label] for label in all_predictions]\n",
    "report = classification_report(all_true_labels_str, all_predictions)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T12:29:08.776984Z",
     "iopub.status.busy": "2022-10-10T12:29:08.776389Z",
     "iopub.status.idle": "2022-10-10T12:29:11.150759Z",
     "shell.execute_reply": "2022-10-10T12:29:11.149707Z",
     "shell.execute_reply.started": "2022-10-10T12:29:08.776944Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(model, 'roberta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T12:30:18.057324Z",
     "iopub.status.busy": "2022-10-10T12:30:18.057073Z",
     "iopub.status.idle": "2022-10-10T12:30:18.174286Z",
     "shell.execute_reply": "2022-10-10T12:30:18.173634Z",
     "shell.execute_reply.started": "2022-10-10T12:30:18.057293Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = all_predictions_list\n",
    "all_outputs = dict()   # Stores all outputs from the test dataset per entity\n",
    "all_outputs_per_sentence = dict()  # Stores separate dictionaries per entity for every sentence in the dataset\n",
    "\n",
    "for i in range(0, len(predictions)): # Sentences iteration\n",
    "    tmp_dict = dict()\n",
    "    max_len = len(predictions[i])\n",
    "    for j in range(0, len(predictions[i])-3): # Word iteration\n",
    "        if predictions[i][j] == 'B-movie-pos':\n",
    "            if not 'positive_movies' in all_outputs.keys():\n",
    "                all_outputs['positive_movies'] = []\n",
    "            if not 'positive_movies' in tmp_dict.keys():\n",
    "                tmp_dict['positive_movies'] = []\n",
    "            tmp_entity = test_sentences[i][j]\n",
    "            k = j+1\n",
    "            while(predictions[i][k] == 'I-movie-pos' and k < max_len-1):\n",
    "                tmp_entity += ' ' + test_sentences[i][k]\n",
    "                if k < len(predictions):\n",
    "                    k +=1 \n",
    "            all_outputs['positive_movies'].append(tmp_entity)\n",
    "            if tmp_entity not in tmp_dict['positive_movies']:\n",
    "                tmp_dict['positive_movies'].append(tmp_entity)\n",
    "        \n",
    "        if predictions[i][j] == 'B-movie-neg':\n",
    "            if not 'negative_movies' in all_outputs.keys():\n",
    "                all_outputs['negative_movies'] = []\n",
    "            if not 'negative_movies' in tmp_dict.keys():\n",
    "                tmp_dict['negative_movies'] = []\n",
    "            tmp_entity = test_sentences[i][j]\n",
    "            k = j+1\n",
    "            while(predictions[i][k] == 'I-movie-neg'):\n",
    "                tmp_entity += ' ' + test_sentences[i][k]\n",
    "                k +=1 \n",
    "            all_outputs['negative_movies'].append(tmp_entity)\n",
    "            if tmp_entity not in tmp_dict['negative_movies']:\n",
    "                tmp_dict['negative_movies'].append(tmp_entity)\n",
    "            \n",
    "        if predictions[i][j] == 'B-keyword-pos':\n",
    "            if not 'positive_keywords' in all_outputs.keys():\n",
    "                all_outputs['positive_keywords'] = []\n",
    "            if not 'positive_keywords' in tmp_dict.keys():\n",
    "                tmp_dict['positive_keywords'] = []\n",
    "            tmp_entity = test_sentences[i][j]\n",
    "            k = j+1\n",
    "            while(predictions[i][k] == 'I-keyword-pos'):\n",
    "                tmp_entity += ' ' + test_sentences[i][k]\n",
    "                #tmp_entity test_sentences[i][k]\n",
    "                k +=1 \n",
    "            all_outputs['positive_keywords'].append(tmp_entity)\n",
    "            if tmp_entity not in tmp_dict['positive_keywords']:\n",
    "                tmp_dict['positive_keywords'].append(tmp_entity)\n",
    "\n",
    "                    \n",
    "        if predictions[i][j] == 'B-keyword-neg':\n",
    "            if not 'negative_keywords' in all_outputs.keys():\n",
    "                all_outputs['negative_keywords'] = []\n",
    "            if not 'negative_keywords' in tmp_dict.keys():\n",
    "                tmp_dict['negative_keywords'] = []\n",
    "            tmp_entity = test_sentences[i][j]\n",
    "            k = j+1\n",
    "            while(predictions[i][k] == 'I-keyword-neg'):\n",
    "                tmp_entity += ' ' + test_sentences[i][k]\n",
    "                k +=1 \n",
    "            all_outputs['negative_keywords'].append(tmp_entity)\n",
    "            if tmp_entity not in tmp_dict['negative_keywords']:\n",
    "                tmp_dict['negative_keywords'].append(tmp_entity)\n",
    "                    \n",
    "                    \n",
    "        if predictions[i][j] == 'B-actor-pos':\n",
    "            if not 'positive_actors' in all_outputs.keys():\n",
    "                all_outputs['positive_actors'] = []\n",
    "            if not 'positive_actors' in tmp_dict.keys():\n",
    "                tmp_dict['positive_actors'] = []\n",
    "            tmp_entity = test_sentences[i][j]\n",
    "            k = j+1\n",
    "            while(predictions[i][k] == 'I-actor-pos'):\n",
    "                tmp_entity += ' ' + test_sentences[i][k]\n",
    "                k +=1 \n",
    "            all_outputs['positive_actors'].append(tmp_entity)\n",
    "            if tmp_entity not in tmp_dict['positive_actors']:\n",
    "                tmp_dict['positive_actors'].append(tmp_entity)\n",
    "            \n",
    "        if predictions[i][j] == 'B-actor-neg':\n",
    "            if not 'negative_actors' in all_outputs.keys():\n",
    "                all_outputs['negative_actors'] = []\n",
    "            if not 'negative_actors' in tmp_dict.keys():\n",
    "                tmp_dict['negative_actors'] = []\n",
    "            tmp_entity = test_sentences[i][j]\n",
    "            k = j+1\n",
    "            while(predictions[i][k] == 'I-actor-neg'):\n",
    "                tmp_entity += ' ' + test_sentences[i][k]\n",
    "                k +=1 \n",
    "            all_outputs['negative_actors'].append(tmp_entity)\n",
    "            if tmp_entity not in tmp_dict['negative_actors']:\n",
    "                tmp_dict['negative_actors'].append(tmp_entity)\n",
    "            \n",
    "        if predictions[i][j] == 'B-gen-pos':\n",
    "            if not 'positive_genres' in all_outputs.keys():\n",
    "                all_outputs['positive_genres'] = []\n",
    "            if not 'positive_genres' in tmp_dict.keys():\n",
    "                tmp_dict['positive_genres'] = []\n",
    "            tmp_entity = test_sentences[i][j]\n",
    "            k = j+1\n",
    "            while(predictions[i][k] == 'I-gen-pos'):\n",
    "                tmp_entity += ' ' + test_sentences[i][k]\n",
    "                k +=1 \n",
    "            all_outputs['positive_genres'].append(tmp_entity)\n",
    "            if tmp_entity not in tmp_dict['positive_genres']:\n",
    "                tmp_dict['positive_genres'].append(tmp_entity)\n",
    "            \n",
    "        if predictions[i][j] == 'B-gen-neg':\n",
    "            if not 'negative_genres' in all_outputs.keys():\n",
    "                all_outputs['negative_genres'] = []\n",
    "            if not 'negative_genres' in tmp_dict.keys():\n",
    "                tmp_dict['negative_genres'] = []\n",
    "            tmp_entity = test_sentences[i][j]\n",
    "            k = j+1\n",
    "            while(predictions[i][k] == 'I-gen-neg'):\n",
    "                tmp_entity += ' ' + test_sentences[i][k]\n",
    "                k +=1 \n",
    "            all_outputs['negative_genres'].append(tmp_entity)\n",
    "            if tmp_entity not in tmp_dict['negative_genres']:\n",
    "                tmp_dict['negative_genres'].append(tmp_entity)\n",
    "    \n",
    "    #print(sentences_test[i][0][0])\n",
    "    if i < len(test_sents):\n",
    "        all_outputs_per_sentence[test_sents[i][0][0]] = tmp_dict\n",
    "    #print(tmp_movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T12:30:57.941941Z",
     "iopub.status.busy": "2022-10-10T12:30:57.941619Z",
     "iopub.status.idle": "2022-10-10T12:30:57.959320Z",
     "shell.execute_reply": "2022-10-10T12:30:57.957743Z",
     "shell.execute_reply.started": "2022-10-10T12:30:57.941905Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_keyphrases(dict):\n",
    "    for key in dict.keys():\n",
    "        if \"positive_keywords\" in dict[key].keys():\n",
    "            tmp_pos_keys = []\n",
    "            for keyphrase in dict[key][\"positive_keywords\"]:\n",
    "                keywords = keyphrase.split(\" \")\n",
    "                tmp_pos_keys.extend(keywords)\n",
    "            dict[key][\"positive_keywords\"] = list(set(tmp_pos_keys))\n",
    "        if \"negative_keywords\" in dict[key].keys():\n",
    "            tmp_neg_keys = []\n",
    "            for keyphrase in dict[key][\"negative_keywords\"]:\n",
    "                keywords = keyphrase.split(\" \")\n",
    "                tmp_neg_keys.extend(keywords)\n",
    "            dict[key][\"negative_keywords\"] = list(set(tmp_neg_keys))\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T12:48:55.975663Z",
     "iopub.status.busy": "2022-10-10T12:48:55.975237Z",
     "iopub.status.idle": "2022-10-10T12:48:56.004967Z",
     "shell.execute_reply": "2022-10-10T12:48:56.004342Z",
     "shell.execute_reply.started": "2022-10-10T12:48:55.975615Z"
    }
   },
   "outputs": [],
   "source": [
    "all_outputs_per_sentence = split_keyphrases(all_outputs_per_sentence)   \n",
    "with open(\"roberta_large_best_umatched_format_1.json\", \"w\") as outfile:\n",
    "    json.dump(all_outputs_per_sentence, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T12:49:05.503091Z",
     "iopub.status.busy": "2022-10-10T12:49:05.502234Z",
     "iopub.status.idle": "2022-10-10T12:49:05.508586Z",
     "shell.execute_reply": "2022-10-10T12:49:05.507712Z",
     "shell.execute_reply.started": "2022-10-10T12:49:05.503049Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   actor-neg       0.67      0.29      0.40         7\n",
      "   actor-pos       0.85      0.44      0.58       131\n",
      "     gen-neg       0.89      0.15      0.26        53\n",
      "     gen-pos       0.87      0.26      0.40       639\n",
      " keyword-neg       0.60      0.48      0.54       200\n",
      " keyword-pos       0.59      0.47      0.52      2475\n",
      "   movie-neg       0.42      0.42      0.42        64\n",
      "   movie-pos       0.71      0.63      0.67      4599\n",
      "\n",
      "   micro avg       0.67      0.54      0.60      8168\n",
      "   macro avg       0.70      0.39      0.47      8168\n",
      "weighted avg       0.68      0.54      0.59      8168\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(test_labels_str, all_predictions_list)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-10T12:49:35.814695Z",
     "iopub.status.busy": "2022-10-10T12:49:35.814414Z",
     "iopub.status.idle": "2022-10-10T12:49:35.835106Z",
     "shell.execute_reply": "2022-10-10T12:49:35.834340Z",
     "shell.execute_reply.started": "2022-10-10T12:49:35.814663Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies\tO\tO\n",
      "surrounding\tO\tO\n",
      "characters\tO\tO\n",
      "who\tO\tO\n",
      "suddenly\tO\tO\n",
      "stop\tO\tO\n",
      "giving\tO\tO\n",
      "a\tO\tO\n",
      "fuck\tO\tO\n",
      "Examples\tO\tO\n",
      ";\tO\tO\n",
      "NEW_LINE\tO\tO\n",
      "NEW_LINE\tO\tO\n",
      "*\tO\tO\n",
      "[\tO\tO\n",
      "Office\tB-movie-pos\tB-movie-pos\n",
      "Space\tI-movie-pos\tI-movie-pos\n",
      "]\tO\tO\n",
      "(\tO\tO\n",
      "NEW_LINE\tO\tO\n",
      "NEW_LINE\tO\tO\n",
      "*\tO\tO\n",
      "[\tO\tO\n",
      "Fight\tB-movie-pos\tB-movie-pos\n",
      "Club\tI-movie-pos\tI-movie-pos\n",
      "]\tO\tO\n",
      "(\tO\tO\n",
      "NEW_LINE\tO\tO\n",
      "NEW_LINE\tO\tO\n",
      "*\tO\tO\n",
      "[\tO\tO\n",
      "Falling\tB-movie-pos\tB-movie-pos\n",
      "Down\tI-movie-pos\tI-movie-pos\n",
      "]\tO\tO\n",
      "(\tO\tO\n",
      "NEW_LINE\tO\tO\n",
      "NEW_LINE\tO\tO\n",
      "*\tO\tO\n",
      "God\tB-movie-pos\tB-movie-pos\n",
      "Bless\tI-movie-pos\tI-movie-pos\n",
      "America\tI-movie-pos\tI-movie-pos\n"
     ]
    }
   ],
   "source": [
    "all_true_labels_bukvi = [idx2tag[tag] for tag in all_true_labels]\n",
    "for token, pred_label, true_label in zip(test_sentences[5], all_predictions_list[5], test_labels[5]):\n",
    "    print(\"{}\\t{}\\t{}\".format(token, pred_label, idx2tag[true_label]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'roberta_best'\n",
    "\n",
    "dict_save = open(\"t2idx_roberta.json\", \"w\")\n",
    "json.dump(tag2idx, dict_save)\n",
    "dict_save.close()\n",
    "\n",
    "dict_save = open(\"idx2t_roberta.json\", \"w\")\n",
    "json.dump(idx2tag, dict_save)\n",
    "dict_save.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
