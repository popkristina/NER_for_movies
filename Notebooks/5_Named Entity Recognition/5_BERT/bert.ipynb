{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom tqdm import tqdm, trange\nimport string\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom keras.preprocessing.sequence import pad_sequences\n\nimport torch\nfrom torch import cuda\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\nimport transformers\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom transformers import BertTokenizer, BertConfig, BertForTokenClassification\nfrom transformers import RobertaTokenizer, RobertaForTokenClassification\nfrom transformers import XLMRobertaForTokenClassification, XLMRobertaTokenizer\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom seqeval.metrics import f1_score, accuracy_score, recall_score\n","metadata":{"execution":{"iopub.status.busy":"2022-01-05T18:04:35.469210Z","iopub.execute_input":"2022-01-05T18:04:35.469552Z","iopub.status.idle":"2022-01-05T18:04:35.485691Z","shell.execute_reply.started":"2022-01-05T18:04:35.469505Z","shell.execute_reply":"2022-01-05T18:04:35.484952Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def read_data():\n    #os.chdir('D:/TU_Graz/Thesis/Datasets/Reddit_features')\n    train = pd.read_csv(\"train_final_all.csv\")\n    test = pd.read_csv(\"test_final_all.csv\")\n    data = train.append(test)\n\n    return train, test, data","metadata":{"execution":{"iopub.status.busy":"2022-01-05T18:04:41.007669Z","iopub.execute_input":"2022-01-05T18:04:41.007928Z","iopub.status.idle":"2022-01-05T18:04:41.012375Z","shell.execute_reply.started":"2022-01-05T18:04:41.007899Z","shell.execute_reply":"2022-01-05T18:04:41.011662Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def data_stats(data):\n    frequencies = data.BIO.value_counts()\n    tags = {}\n    for tag, count in zip(frequencies.index, frequencies):\n        if tag != \"O\":\n            if tag[2:5] not in tags.keys():\n                tags[tag[2:5]] = count\n            else:\n                tags[tag[2:5]] += count\n        continue\n    \n    print(\"Number of tags: {}\".format(len(data.BIO.unique())))\n    print(\"Tag frequencies: {}\".format(frequencies))\n    print(\"Categories: \")\n    print(sorted(tags.items(), key=lambda x: x[1], reverse=True))","metadata":{"execution":{"iopub.status.busy":"2022-01-05T18:04:48.015805Z","iopub.execute_input":"2022-01-05T18:04:48.016059Z","iopub.status.idle":"2022-01-05T18:04:48.022725Z","shell.execute_reply.started":"2022-01-05T18:04:48.016031Z","shell.execute_reply":"2022-01-05T18:04:48.021742Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def group_sentences(data, category):\n    all_sents = []\n    sent_ids = data['Sent_id'].unique()\n    for curr_id in sent_ids:\n        tmp_df = data[data['Sent_id'] == curr_id]\n        tmp_df = pd.concat([tmp_df['Token'], tmp_df[\"Token_index\"], tmp_df.iloc[:,4:149], tmp_df[category]], axis = 1)\n        records = tmp_df.to_records(index=False)\n        all_sents.append(records)\n    return all_sents","metadata":{"execution":{"iopub.status.busy":"2022-01-05T18:04:49.279009Z","iopub.execute_input":"2022-01-05T18:04:49.279278Z","iopub.status.idle":"2022-01-05T18:04:49.284975Z","shell.execute_reply.started":"2022-01-05T18:04:49.279249Z","shell.execute_reply":"2022-01-05T18:04:49.284019Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def set_processor_params():\n    device = 'cuda' if cuda.is_available() else 'cpu'\n    n_gpu = torch.cuda.device_count()\n    torch.cuda.get_device_name(0)\n    return device, n_gpu","metadata":{"execution":{"iopub.status.busy":"2022-01-05T18:04:49.909514Z","iopub.execute_input":"2022-01-05T18:04:49.910056Z","iopub.status.idle":"2022-01-05T18:04:49.916697Z","shell.execute_reply.started":"2022-01-05T18:04:49.910024Z","shell.execute_reply":"2022-01-05T18:04:49.915991Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def set_session_params():\n    max_length = 300\n    bs = 16\n    epochs = 7\n    learning_rate = 1e-05\n    max_grad_norm = 10\n    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n    return max_length, bs, epochs, learning_rate, max_grad_norm, tokenizer","metadata":{"execution":{"iopub.status.busy":"2022-01-05T18:04:57.279424Z","iopub.execute_input":"2022-01-05T18:04:57.279686Z","iopub.status.idle":"2022-01-05T18:04:57.284779Z","shell.execute_reply.started":"2022-01-05T18:04:57.279657Z","shell.execute_reply":"2022-01-05T18:04:57.283838Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def remove_sents_over_threshold(sents, threshold):\n    sentences = list()\n    for s in sents:\n        if len(s) < threshold:\n            sentences.append(s)\n    return sentences","metadata":{"execution":{"iopub.status.busy":"2022-01-05T18:04:59.006061Z","iopub.execute_input":"2022-01-05T18:04:59.006896Z","iopub.status.idle":"2022-01-05T18:04:59.011167Z","shell.execute_reply.started":"2022-01-05T18:04:59.006852Z","shell.execute_reply":"2022-01-05T18:04:59.010367Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def tokenize(sentence, sentence_labels):\n    tokenized_sentence = []\n    labels = []\n    for word, label in zip(sentence, sentence_labels):\n        str_word = str(word)\n        tokenized_word = tokenizer.tokenize(str_word) # Tokenize the word\n        n_subwords = len(tokenized_word) # Count subwords\n        tokenized_sentence.extend(tokenized_word) # Add to the final tokenized list\n        labels.extend([label] * n_subwords) # Add the same label of the original word to all of its subwords\n    return tokenized_sentence, labels","metadata":{"execution":{"iopub.status.busy":"2022-01-05T18:05:00.070013Z","iopub.execute_input":"2022-01-05T18:05:00.070399Z","iopub.status.idle":"2022-01-05T18:05:00.075789Z","shell.execute_reply.started":"2022-01-05T18:05:00.070367Z","shell.execute_reply":"2022-01-05T18:05:00.074827Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def plot_learning_curves():\n    sns.set(style='darkgrid')\n    sns.set(font_scale=1.5)\n    plt.rcParams[\"figure.figsize\"] = (6,6)\n    plt.plot(loss_values, 'b-o', label=\"training loss\")\n    plt.plot(validation_loss_values, 'r-o', label=\"validation loss\")\n    plt.title(\"Learning curve\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-05T18:05:09.859101Z","iopub.execute_input":"2022-01-05T18:05:09.859354Z","iopub.status.idle":"2022-01-05T18:05:09.865596Z","shell.execute_reply.started":"2022-01-05T18:05:09.859325Z","shell.execute_reply":"2022-01-05T18:05:09.864404Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train, test, data = read_data() # We'll save the test set for later\n#data_stats(data)\ndevice, n_gpu = set_processor_params()\n\ntag_values = list(set(train[\"BIO\"].values))\ntag_values.append(\"PAD\")\ntag2idx = {t: i for i, t in enumerate(tag_values)}\nidx2tag = {i: t for i, t in enumerate(tag_values)}\n\nsents = group_sentences(data, 'BIO')\nsents = remove_sents_over_threshold(sents, 300)\nsentences = [[word[0] for word in sentence] for sentence in sents]\nlabels = [[tag2idx[w[len(w)-1]] for w in s] for s in sents]\ntrain_sents, test_sents, train_labels, test_labels = train_test_split(sentences, labels, test_size=0.25, shuffle = False)\n\nMAX_LEN, BATCH_SIZE, EPOCHS, LEARNING_RATE, MAX_GRAD_NORM, tokenizer = set_session_params()\n\ntokenized_texts_and_labels = [tokenize(sentence, sentence_labels) for sentence, sentence_labels in zip(train_sents, train_labels)]\n\ntokenized_texts = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels]\nlabels_subwords = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels]\n\n# Cut the token and label sequences to the max length\ninput_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts], maxlen = MAX_LEN, dtype=\"long\", value=0.0, \n                          truncating=\"post\", padding=\"post\")\ninput_tags = pad_sequences([[l for l in lab] for lab in labels_subwords], maxlen = MAX_LEN, value = tag2idx[\"PAD\"], \n                           padding=\"post\", dtype=\"long\", truncating=\"post\")\nattention_masks = [[float(i != 0.0) for i in ii] for ii in input_ids]\n\n# Train and validation split\ntr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, input_tags, test_size=0.2)\ntr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids, test_size=0.2)\ntr_inputs = torch.tensor(tr_inputs)\nval_inputs = torch.tensor(val_inputs)\ntr_tags = torch.tensor(tr_tags)\nval_tags = torch.tensor(val_tags)\ntr_masks = torch.tensor(tr_masks)\nval_masks = torch.tensor(val_masks)\n\ntrain_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)\nvalid_data = TensorDataset(val_inputs, val_masks, val_tags)\nvalid_sampler = SequentialSampler(valid_data)\nvalid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=BATCH_SIZE)\n\n# Pretrained model params\nmodel = RobertaForTokenClassification.from_pretrained('roberta-base', num_labels = len(tag2idx), output_attentions = False,output_hidden_states = False)\nmodel.cuda(); # Pass the model parameters to gpu\n\n# Set optimizer parameters\nparam_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'gamma', 'beta']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n     'weight_decay_rate': 0.01},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n     'weight_decay_rate': 0.0}]\noptimizer = AdamW(optimizer_grouped_parameters, lr=3e-5, eps=1e-8)\n\n# Total number of training steps is number of batches * number of epochs.\ntotal_steps = len(train_dataloader) * EPOCHS\n\n# Create the learning rate scheduler.\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)","metadata":{"execution":{"iopub.status.busy":"2022-01-05T18:05:11.261202Z","iopub.execute_input":"2022-01-05T18:05:11.261464Z","iopub.status.idle":"2022-01-05T18:06:34.063126Z","shell.execute_reply.started":"2022-01-05T18:05:11.261434Z","shell.execute_reply":"2022-01-05T18:06:34.062312Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"loss_values, validation_loss_values = [], []\naccuracy_values, validation_accuracy_values = [], []\n\nfor i in trange(EPOCHS, desc=\"Epoch\"):\n    \n    # TRAINING\n    # Perform one full pass over the training set\n    model.train() # Put the model into training mode\n    total_loss, total_accuracy = 0, 0 # Reset the total loss and acc. for current epoch\n    \n    # Training loop\n    for step, batch in enumerate(train_dataloader):\n        batch = tuple(t.to(device) for t in batch) # add batch to gpu\n        b_input_ids, b_input_mask, b_labels = batch # Input ids, mask and labels of the current batch\n        model.zero_grad() # Always clear any previously calculated gradients before performing a backward pass\n    \n        # Forward pass\n        # This will return the loss (rather than the model output) because we have provided the `labels`.\n        outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n        loss = outputs[0]\n        \n        # Perform a backward pass to calculate the gradients\n        loss.backward()\n        total_loss += loss.item() # track train loss\n        \n        # Clip the norm of the gradient to help prevent the exploding gradients problem\n        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=MAX_GRAD_NORM)\n        \n        optimizer.step() # update parameters\n        scheduler.step() # Update the learning rate\n        \n    avg_train_loss = total_loss / len(train_dataloader) # Calc. avg loss over training data\n    print(\"Average train loss: {}\".format(avg_train_loss))\n    loss_values.append(avg_train_loss) # Store the loss value for plotting the learning curve\n\n    # VALIDATION\n    # After the completion of each training epoch, measure performance on validation set  \n    model.eval() # Put the model into evaluation mode\n    eval_loss, eval_accuracy = 0, 0 # Reset the validation loss for current epoch\n    nb_eval_steps, nb_eval_examples = 0, 0\n    predictions , true_labels = [], []\n    \n    # Validation loop\n    for batch in valid_dataloader:\n        batch = tuple(t.to(device) for t in batch)\n        b_input_ids, b_input_mask, b_labels = batch\n    \n        # Telling the model not to compute or store gradients, to save memory and speed up validation\n        with torch.no_grad():\n            # Forward pass, calculate logit predictions\n            # This will return the logits rather than the loss because we have not provided labels\n            outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n            \n            #encoded_input = tokenizer(text, return_tensors='pt')\n            #output = model(**encoded_input)\n        \n        logits = outputs[1].detach().cpu().numpy() # Move logits to cpu\n        label_ids = b_labels.to('cpu').numpy() # Move labels to cpu\n        eval_loss += outputs[0].mean().item() # Valid. loss for current batch\n        \n        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n        true_labels.extend(label_ids)\n        \n    eval_loss = eval_loss / len(valid_dataloader)\n    validation_loss_values.append(eval_loss)\n    print(\"Validation loss: {}\".format(eval_loss))\n    \n    # Calculate the accuracy for this batch of test sentences\n    pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)\n                                 for p_i, l_i in zip(p, l) if tag_values[l_i] != \"PAD\"]\n    valid_tags = [tag_values[l_i] for l in true_labels\n                                  for l_i in l if tag_values[l_i] != \"PAD\"]\n    validation_accuracy_values.append(accuracy_score(pred_tags, valid_tags))\n    print(\"Validation Accuracy: {}\".format(accuracy_score(pred_tags, valid_tags)))","metadata":{"execution":{"iopub.status.busy":"2022-01-05T18:06:56.371782Z","iopub.execute_input":"2022-01-05T18:06:56.372075Z","iopub.status.idle":"2022-01-05T18:10:28.324244Z","shell.execute_reply.started":"2022-01-05T18:06:56.372032Z","shell.execute_reply":"2022-01-05T18:10:28.323518Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"plot_learning_curves()","metadata":{"execution":{"iopub.status.busy":"2022-01-05T18:10:52.696683Z","iopub.execute_input":"2022-01-05T18:10:52.696939Z","iopub.status.idle":"2022-01-05T18:10:52.959775Z","shell.execute_reply.started":"2022-01-05T18:10:52.696910Z","shell.execute_reply":"2022-01-05T18:10:52.959127Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"print(\"Prepare test set...\")\ntest_sentences = [[word for word in sentence] for sentence in test_sents]\n#test_sentences = [\" \".join(sentence) for sentence in test_sentences]\n#true_labels = [[tag for w in s] for s in test_labels]\n\nprint(\"Tokenize and predict...\")\nall_predictions = []\nall_true_labels = []\n\nfor lab in test_labels:\n    all_true_labels.extend(lab)\n    \nfor test_sentence in test_sentences:\n    tokenized_sentence = tokenizer.encode(test_sentence)\n    input_ids = torch.tensor([tokenized_sentence]).cuda()\n    with torch.no_grad():\n        output = model(input_ids)\n    label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)\n    label_indices = label_indices[0]\n    label_indices = label_indices[1:-1]\n    \n    tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n    new_tokens, new_labels = [], []\n    for token, label_idx in zip(tokens, label_indices):\n        if token.startswith(\"##\"):\n            new_tokens[-1] = new_tokens[-1] + token[2:]\n        else:\n            new_labels.append(tag_values[label_idx])\n            new_tokens.append(token)\n    all_predictions.extend(new_labels)\n    \nall_preds = [tag2idx[label] for label in all_predictions]\nreport = classification_report(all_true_labels, all_preds)\nprint(report)\n\nfor token, label, true in zip(test_sentences[0], new_labels, true_labels[0]):\n    print(\"{}\\t{}\\t{}\".format(token,label,idx2tag[true]))","metadata":{"execution":{"iopub.status.busy":"2022-01-05T18:11:01.178610Z","iopub.execute_input":"2022-01-05T18:11:01.178880Z","iopub.status.idle":"2022-01-05T18:11:04.444132Z","shell.execute_reply.started":"2022-01-05T18:11:01.178843Z","shell.execute_reply":"2022-01-05T18:11:04.443396Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}