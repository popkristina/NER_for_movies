{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-11-30T18:24:16.885066Z",
     "iopub.status.busy": "2021-11-30T18:24:16.884808Z",
     "iopub.status.idle": "2021-11-30T18:24:22.195373Z",
     "shell.execute_reply": "2021-11-30T18:24:22.194199Z",
     "shell.execute_reply.started": "2021-11-30T18:24:16.885041Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "#import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow_hub as hub\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.compat.v1.keras import backend as K\n",
    "\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import Concatenate, LSTM, TimeDistributed, Dense, BatchNormalization, Bidirectional, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T18:24:22.200329Z",
     "iopub.status.busy": "2021-11-30T18:24:22.199724Z",
     "iopub.status.idle": "2021-11-30T18:24:22.206167Z",
     "shell.execute_reply": "2021-11-30T18:24:22.205246Z",
     "shell.execute_reply.started": "2021-11-30T18:24:22.200290Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    os.chdir('D:/TU_Graz/Thesis/Datasets/Reddit_features')\n",
    "    train = pd.read_csv(\"train_final_all.csv\")\n",
    "    test = pd.read_csv(\"test_final_all.csv\")\n",
    "    data = train.append(test)\n",
    "\n",
    "    return train, test, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T18:24:22.208600Z",
     "iopub.status.busy": "2021-11-30T18:24:22.208036Z",
     "iopub.status.idle": "2021-11-30T18:24:22.227834Z",
     "shell.execute_reply": "2021-11-30T18:24:22.226805Z",
     "shell.execute_reply.started": "2021-11-30T18:24:22.208561Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_lists(data_df, word_category, class_category):\n",
    "    words = list(set(data_df[word_category].values))\n",
    "    words.append(\"ENDPAD\")\n",
    "    n_words = len(words)\n",
    "    tags = list(set(data_df[class_category].values))\n",
    "    n_tags = len(tags)\n",
    "    return words, n_words, tags, n_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T18:28:03.660773Z",
     "iopub.status.busy": "2021-11-30T18:28:03.660435Z",
     "iopub.status.idle": "2021-11-30T18:28:03.666532Z",
     "shell.execute_reply": "2021-11-30T18:28:03.665742Z",
     "shell.execute_reply.started": "2021-11-30T18:28:03.660744Z"
    }
   },
   "outputs": [],
   "source": [
    "def group_sentences(data, sent_identificator, category):\n",
    "    all_sents = []\n",
    "    sent_ids = data[sent_identificator].unique()\n",
    "    for curr_id in sent_ids:\n",
    "        tmp_df = data[data[sent_identificator] == curr_id]\n",
    "        tmp_df = pd.concat([tmp_df['Token'], tmp_df[\"Token_index\"], tmp_df.iloc[:,4:44], tmp_df[category]], axis = 1) \n",
    "        records = tmp_df.to_records(index=False)\n",
    "        all_sents.append(records)\n",
    "        \n",
    "    return all_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T18:28:05.018602Z",
     "iopub.status.busy": "2021-11-30T18:28:05.018280Z",
     "iopub.status.idle": "2021-11-30T18:28:05.023397Z",
     "shell.execute_reply": "2021-11-30T18:28:05.022145Z",
     "shell.execute_reply.started": "2021-11-30T18:28:05.018573Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_sents_over_threshold(sents, threshold):\n",
    "    sentences = list()\n",
    "    for s in sents:\n",
    "        if len(s) < threshold:\n",
    "            sentences.append(s)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T18:28:45.882032Z",
     "iopub.status.busy": "2021-11-30T18:28:45.881694Z",
     "iopub.status.idle": "2021-11-30T18:28:45.891638Z",
     "shell.execute_reply": "2021-11-30T18:28:45.890775Z",
     "shell.execute_reply.started": "2021-11-30T18:28:45.882000Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_and_pad(sentences, max_len, tag2idx):\n",
    "    X1 = [[w[0] for w in s] for s in sentences]\n",
    "    new_X = []\n",
    "    for seq in X1:\n",
    "        new_seq = []\n",
    "        for i in range(max_len):\n",
    "            try:\n",
    "                new_seq.append(seq[i])\n",
    "            except:\n",
    "                new_seq.append(\"__PAD__\")\n",
    "        new_X.append(new_seq)\n",
    "    X1 = new_X\n",
    "    \n",
    "    X2 = []\n",
    "    for sentence in sentences:\n",
    "        sent_ft = list()\n",
    "        for word in sentence:\n",
    "            ft = [word[i] for i in range(1, 41)]\n",
    "            sent_ft.append(ft)\n",
    "        for j in range(len(sentence)-1, max_len-1):\n",
    "            ft = [0] * 40\n",
    "            sent_ft.append(ft)\n",
    "        X2.append(sent_ft)  \n",
    "        \n",
    "    y = [[tag2idx[w[len(w)-1]] for w in s] for s in sentences]\n",
    "    y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tag2idx[\"O\"])\n",
    "    \n",
    "    return X1, X2, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T18:28:49.075032Z",
     "iopub.status.busy": "2021-11-30T18:28:49.074687Z",
     "iopub.status.idle": "2021-11-30T18:28:49.079526Z",
     "shell.execute_reply": "2021-11-30T18:28:49.078688Z",
     "shell.execute_reply.started": "2021-11-30T18:28:49.074999Z"
    }
   },
   "outputs": [],
   "source": [
    "def ElmoEmbedding(x):\n",
    "    return elmo_model(inputs={\"tokens\": tf.squeeze(tf.cast(x, tf.string)),\n",
    "                              \"sequence_len\": tf.constant(batch_size*[max_len])},\n",
    "                      signature=\"tokens\",\n",
    "                      as_dict=True)[\"elmo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T18:29:10.071648Z",
     "iopub.status.busy": "2021-11-30T18:29:10.071265Z",
     "iopub.status.idle": "2021-11-30T18:29:10.079088Z",
     "shell.execute_reply": "2021-11-30T18:29:10.078231Z",
     "shell.execute_reply.started": "2021-11-30T18:29:10.071615Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model(max_len, n_words, n_tags): \n",
    "    word_input_layer = Input(shape=(max_len, 40, ))\n",
    "    elmo_input_layer = Input(shape=(max_len,), dtype=tf.string)\n",
    "    \n",
    "    word_output_layer = Dense(n_tags, activation = 'softmax')(word_input_layer)\n",
    "    elmo_output_layer = Lambda(ElmoEmbedding, output_shape=(None, 1024))(elmo_input_layer)\n",
    "    \n",
    "    output_layer = Concatenate()([word_output_layer, elmo_output_layer])\n",
    "    output_layer = BatchNormalization()(output_layer)\n",
    "    output_layer = Bidirectional(LSTM(units=512, return_sequences=True, recurrent_dropout=0.2, dropout=0.2))(output_layer)\n",
    "    output_layer = TimeDistributed(Dense(n_tags, activation='softmax'))(output_layer)\n",
    "    \n",
    "    model = Model([elmo_input_layer, word_input_layer], output_layer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T18:29:16.582058Z",
     "iopub.status.busy": "2021-11-30T18:29:16.581715Z",
     "iopub.status.idle": "2021-11-30T18:29:16.586977Z",
     "shell.execute_reply": "2021-11-30T18:29:16.586134Z",
     "shell.execute_reply.started": "2021-11-30T18:29:16.582027Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_learning_curves(hist, curve1, curve2):\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.plot(hist[curve1])\n",
    "    plt.plot(hist[curve2])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the data...\n",
      "Creating sets of words and tags...\n",
      "Creating sentence list...\n",
      "Creating word and tag maps...\n",
      "Preparing and padding training data...\n",
      "Splitting data...\n",
      "Setting parameters...\n",
      "Building the model...\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Kiki\\anaconda3\\lib\\site-packages\\keras\\layers\\normalization\\batch_normalization.py:520: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Kiki\\anaconda3\\lib\\site-packages\\keras\\layers\\normalization\\batch_normalization.py:520: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 300, 40)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 300)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 300, 17)      697         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda (Lambda)                 (32, None, 1024)     0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (32, 300, 1041)      0           dense[0][0]                      \n",
      "                                                                 lambda[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (32, 300, 1041)      4164        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (32, 300, 1024)      6365184     batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (32, 300, 17)        17425       bidirectional[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 6,387,470\n",
      "Trainable params: 6,385,388\n",
      "Non-trainable params: 2,082\n",
      "__________________________________________________________________________________________________\n",
      "Train on 928 samples, validate on 224 samples\n",
      "Epoch 1/2\n",
      "416/928 [============>.................] - ETA: 12:26 - loss: 0.5465 - accuracy: 0.8661"
     ]
    }
   ],
   "source": [
    "max_len = 300\n",
    "batch_size = 32\n",
    "#batch_size = 1\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "print(\"Loading the data...\")\n",
    "train, test, data = read_data()\n",
    "\n",
    "################################## TRAIN ################################\n",
    "\n",
    "print(\"Creating sets of words and tags...\")\n",
    "words, n_words, tags, n_tags = create_lists(train, \"Token\", \"BIO\")\n",
    "\n",
    "print(\"Creating sentence list...\")\n",
    "sents = group_sentences(train, \"Sent_id\", \"BIO\")\n",
    "sentences = [s for s in sents if len(s) < max_len]\n",
    "\n",
    "print(\"Creating word and tag maps...\")\n",
    "tag2idx = {t: i for i, t in enumerate(tags)}\n",
    "\n",
    "print(\"Preparing and padding training data...\")\n",
    "X1, X2, y = prepare_and_pad(sentences, max_len, tag2idx)\n",
    "\n",
    "print(\"Splitting data...\")\n",
    "X1_train, X1_valid, y_train, y_valid = train_test_split(X1, y, test_size=0.2, random_state=2021)\n",
    "X2_train, X2_valid, y_train, y_valid = train_test_split(X2, y, test_size=0.2, random_state=2021)\n",
    "X1_train = X1_train[:(len(X1_train) // batch_size) * batch_size]\n",
    "X2_train = X2_train[:(len(X2_train) // batch_size) * batch_size]\n",
    "X1_valid = X1_valid[:(len(X1_valid) // batch_size) * batch_size]\n",
    "X2_valid = X2_valid[:(len(X2_valid) // batch_size) * batch_size]\n",
    "\n",
    "y_train = y_train[:(len(y_train) // batch_size) * batch_size]\n",
    "y_valid = y_valid[:(len(y_valid) // batch_size) * batch_size] \n",
    "y_train = y_train.reshape(y_train.shape[0], y_train.shape[1], 1)\n",
    "y_valid = y_valid.reshape(y_valid.shape[0], y_valid.shape[1], 1)\n",
    "\n",
    "print(\"Setting parameters...\")\n",
    "tf.disable_eager_execution()\n",
    "elmo_model = hub.Module(\"C:/Users/Kiki/Projects/ner_movies/Scripts/module_elmo3\", trainable=True)\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)\n",
    "sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "\n",
    "print(\"Building the model...\")\n",
    "model = build_model(max_len, n_words, n_tags)\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit([np.array(X1_train), np.array(X2_train).reshape((len(X2_train), max_len, 40))],\n",
    "                    y_train,\n",
    "                    validation_data=([np.array(X1_valid), np.array(X2_valid).reshape((len(X2_valid), max_len, 40))], y_valid),\n",
    "                    batch_size=batch_size, epochs=2, verbose=1)\n",
    "#saver.save(sess, 'utilities/my_test_model', global_step=1000)\n",
    "hist = pd.DataFrame(history.history)\n",
    "\n",
    "#print(\"Load a previously saved model...\")\n",
    "#model = load_model(\"../input/trainedmodel/finalmodel.h5\")\n",
    "#reconstructed = tf.keras.models.load_model('../input/tranedmodel/finalmodel.h5')\n",
    "\n",
    "print(\"Plotting learning curves...\")\n",
    "plot_learning_curves(hist, \"acc\", \"val_acc\")\n",
    "plot_learning_curves(hist, \"loss\", \"val_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T18:54:15.241723Z",
     "iopub.status.busy": "2021-11-30T18:54:15.241376Z",
     "iopub.status.idle": "2021-11-30T18:54:30.711069Z",
     "shell.execute_reply": "2021-11-30T18:54:30.709523Z",
     "shell.execute_reply.started": "2021-11-30T18:54:15.241691Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict([X1_te, np.array(X2_te).reshape((len(X2_te), max_len, 40))])\n",
    "p = np.argmax(y_pred, axis=-1)\n",
    "y_orig = []\n",
    "for sent in y_te:\n",
    "    for tag in sent:\n",
    "        y_orig.append(tag)\n",
    "y_preds = []\n",
    "for sent in p:\n",
    "    for tag in sent:\n",
    "        y_preds.append(tag)\n",
    "        \n",
    "report = classification_report( y_orig, y_preds )\n",
    "print(report) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T18:55:35.297513Z",
     "iopub.status.busy": "2021-11-30T18:55:35.297180Z",
     "iopub.status.idle": "2021-11-30T18:55:35.304227Z",
     "shell.execute_reply": "2021-11-30T18:55:35.303290Z",
     "shell.execute_reply.started": "2021-11-30T18:55:35.297483Z"
    }
   },
   "outputs": [],
   "source": [
    "tag2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
