{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-11-18T19:41:42.222199Z",
     "iopub.status.busy": "2021-11-18T19:41:42.221801Z",
     "iopub.status.idle": "2021-11-18T19:41:47.480707Z",
     "shell.execute_reply": "2021-11-18T19:41:47.479825Z",
     "shell.execute_reply.started": "2021-11-18T19:41:42.222109Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow_hub as hub\n",
    "#from keras import backend as K\n",
    "from tensorflow.compat.v1.keras import backend as K\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from keras.models import Model, Input\n",
    "from keras.layers.merge import add\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-18T19:41:47.482598Z",
     "iopub.status.busy": "2021-11-18T19:41:47.482282Z",
     "iopub.status.idle": "2021-11-18T19:41:47.489589Z",
     "shell.execute_reply": "2021-11-18T19:41:47.488695Z",
     "shell.execute_reply.started": "2021-11-18T19:41:47.482564Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    #os.chdir('D:/TU_Graz/Thesis/Datasets/Reddit_features')\n",
    "    \n",
    "    train = pd.read_csv(\"../input/bio-tagged/train_final_all.csv\")\n",
    "    test = pd.read_csv(\"../input/bio-tagged/test_final_all.csv\")\n",
    "    \n",
    "    #train = pd.read_csv(\"train_final_all.csv\")\n",
    "    #test = pd.read_csv(\"test_final_all.csv\")\n",
    "    data = train.append(test)\n",
    "\n",
    "    return train, test, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-18T19:41:51.579976Z",
     "iopub.status.busy": "2021-11-18T19:41:51.579642Z",
     "iopub.status.idle": "2021-11-18T19:41:51.587413Z",
     "shell.execute_reply": "2021-11-18T19:41:51.586580Z",
     "shell.execute_reply.started": "2021-11-18T19:41:51.579946Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_lists(data, category):\n",
    "    words = list(set(data[\"Token\"].values))\n",
    "    words.append(\"ENDPAD\")\n",
    "    n_words = len(words)\n",
    "    tags = list(set(data[\"BIO\"].values))\n",
    "    n_tags = len(tags)\n",
    "\n",
    "    return words, n_words, tags, n_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-18T19:41:52.111386Z",
     "iopub.status.busy": "2021-11-18T19:41:52.110965Z",
     "iopub.status.idle": "2021-11-18T19:41:52.117296Z",
     "shell.execute_reply": "2021-11-18T19:41:52.116437Z",
     "shell.execute_reply.started": "2021-11-18T19:41:52.111352Z"
    }
   },
   "outputs": [],
   "source": [
    "def group_sentences(data, category):\n",
    "    all_sents = []\n",
    "    sent_ids = data['Sent_id'].unique()\n",
    "    for curr_id in sent_ids:\n",
    "        tmp_df = data[data['Sent_id'] == curr_id]\n",
    "        tmp_df = pd.concat([tmp_df['Token'], tmp_df[\"Token_index\"], tmp_df.iloc[:,4:149], tmp_df[category]], axis = 1)\n",
    "        records = tmp_df.to_records(index=False)\n",
    "        all_sents.append(records)\n",
    "    return all_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-18T19:41:55.102560Z",
     "iopub.status.busy": "2021-11-18T19:41:55.102220Z",
     "iopub.status.idle": "2021-11-18T19:41:55.107690Z",
     "shell.execute_reply": "2021-11-18T19:41:55.106734Z",
     "shell.execute_reply.started": "2021-11-18T19:41:55.102527Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_sents_over_threshold(sents, threshold):\n",
    "    sentences = list()\n",
    "    for s in sents:\n",
    "        if len(s) < threshold:\n",
    "            sentences.append(s)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-18T19:41:55.421086Z",
     "iopub.status.busy": "2021-11-18T19:41:55.420706Z",
     "iopub.status.idle": "2021-11-18T19:41:55.428204Z",
     "shell.execute_reply": "2021-11-18T19:41:55.427102Z",
     "shell.execute_reply.started": "2021-11-18T19:41:55.421049Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_and_pad(sentences, max_len, tag2idx):\n",
    "    \n",
    "    X = [[w[0] for w in s] for s in sentences]\n",
    "\n",
    "    new_X = []\n",
    "    for seq in X:\n",
    "        new_seq = []\n",
    "        for i in range(max_len):\n",
    "            try:\n",
    "                new_seq.append(seq[i])\n",
    "            except:\n",
    "                new_seq.append(\"__PAD__\")\n",
    "        new_X.append(new_seq)\n",
    "    X = new_X\n",
    "    \n",
    "    y = [[tag2idx[w[len(w)-1]] for w in s] for s in sentences]\n",
    "    y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tag2idx[\"O\"])\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-18T19:41:56.300947Z",
     "iopub.status.busy": "2021-11-18T19:41:56.300627Z",
     "iopub.status.idle": "2021-11-18T19:41:56.306220Z",
     "shell.execute_reply": "2021-11-18T19:41:56.305305Z",
     "shell.execute_reply.started": "2021-11-18T19:41:56.300917Z"
    }
   },
   "outputs": [],
   "source": [
    "def ElmoEmbedding(x):\n",
    "    return elmo_model(inputs={\"tokens\": tf.squeeze(tf.cast(x, tf.string)),\n",
    "                              \"sequence_len\": tf.constant(batch_size*[max_len])},\n",
    "                      signature=\"tokens\",\n",
    "                      as_dict=True)[\"elmo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-18T19:41:56.492741Z",
     "iopub.status.busy": "2021-11-18T19:41:56.492423Z",
     "iopub.status.idle": "2021-11-18T19:41:56.499305Z",
     "shell.execute_reply": "2021-11-18T19:41:56.498181Z",
     "shell.execute_reply.started": "2021-11-18T19:41:56.492711Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model(max_len): \n",
    "    input_seq = Input(shape=(max_len,), dtype=tf.string)\n",
    "    elmo = Lambda(ElmoEmbedding, output_shape=(None, 1024))(input_seq)\n",
    "    bilstm_1 = Bidirectional(LSTM(units=512, return_sequences=True, recurrent_dropout=0.2, dropout=0.2))(elmo)\n",
    "    bilstm_2 = Bidirectional(LSTM(units=512, return_sequences=True, recurrent_dropout=0.2, dropout=0.2))(bilstm_1)\n",
    "    x = add([bilstm_1, bilstm_2])  # residual connection to the first biLSTM\n",
    "    out = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(x)\n",
    "\n",
    "    model = Model(input_seq, out)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-18T19:41:58.971682Z",
     "iopub.status.busy": "2021-11-18T19:41:58.971370Z",
     "iopub.status.idle": "2021-11-18T19:41:58.976850Z",
     "shell.execute_reply": "2021-11-18T19:41:58.975830Z",
     "shell.execute_reply.started": "2021-11-18T19:41:58.971654Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_learning_curves(hist, curve1, curve2):\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.plot(hist[curve1])\n",
    "    plt.plot(hist[curve2])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-18T19:42:06.274924Z",
     "iopub.status.busy": "2021-11-18T19:42:06.274612Z",
     "iopub.status.idle": "2021-11-18T20:02:00.598239Z",
     "shell.execute_reply": "2021-11-18T20:02:00.597253Z",
     "shell.execute_reply.started": "2021-11-18T19:42:06.274896Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read the data\n",
    "print(\"Loading the data...\")\n",
    "train, test, data = read_data()\n",
    "print(\"Done.\")\n",
    "\n",
    "# Create word, tag and char lists\n",
    "print(\"Creating sets of words and tags...\")\n",
    "words, n_words, tags, n_tags = create_lists(data, \"BIO\")\n",
    "print(\"Done.\")\n",
    "\n",
    "# Create list of sentences\n",
    "print(\"Creating sentence list...\")\n",
    "sents = group_sentences(data, 'BIO')\n",
    "print(\"Done.\")\n",
    "\n",
    "# Remove submissions longer than a certain threshold\n",
    "print(\"Removing submissions longer than threshold...\")\n",
    "sentences = remove_sents_over_threshold(sents, 300)\n",
    "print(\"Done.\")\n",
    "\n",
    "# Create word and tag maps\n",
    "print(\"Creating word and tag maps...\")\n",
    "max_len = 300\n",
    "tag2idx = {t: i for i, t in enumerate(tags)}\n",
    "print(\"Done.\")\n",
    "\n",
    "# Pad data\n",
    "print(\"Preparing and padding training data...\")\n",
    "X, y = prepare_and_pad(sentences, max_len, tag2idx)\n",
    "print(\"Done\")\n",
    "\n",
    "# Split to train and test\n",
    "print(\"Splitting data...\")\n",
    "#X_tr = X[0:1185]\n",
    "#y_tr = y[0:1185]\n",
    "#X_te = X[1186:]\n",
    "#y_te = y[1186:]\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.22, shuffle=False)\n",
    "print(\"Done.\")\n",
    "\n",
    "# Setting parameters\n",
    "print(\"Setting parameters...\")\n",
    "batch_size = 32\n",
    "plt.style.use(\"ggplot\")\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "sess = tf.compat.v1.Session()\n",
    "K.set_session(sess)\n",
    "elmo_model = hub.Module(\"https://tfhub.dev/google/elmo/3\", trainable=True)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.tables_initializer())\n",
    "print(\"Done.\")\n",
    "\n",
    "# Build the model\n",
    "print(\"Building the model...\")\n",
    "model = build_model(max_len)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "print(\"Done.\")\n",
    "\n",
    "# Print the model\n",
    "print(\"Print the model...\")\n",
    "plot_model(model,'Elmo_tag.png',show_shapes= True)\n",
    "print(\"Done.\")\n",
    "\n",
    "# Split the train data to train and validation data\n",
    "print(\"Split to train and validation data...\")\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X_tr, y_tr, test_size=0.2, random_state=2021)\n",
    "X_tr = X_tr[:(len(X_tr)//batch_size) * batch_size]\n",
    "X_val = X_val[:(len(X_val)//batch_size) * batch_size]\n",
    "y_tr = y_tr[:(len(y_tr)//batch_size) * batch_size] \n",
    "y_val = y_val[:(len(y_val)//batch_size) * batch_size] \n",
    "##X_tr, X_val = X_tr[:29*batch_size], X_tr[-7*batch_size:]\n",
    "#y_tr, y_val = y_tr[:29*batch_size], y_tr[-7*batch_size:]\n",
    "y_tr = y_tr.reshape(y_tr.shape[0], y_tr.shape[1], 1)\n",
    "y_val = y_val.reshape(y_val.shape[0], y_val.shape[1], 1)\n",
    "print(\"Done.\")\n",
    "\n",
    "history = model.fit(np.array(X_tr), y_tr, validation_data=(np.array(X_val), y_val),\n",
    "                    batch_size=batch_size, epochs=10, verbose=1)\n",
    "\n",
    "#history = model.fit(np.array(X_tr), y_tr, batch_size=24, epochs=5, validation_split=0.3, verbose=1)\n",
    "hist = pd.DataFrame(history.history)\n",
    "\n",
    "# Plotting learning curves\n",
    "print(\"Plotting learning curves...\")\n",
    "plot_learning_curves(hist, \"accuracy\", \"val_accuracy\")\n",
    "plot_learning_curves(hist, \"loss\", \"val_loss\")\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-18T20:02:30.842043Z",
     "iopub.status.busy": "2021-11-18T20:02:30.841684Z",
     "iopub.status.idle": "2021-11-18T20:02:49.910687Z",
     "shell.execute_reply": "2021-11-18T20:02:49.909829Z",
     "shell.execute_reply.started": "2021-11-18T20:02:30.841990Z"
    }
   },
   "outputs": [],
   "source": [
    "p = model.predict(np.array(X_te))\n",
    "#p = model.predict(np.array(X_te[i:i+batch_size]))[0]\n",
    "p = np.argmax(p, axis=-1)\n",
    "#y_te = y_te[0:192]\n",
    "\n",
    "y_orig = []\n",
    "for sent in y_te:\n",
    "    for tag in sent:\n",
    "        y_orig.append(tag)\n",
    "        \n",
    "y_preds = []\n",
    "for sent in p:\n",
    "    for tag in sent:\n",
    "        y_preds.append(tag)\n",
    "    \n",
    "report = classification_report( y_orig, y_preds )\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-18T20:22:41.510945Z",
     "iopub.status.busy": "2021-11-18T20:22:41.510476Z",
     "iopub.status.idle": "2021-11-18T20:22:41.522715Z",
     "shell.execute_reply": "2021-11-18T20:22:41.521853Z",
     "shell.execute_reply.started": "2021-11-18T20:22:41.510901Z"
    }
   },
   "outputs": [],
   "source": [
    "tag2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-18T20:22:51.661114Z",
     "iopub.status.busy": "2021-11-18T20:22:51.660776Z",
     "iopub.status.idle": "2021-11-18T20:22:54.121052Z",
     "shell.execute_reply": "2021-11-18T20:22:54.120112Z",
     "shell.execute_reply.started": "2021-11-18T20:22:51.661082Z"
    }
   },
   "outputs": [],
   "source": [
    "i = 1\n",
    "p = model.predict(np.array(X_te[i:i+batch_size]))[0]\n",
    "p = np.argmax(p, axis=-1)\n",
    "print(\"{:15} {:5}: ({})\".format(\"Word\", \"Pred\", \"True\"))\n",
    "print(\"=\"*30)\n",
    "for w, true, pred in zip(X_te[i], y_te[i], p):\n",
    "    if w != \"__PAD__\":\n",
    "        print(\"{:15}:{:5} ({})\".format(w, tags[pred], tags[true]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-18T00:03:33.613759Z",
     "iopub.status.busy": "2021-11-18T00:03:33.613425Z",
     "iopub.status.idle": "2021-11-18T00:03:35.435509Z",
     "shell.execute_reply": "2021-11-18T00:03:35.434519Z",
     "shell.execute_reply.started": "2021-11-18T00:03:33.613728Z"
    }
   },
   "outputs": [],
   "source": [
    "i = 19\n",
    "p = model.predict(np.array(X_te[i:i+batch_size]))[0]\n",
    "p = np.argmax(p, axis=-1)\n",
    "print(\"{:15} {:5}: ({})\".format(\"Word\", \"Pred\", \"True\"))\n",
    "print(\"=\"*30)\n",
    "for w, true, pred in zip(X_te[i], y_te[i], p):\n",
    "    if w != \"__PAD__\":\n",
    "        print(\"{:15}:{:5} ({})\".format(w, tags[pred], tags[true]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
