{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-11-30T18:24:16.885066Z",
     "iopub.status.busy": "2021-11-30T18:24:16.884808Z",
     "iopub.status.idle": "2021-11-30T18:24:22.195373Z",
     "shell.execute_reply": "2021-11-30T18:24:22.194199Z",
     "shell.execute_reply.started": "2021-11-30T18:24:16.885041Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow_hub as hub\n",
    "#from keras import backend as K\n",
    "from tensorflow.compat.v1.keras import backend as K\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "from keras.models import Model, Input\n",
    "from keras.layers.merge import add\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Conv1D, Concatenate\n",
    "from keras.layers import Bidirectional, concatenate, SpatialDropout1D, GlobalMaxPooling1D\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, BatchNormalization, Bidirectional, Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T18:24:22.200329Z",
     "iopub.status.busy": "2021-11-30T18:24:22.199724Z",
     "iopub.status.idle": "2021-11-30T18:24:22.206167Z",
     "shell.execute_reply": "2021-11-30T18:24:22.205246Z",
     "shell.execute_reply.started": "2021-11-30T18:24:22.200290Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    #os.chdir('D:/TU_Graz/Thesis/Datasets/Reddit_features')\n",
    "    \n",
    "    train = pd.read_csv(\"../input/bio-tagged/train_final_all.csv\")\n",
    "    test = pd.read_csv(\"../input/bio-tagged/test_final_all.csv\")\n",
    "    \n",
    "    #train = pd.read_csv(\"train_final_all.csv\")\n",
    "    #test = pd.read_csv(\"test_final_all.csv\")\n",
    "    data = train.append(test)\n",
    "\n",
    "    return train, test, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T18:24:22.208600Z",
     "iopub.status.busy": "2021-11-30T18:24:22.208036Z",
     "iopub.status.idle": "2021-11-30T18:24:22.227834Z",
     "shell.execute_reply": "2021-11-30T18:24:22.226805Z",
     "shell.execute_reply.started": "2021-11-30T18:24:22.208561Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_lists(data, category):\n",
    "    words = list(set(data[\"Token\"].values))\n",
    "    words.append(\"ENDPAD\")\n",
    "    n_words = len(words)\n",
    "    tags = list(set(data[\"BIO\"].values))\n",
    "    n_tags = len(tags)\n",
    "\n",
    "    return words, n_words, tags, n_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T18:28:03.660773Z",
     "iopub.status.busy": "2021-11-30T18:28:03.660435Z",
     "iopub.status.idle": "2021-11-30T18:28:03.666532Z",
     "shell.execute_reply": "2021-11-30T18:28:03.665742Z",
     "shell.execute_reply.started": "2021-11-30T18:28:03.660744Z"
    }
   },
   "outputs": [],
   "source": [
    "def group_sentences(data, category):\n",
    "    all_sents = []\n",
    "    sent_ids = data['Sent_id'].unique()\n",
    "    for curr_id in sent_ids:\n",
    "        tmp_df = data[data['Sent_id'] == curr_id]\n",
    "        tmp_df = pd.concat([tmp_df['Token'], tmp_df[\"Token_index\"], tmp_df.iloc[:,4:44], tmp_df[category]], axis = 1)\n",
    "        #tmp_df = pd.concat([tmp_df['Token'], tmp_df[\"Token_index\"], tmp_df.iloc[:,4:149], tmp_df[category]], axis = 1)\n",
    "        records = tmp_df.to_records(index=False)\n",
    "        all_sents.append(records)\n",
    "    return all_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T18:28:05.018602Z",
     "iopub.status.busy": "2021-11-30T18:28:05.018280Z",
     "iopub.status.idle": "2021-11-30T18:28:05.023397Z",
     "shell.execute_reply": "2021-11-30T18:28:05.022145Z",
     "shell.execute_reply.started": "2021-11-30T18:28:05.018573Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_sents_over_threshold(sents, threshold):\n",
    "    sentences = list()\n",
    "    for s in sents:\n",
    "        if len(s) < threshold:\n",
    "            sentences.append(s)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T18:28:45.882032Z",
     "iopub.status.busy": "2021-11-30T18:28:45.881694Z",
     "iopub.status.idle": "2021-11-30T18:28:45.891638Z",
     "shell.execute_reply": "2021-11-30T18:28:45.890775Z",
     "shell.execute_reply.started": "2021-11-30T18:28:45.882000Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_and_pad(sentences, max_len, tag2idx):\n",
    "    \n",
    "    X1 = [[w[0] for w in s] for s in sentences]\n",
    "    \n",
    "    new_X = []\n",
    "    for seq in X1:\n",
    "        new_seq = []\n",
    "        for i in range(max_len):\n",
    "            try:\n",
    "                new_seq.append(seq[i])\n",
    "            except:\n",
    "                new_seq.append(\"__PAD__\")\n",
    "        new_X.append(new_seq)\n",
    "    X1 = new_X\n",
    "    \n",
    "    X2 = []\n",
    "    for sentence in sentences:\n",
    "        sent_ft = list()\n",
    "        for word in sentence:\n",
    "            ft = list()\n",
    "            #for i in range(1, 147):\n",
    "            for i in range(1, 41):\n",
    "                ft.append(word[i])\n",
    "            sent_ft.append(ft)\n",
    "        for j in range(len(sentence)-1, max_len-1):\n",
    "            ft = list()\n",
    "            #for i in range(1, 147):\n",
    "            for i in range(1, 41):\n",
    "                ft.append(0)\n",
    "            sent_ft.append(ft)\n",
    "        X2.append(sent_ft)  \n",
    "        \n",
    "    y = [[tag2idx[w[len(w)-1]] for w in s] for s in sentences]\n",
    "    y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tag2idx[\"O\"])\n",
    "    \n",
    "    return X1, X2, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T18:28:49.075032Z",
     "iopub.status.busy": "2021-11-30T18:28:49.074687Z",
     "iopub.status.idle": "2021-11-30T18:28:49.079526Z",
     "shell.execute_reply": "2021-11-30T18:28:49.078688Z",
     "shell.execute_reply.started": "2021-11-30T18:28:49.074999Z"
    }
   },
   "outputs": [],
   "source": [
    "def ElmoEmbedding(x):\n",
    "    return elmo_model(inputs={\"tokens\": tf.squeeze(tf.cast(x, tf.string)),\n",
    "                              \"sequence_len\": tf.constant(batch_size*[max_len])},\n",
    "                      signature=\"tokens\",\n",
    "                      as_dict=True)[\"elmo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T18:29:10.071648Z",
     "iopub.status.busy": "2021-11-30T18:29:10.071265Z",
     "iopub.status.idle": "2021-11-30T18:29:10.079088Z",
     "shell.execute_reply": "2021-11-30T18:29:10.078231Z",
     "shell.execute_reply.started": "2021-11-30T18:29:10.071615Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model(max_len, n_words, n_tags): \n",
    "    \n",
    "    # Input Layers\n",
    "    word_input_layer = Input(shape=(max_len, 40))\n",
    "    #word_input_layer = Input(shape=(max_len, 146))\n",
    "    elmo_input_layer = Input(shape=(max_len,), dtype=tf.string)\n",
    "    word_output_layer = Dense(n_tags, activation = 'softmax')(word_input_layer)\n",
    "    elmo_output_layer = Lambda(ElmoEmbedding, output_shape=(None, 1024))(elmo_input_layer)\n",
    "    output_layer = Concatenate()([word_output_layer, elmo_output_layer])\n",
    "    output_layer = BatchNormalization()(output_layer)\n",
    "    output_layer = Bidirectional(LSTM(units=512, return_sequences=True, recurrent_dropout=0.2, dropout=0.2))(output_layer)\n",
    "    output_layer = Dense(n_tags, activation='softmax')(output_layer)\n",
    "\n",
    "    #input_text = Input(shape=(max_len,), dtype=tf.string)\n",
    "    #embedding = Lambda(ElmoEmbedding, output_shape=(None, 1024))(input_text)\n",
    "    #x = Bidirectional(LSTM(units=512, return_sequences=True, recurrent_dropout=0.2, dropout=0.2))(embedding)\n",
    "    #x_rnn = Bidirectional(LSTM(units=512, return_sequences=True, recurrent_dropout=0.2, dropout=0.2))(x)\n",
    "    #x = add([x, x_rnn])  # residual connection to the first biLSTM\n",
    "    #out_tmp = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(x)\n",
    "    \n",
    "    # main LSTM\n",
    "    #x_new = concatenate([out_tmp, fts])\n",
    "    #x_sp = SpatialDropout1D(0.1)(x_new)\n",
    "    #main_lstm = Bidirectional(LSTM(units=200, return_sequences=True, recurrent_dropout=0.3))(x_sp)\n",
    "    #out = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(main_lstm)\n",
    "    model = Model([elmo_input_layer, word_input_layer], output_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T18:29:16.582058Z",
     "iopub.status.busy": "2021-11-30T18:29:16.581715Z",
     "iopub.status.idle": "2021-11-30T18:29:16.586977Z",
     "shell.execute_reply": "2021-11-30T18:29:16.586134Z",
     "shell.execute_reply.started": "2021-11-30T18:29:16.582027Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_learning_curves(hist, curve1, curve2):\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.plot(hist[curve1])\n",
    "    plt.plot(hist[curve2])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T18:30:03.567799Z",
     "iopub.status.busy": "2021-11-30T18:30:03.567451Z",
     "iopub.status.idle": "2021-11-30T18:53:38.853252Z",
     "shell.execute_reply": "2021-11-30T18:53:38.852207Z",
     "shell.execute_reply.started": "2021-11-30T18:30:03.567768Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read the data\n",
    "print(\"Loading the data...\")\n",
    "train, test, data = read_data()\n",
    "print(\"Done.\")\n",
    "\n",
    "# Create word, tag and char lists\n",
    "print(\"Creating sets of words and tags...\")\n",
    "words, n_words, tags, n_tags = create_lists(data, \"BIO\")\n",
    "print(\"Done.\")\n",
    "\n",
    "# Create list of sentences\n",
    "print(\"Creating sentence list...\")\n",
    "sents = group_sentences(data, 'BIO')\n",
    "print(\"Done.\")\n",
    "\n",
    "# Remove submissions longer than a certain threshold\n",
    "print(\"Removing submissions longer than threshold...\")\n",
    "sentences = remove_sents_over_threshold(sents, 300)\n",
    "print(\"Done.\")\n",
    "\n",
    "# Create word and tag maps\n",
    "print(\"Creating word and tag maps...\")\n",
    "max_len = 300\n",
    "tag2idx = {t: i for i, t in enumerate(tags)}\n",
    "print(\"Done.\")\n",
    "\n",
    "# Pad data\n",
    "print(\"Preparing and padding training data...\")\n",
    "X1, X2, y = prepare_and_pad(sentences, max_len, tag2idx)\n",
    "print(\"Done\")\n",
    "\n",
    "# Split to train and test\n",
    "print(\"Splitting data...\")\n",
    "#X_tr = X[0:1185]\n",
    "#y_tr = y[0:1185]\n",
    "#X_te = X[1186:]\n",
    "#y_te = y[1186:]\n",
    "X1_tr, X1_te, y1_tr, y1_te = train_test_split(X1, y, test_size=0.22, shuffle=False)\n",
    "X2_tr, X2_te, y2_tr, y2_te = train_test_split(X2, y, test_size=0.22, shuffle=False)\n",
    "y_te = y1_te\n",
    "print(\"Done.\")\n",
    "\n",
    "# Setting parameters\n",
    "print(\"Setting parameters...\")\n",
    "batch_size = 32\n",
    "plt.style.use(\"ggplot\")\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "sess = tf.compat.v1.Session()\n",
    "K.set_session(sess)\n",
    "elmo_model = hub.Module(\"https://tfhub.dev/google/elmo/3\", trainable=True)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.tables_initializer())\n",
    "print(\"Done.\")\n",
    "\n",
    "# Build the model\n",
    "print(\"Building the model...\")\n",
    "model = build_model(max_len, n_words, n_tags)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "print(\"Done.\")\n",
    "\n",
    "# Split the train data to train and validation data\n",
    "print(\"Split to train and validation data...\")\n",
    "X1_tr, X1_val, y1_tr, y1_val = train_test_split(X1_tr, y1_tr, test_size=0.2, random_state=2021)\n",
    "X2_tr, X2_val, y2_tr, y2_val = train_test_split(X2_tr, y2_tr, test_size=0.2, random_state=2021)\n",
    "X1_tr = X1_tr[:(len(X1_tr)//batch_size) * batch_size]\n",
    "X2_tr = X2_tr[:(len(X2_tr)//batch_size) * batch_size]\n",
    "X1_val = X1_val[:(len(X1_val)//batch_size) * batch_size]\n",
    "X2_val = X2_val[:(len(X2_val)//batch_size) * batch_size]\n",
    "y_tr = y1_tr\n",
    "y_tr = y_tr[:(len(y_tr)//batch_size) * batch_size]\n",
    "y_val = y1_val\n",
    "y_val = y_val[:(len(y_val)//batch_size) * batch_size] \n",
    "y_tr = y_tr.reshape(y_tr.shape[0], y_tr.shape[1], 1)\n",
    "y_val = y_val.reshape(y_val.shape[0], y_val.shape[1], 1)\n",
    "print(\"Done.\")\n",
    "\n",
    "# Fit the model\n",
    "print(\"Fitting the model....\")\n",
    "history = model.fit([np.array(X1_tr), np.array(X2_tr).reshape((len(X2_tr), max_len, 40))], \n",
    "                    y_tr, \n",
    "                    validation_data=([np.array(X1_val), np.array(X2_val).reshape((len(X2_val), max_len, 40))], y_val),\n",
    "                    batch_size=batch_size, epochs=20, verbose=1)\n",
    "\n",
    "#history = model.fit(np.array(X_tr), y_tr, batch_size=24, epochs=5, validation_split=0.3, verbose=1)\n",
    "hist = pd.DataFrame(history.history)\n",
    "\n",
    "\n",
    "# Plotting learning curves\n",
    "print(\"Plotting learning curves...\")\n",
    "plot_learning_curves(hist, \"accuracy\", \"val_accuracy\")\n",
    "plot_learning_curves(hist, \"loss\", \"val_loss\")\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T18:54:15.241723Z",
     "iopub.status.busy": "2021-11-30T18:54:15.241376Z",
     "iopub.status.idle": "2021-11-30T18:54:30.711069Z",
     "shell.execute_reply": "2021-11-30T18:54:30.709523Z",
     "shell.execute_reply.started": "2021-11-30T18:54:15.241691Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict([X1_te, np.array(X2_te).reshape((len(X2_te), max_len, 40))])\n",
    "p = np.argmax(y_pred, axis=-1)\n",
    "y_orig = []\n",
    "for sent in y_te:\n",
    "    for tag in sent:\n",
    "        y_orig.append(tag)\n",
    "y_preds = []\n",
    "for sent in p:\n",
    "    for tag in sent:\n",
    "        y_preds.append(tag)\n",
    "        \n",
    "report = classification_report( y_orig, y_preds )\n",
    "print(report) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-30T18:55:35.297513Z",
     "iopub.status.busy": "2021-11-30T18:55:35.297180Z",
     "iopub.status.idle": "2021-11-30T18:55:35.304227Z",
     "shell.execute_reply": "2021-11-30T18:55:35.303290Z",
     "shell.execute_reply.started": "2021-11-30T18:55:35.297483Z"
    }
   },
   "outputs": [],
   "source": [
    "tag2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
