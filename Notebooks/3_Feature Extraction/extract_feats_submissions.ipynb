{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk import RegexpParser\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import en_core_web_sm\n",
    "from nltk import FreqDist\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    # Define path\n",
    "    os.chdir('D:/TU_Graz/Thesis/Datasets/Reddit_preprocessed')\n",
    "\n",
    "    # Read datasets\n",
    "    submissions = pd.read_csv(\"train_submissions_simplified_new_line.csv\")\n",
    "    #submissions_tokenized = pd.read_csv(\"submissions_tokenized_v2.csv\") # Tokenized with removed punctuation\n",
    "    submissions_tokenized = pd.read_csv(\"train_submissions_tokenized_final_new_line.csv\") # Tokenized without removed punctuation\n",
    "    \n",
    "    # Add an additional integer column to keep track of the current sentence, we need this for grouping the tokens\n",
    "    sentence_nums = []\n",
    "    sentence_nums = [int(sent.split(\" \")[1]) for sent in submissions_tokenized[\"Sentence\"]]\n",
    "    submissions_tokenized[\"Sentence_index\"] = pd.Series(sentence_nums)\n",
    "    \n",
    "    return submissions, submissions_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, c, s) for w, p, c, s in zip(s[\"Words\"].values.tolist(),\n",
    "                                                                 s[\"POS_tag\"].values.tolist(),\n",
    "                                                                 s[\"Chunk_tag\"].values.tolist(),\n",
    "                                                                 s[\"sent_id\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sentence_index\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "\n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_sents(submissions_tokenized):\n",
    "    getter = SentenceGetter(submissions_tokenized)\n",
    "    sentences = [[word[0] for word in sentence] for sentence in getter.sentences]\n",
    "    labels = [[s[1] for s in sentence] for sentence in getter.sentences]\n",
    "    sent_ids = [[s_id[3] for s_id in sentence] for sentence in getter.sentences]\n",
    "    \n",
    "    return sentences, labels, sent_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All possible features we can draw out from spacy\n",
    "https://spacy.io/api/doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_feats_sentence(index, sentence, sent_id):\n",
    "    data = pd.DataFrame()\n",
    "    string = \"Sentence \" + str(index)\n",
    "    whole_sent = \" \".join(sentence)\n",
    "    doc = nlp(whole_sent)\n",
    "    \n",
    "    for X in doc:\n",
    "        data = data.append([[sent_id[0], string, X.i, X.text, X.tag, X.pos, X.ent_type, X.ent_iob, X.lemma, X.norm, X.shape, X.lex_id, \n",
    "                             X.is_digit, X.is_ascii, X.is_alpha, X.is_punct, X.is_left_punct, X.is_right_punct, X.rank, \n",
    "                             X.is_bracket, X.is_space, X.is_quote, X.is_currency, X.is_stop, X.dep, X.lang, X.prob, X.sentiment, \n",
    "                             X.is_lower, X.is_upper, X.like_num, X.is_oov, X.n_lefts, X.n_rights, X.is_sent_start, \n",
    "                             X.has_vector, X.ent_kb_id, X.ent_id, X.lower, X.prefix, X.suffix, X.idx, X.cluster, len(X)]])\n",
    "       \n",
    "    data.columns = ['Sent_id', 'Sentence','Token_index','Token','POS_tag', 'POS_universal', 'NER_tag','NER_iob','lemma','norm',\n",
    "                    'shape', 'lex_id', 'is_digit', 'is_ascii', 'is_alpha', 'is_punct', 'is_left_punct', \n",
    "                    'is_right_punct', 'rank', 'is_bracket', 'is_space', 'is_quote', 'is_currency', 'stopword', 'dependency', \n",
    "                    'language', 'log_prob', 'sent', 'is_lower', 'is_upper', 'like_num', 'out_of_vocab', 'num_lefts', \n",
    "                    'num_rights', 'sent_start', 'has_vector', 'knowledge_base', 'id_entity', 'lower', \n",
    "                    'prefix', 'suffix', 'chr_offset', 'brown_cluster', 'num_chars']\n",
    "    return data\n",
    "\n",
    "def spacy_feats_all(sentences, sent_ids):\n",
    "    data = pd.DataFrame()\n",
    "    \n",
    "    for i in range(0, len(sentences)):\n",
    "        tmp_df = spacy_feats_sentence(i, sentences[i], sent_ids[i])\n",
    "        data = data.append([tmp_df])\n",
    "    data.columns = ['Sent_id','Sentence','Token_index','Token','POS_tag', 'POS_universal', 'NER_tag','NER_iob','lemma','norm',\n",
    "                    'shape', 'lex_id', 'is_digit', 'is_ascii', 'is_alpha', 'is_punct', 'is_left_punct', \n",
    "                    'is_right_punct', 'rank', 'is_bracket', 'is_space', 'is_quote', 'is_currency', 'stopword', 'dependency', \n",
    "                    'language', 'log_prob', 'sent', 'is_lower', 'is_upper', 'like_num', 'out_of_vocab', 'num_lefts', \n",
    "                    'num_rights', 'sent_start', 'has_vector', 'knowledge_base', 'id_entity', 'lower', \n",
    "                    'prefix', 'suffix', 'chr_offset', 'brown_cluster', 'num_chars']\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_feats_tensors(index, sentence):\n",
    "    data = pd.DataFrame()\n",
    "    string = \"Sentence \" + str(index)\n",
    "    whole_sent = \" \".join(sentence)\n",
    "    doc = nlp(whole_sent)\n",
    "    for X in doc:\n",
    "        tensors = [item for item in X.tensor]\n",
    "        data = data.append([tensors])\n",
    "    return data\n",
    "\n",
    "def spacy_feats_tensors_all(sentences):\n",
    "    data = pd.DataFrame()\n",
    "    \n",
    "    for i in range(0, len(sentences)):\n",
    "        tmp_df = spacy_feats_tensors(i, sentences[i])\n",
    "        data = data.append([tmp_df])  \n",
    "    tensors = [\"Vector_ \" + str(i) for i in range(0, 96)]\n",
    "    data.columns = [tensors]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various sentiment features with the pretrained Vader Sentiment Analyser from NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_feats(sentence, words):\n",
    "    all_sentence_scores = pd.DataFrame()\n",
    "    words = words.tolist()\n",
    "    whole_sent = \" \".join(sentence)\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    s_neg_score, s_neu_score, s_pos_score, s_compound_score = analyzer.polarity_scores(whole_sent).values()\n",
    "    for i in range(0, len(words)):\n",
    "        w_neg_score, w_neu_score, w_pos_score, w_compound_score = analyzer.polarity_scores(words[i]).values()\n",
    "        if s_compound_score == 0:\n",
    "            s_compound_score = 0.0001\n",
    "        sent_ratio = w_compound_score/s_compound_score        \n",
    "        if i == 0:\n",
    "            predecessor_sent = 0\n",
    "        else:\n",
    "            _,_,_,predecessor_sent = analyzer.polarity_scores(words[i-1]).values()\n",
    "        if i < len(words)-1:\n",
    "            _,_,_,successor_sent = analyzer.polarity_scores(words[i+1]).values()\n",
    "        else:\n",
    "            successor_sent = 0\n",
    "        all_sentence_scores = all_sentence_scores.append([[s_compound_score, w_neg_score, w_neu_score, w_pos_score, w_compound_score, sent_ratio, predecessor_sent, successor_sent]])\n",
    "    all_sentence_scores.columns = [\"Sentence_sent\", \"Neg_sent_score\", \"Neu_sent_score\", \"Pos_sent_score\", \"Sent_score\", \"word_to_sentence_sent_ratio\", \"prev_word_sent\", \"next_word_sent\"]\n",
    "    return all_sentence_scores\n",
    "\n",
    "def sentiment_feats_all(sentences, df_subset):\n",
    "    feats = pd.DataFrame()\n",
    "    for i in range(0, len(sentences)):\n",
    "        words_curr_sent = df_subset[df_subset[\"Sentence\"] == \"Sentence \" + str(i)][\"Token\"]\n",
    "        new_feats = sentiment_feats(sentences[i], words_curr_sent)\n",
    "        feats = feats.append([new_feats])\n",
    "    feats.columns = [\"Sentence sent\", \"Neg_sent_score\", \"Neu_sent_score\", \"Pos_sent_score\", \"Sent_score\", \"word_to_sentence_sent_ratio\", \"prev_word_sent\", \"next_word_sent\"]\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term frequency features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_feats(sentences, df_subset):\n",
    "    feats = pd.DataFrame()\n",
    "    freqdist = FreqDist(df_subset[\"Token\"])\n",
    "    total_word_count = sum(freqdist.values())\n",
    "    all_freqs = freqdist.most_common()\n",
    "    for word in df_subset[\"Token\"]:\n",
    "        abs_frequency = freqdist[word]\n",
    "        normalized_frequency = freqdist[word] / total_word_count\n",
    "        feats = feats.append([[normalized_frequency, abs_frequency]])\n",
    "    feats.columns = [\"norm_freq\", \"abs_freq\"]\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bigrams and trigrams frequency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bigrams_trigrams(sentences):\n",
    "    stops = stopwords.words('english')\n",
    "    stops.append('new_line')\n",
    "    all_sentences = []\n",
    "    for sentence in sentences:\n",
    "        all_sentences.append(\" \".join(sentence))\n",
    "    vectorizer_bigrams = TfidfVectorizer(analyzer = \"word\", ngram_range=(2, 2), tokenizer = None, preprocessor = None, \n",
    "                                 stop_words = stops, max_features = 25, max_df = 0.9) \n",
    "    vectorizer_trigrams = TfidfVectorizer(analyzer = \"word\", ngram_range=(3, 3), tokenizer = None, preprocessor = None, \n",
    "                                 stop_words = stops, max_features = 25, max_df = 0.9) \n",
    "    feats_bigrams = vectorizer_bigrams.fit_transform(all_sentences)\n",
    "    feats_trigrams = vectorizer_trigrams.fit_transform(all_sentences)\n",
    "    bigrams = vectorizer_bigrams.get_feature_names()\n",
    "    trigrams = vectorizer_trigrams.get_feature_names()\n",
    "    return bigrams, trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_ngrams(ngrams, words):\n",
    "    feats = pd.DataFrame()\n",
    "    n = len(ngrams[0].split(\" \"))\n",
    "    words = words.tolist()\n",
    "    is_predecessor = list()\n",
    "    \n",
    "    if n == 2:\n",
    "        curr_sent_ngrams = nltk.bigrams(words)\n",
    "        curr_ngrams = [ngram for ngram in curr_sent_ngrams] \n",
    "    elif n == 3:\n",
    "        curr_sent_ngrams = nltk.trigrams(words)\n",
    "        curr_ngrams = [ngram for ngram in curr_sent_ngrams]     \n",
    "         \n",
    "    for ngram in ngrams:\n",
    "        splits = ngram.split(\" \")\n",
    "        tmp = []\n",
    "        if n == 2:\n",
    "            for i in range(0, len(words)):\n",
    "                counter = 0\n",
    "                for j in range(0, i):\n",
    "                    if i >= 2:\n",
    "                        if j+2 < i:\n",
    "                            if (words[j], words[j+1]) == splits:\n",
    "                                counter += 1\n",
    "                tmp.append(counter)\n",
    "        elif n == 3:\n",
    "            for i in range(0, len(words)):\n",
    "                counter = 0\n",
    "                for j in range(0, i):\n",
    "                    if i >= 3:\n",
    "                        if j+3 < i:\n",
    "                            if(words[j], words[j+1], words[j+2]) == splits:\n",
    "                                counter += 1\n",
    "                tmp.append(counter)\n",
    "        is_predecessor.append(tmp)\n",
    "    index = 1\n",
    "    for p in is_predecessor:\n",
    "        feats[str(index)] = pd.Series(p)\n",
    "        index = index + 1\n",
    "    return(feats)\n",
    "    \n",
    "def tf_ngrams_all(sentences, df_subset, ngrams):\n",
    "    feats = pd.DataFrame()\n",
    "    for i in range(0, len(sentences)):\n",
    "        words_curr_sent = df_subset[df_subset[\"Sentence\"] == \"Sentence \" + str(i)][\"Token\"]\n",
    "        ngram_feats = tf_ngrams(ngrams, words_curr_sent)\n",
    "        feats = feats.append([ngram_feats])\n",
    "    names = [str(ngram) + \"_is_predecessor\" for ngram in ngrams]\n",
    "    feats.columns = [names]\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data: submissions has the preserved form od the reddit text, submissions_tokenized contains the tokenized \n",
    "# sentences. We need both to preserve the sentence order when extracting the features\n",
    "submissions, submissions_tokenized = read_data()\n",
    "\n",
    "# Submissions_tokenized contain the entire dataset, we will also need the tokens from each sentence separate, so \n",
    "# we group them\n",
    "sentences, labels, sent_ids = group_sents(submissions_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm looking for a thought provoking, dark, suspenseful movie. I'm looking for a movie that really makes you think and takes you away from reality to where you're just thinking about how the movie is going to play out. I like the kind of movies that don't make too much sense at first, which adds to the thinking aspect of them. I loved Shutter Island because of the ending, so if you have any movies that have sort of an eerie feeling with suspense and a good plot to them would be awesome. Thanks in advance.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submissions.text[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking VBG VERB 100 Xxxxx True False\n",
      "for IN ADP 85 xxx True True\n",
      "a DT DET 90 x True True\n",
      "thought NN NOUN 92 xxxx True False\n",
      "provoking JJ ADJ 84 xxxx True False\n",
      ", , PUNCT 97 , False False\n",
      "dark JJ ADJ 84 xxxx True False\n",
      ", , PUNCT 97 , False False\n",
      "suspenseful JJ ADJ 84 xxxx True False\n",
      "movie NN NOUN 92 xxxx True False\n",
      ". . PUNCT 97 . False False\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Looking for a thought provoking, dark, suspenseful movie.\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.tag_, token.pos_, token.pos, token.shape_, \n",
    "          token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 1st group of features\n",
      "Extracted 2nd group of features\n",
      "Extracted 3rd group of features\n",
      "Extracted 4th group of features\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Extract different sorts of features\n",
    "feats1 = spacy_feats_all(sentences, sent_ids)\n",
    "print(\"Extracted 1st group of features\")\n",
    "\n",
    "feats2 = spacy_feats_tensors_all(sentences)\n",
    "print(\"Extracted 2nd group of features\")\n",
    "\n",
    "feats3 = sentiment_feats_all(sentences, feats1[[\"Sentence\", \"Token\"]])\n",
    "print(\"Extracted 3rd group of features\")\n",
    "\n",
    "feats4 = tf_feats(sentences, feats1[[\"Sentence\", \"Token\"]])\n",
    "print(\"Extracted 4th group of features\")\n",
    "\n",
    "#bigrams, trigrams = extract_bigrams_trigrams(sentences)\n",
    "#feats5 = tf_ngrams_all(sentences, feats1[[\"Sentence\", \"Token\"]], bigrams)\n",
    "#print(\"Extracted 5th group of features\")\n",
    "\n",
    "#feats6 = tf_ngrams_all(sentences, feats1[[\"Sentence\", \"Token\"]], trigrams)\n",
    "#print(\"Extracted 6th group of features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = pd.concat([feats1, feats2, feats3, feats4], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats.to_csv(\"../Reddit_features/train_submissions_features_v2_num_FINALL_new_sent.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
