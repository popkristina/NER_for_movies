{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path\n",
    "os.chdir('D:/TU_Graz/Thesis/Datasets/Reddit_preprocessed')\n",
    "\n",
    "# Read datasets\n",
    "submissions = pd.read_csv(\"submissions_simplified.csv\")\n",
    "submissions_tokenized = pd.read_csv(\"submissions_tokenized_v2.csv\") # Tokenized with removed punctuation\n",
    "#submissions_tokenized = pd.read_csv(\"submissions_tokenized.csv\") # Tokenized without removed punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, p, c) for w, p, c in zip(s[\"Words\"].values.tolist(),\n",
    "                                                           s[\"POS_tag\"].values.tolist(),\n",
    "                                                           s[\"Chunk_tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sentence\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "\n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "getter = SentenceGetter(submissions_tokenized)\n",
    "sentences = [[word[0] for word in sentence] for sentence in getter.sentences]\n",
    "labels = [[s[1] for s in sentence] for sentence in getter.sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk import RegexpParser\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All possible features we can draw out from spacy\n",
    "https://spacy.io/api/doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_feats_sentence(index, sentence):\n",
    "    data = pd.DataFrame()\n",
    "    string = \"Sentence \" + str(index)\n",
    "    whole_sent = \" \".join(sentence)\n",
    "    doc = nlp(whole_sent)\n",
    "    \n",
    "    for X in doc:\n",
    "        data = data.append([[string, X.i, X.text, X.tag_, X.ent_type_, X.ent_iob_, X.lemma_, X.norm_, X.shape_, \n",
    "                             X.is_digit, X.is_ascii, X.is_alpha, X.is_punct, X.is_left_punct, X.is_right_punct,\n",
    "                             X.is_bracket, X.is_quote, X.is_stop, X.dep_, X.lang_, X.prob, X.sentiment, X.is_lower, \n",
    "                             X.is_upper, X.like_num, X.is_oov, X.tensor]])\n",
    "    \n",
    "    data.columns = ['Sentence','Token_index','Token','POS_tag','NER_tag','NER_iob','lemma','norm','shape',\n",
    "                                 'is_digit', 'is_ascii', 'is_alpha', 'is_punct', 'is_left_punct', 'is_right_punct',\n",
    "                                 'is_bracket', 'is_quote', 'stopword', 'dependency', 'language', 'log_prob', 'sent', \n",
    "                                 'is_lower', 'is_upper', 'like_num', 'out_of_vocab', 'tensor']\n",
    "    return data\n",
    "\n",
    "def spacy_feats_all(sentences):\n",
    "    data = pd.DataFrame()\n",
    "    \n",
    "    for i in range(0, len(sentences)):\n",
    "        tmp_df = spacy_feats_sentence(i, sentences[i])\n",
    "        data = data.append([tmp_df])\n",
    "    \n",
    "    data.columns = ['Sentence','Token_index','Token','POS_tag','NER_tag','NER_iob','lemma','norm','shape',\n",
    "                                 'is_digit', 'is_ascii', 'is_alpha', 'is_punct', 'is_left_punct', 'is_right_punct',\n",
    "                                 'is_bracket', 'is_quote', 'stopword', 'dependency', 'language', 'log_prob', 'sent', \n",
    "                                 'is_lower', 'is_upper', 'like_num', 'out_of_vocab', 'tensor']\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = spacy_feats_all(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various sentiment features with the pretrained Vader Sentiment Analyser from NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_feats(sentence, words):\n",
    "    all_sentence_scores = pd.DataFrame()\n",
    "    words = words.tolist()\n",
    "    whole_sent = \" \".join(sentence)\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    s_neg_score, s_neu_score, s_pos_score, s_compound_score = analyzer.polarity_scores(whole_sent).values()\n",
    "    for i in range(0, len(words)):\n",
    "        w_neg_score, w_neu_score, w_pos_score, w_compound_score = analyzer.polarity_scores(words[i]).values()\n",
    "        if s_compound_score == 0:\n",
    "            s_compound_score = 0.0001\n",
    "        sent_ratio = w_compound_score/s_compound_score\n",
    "        if i == 0:\n",
    "            predecessor_sent = 0\n",
    "        else:\n",
    "            _,_,_,predecessor_sent = analyzer.polarity_scores(words[i-1]).values()\n",
    "        if i < len(words)-1:\n",
    "            _,_,_,successor_sent = analyzer.polarity_scores(words[i+1]).values()\n",
    "        else:\n",
    "            successor_sent = 0\n",
    "        all_sentence_scores = all_sentence_scores.append([[s_compound_score, w_neg_score, w_neu_score, w_pos_score, w_compound_score, sent_ratio, predecessor_sent, successor_sent]])\n",
    "    all_sentence_scores.columns = [\"Sentence_sent\", \"Neg_sent_score\", \"Neu_sent_score\", \"Pos_sent_score\", \"Sent_score\", \"word_to_sentence_sent_ratio\", \"prev_word_sent\", \"next_word_sent\"]\n",
    "    return all_sentence_scores\n",
    "\n",
    "def sentiment_feats_all(sentences, df_subset):\n",
    "    feats = pd.DataFrame()\n",
    "    for i in range(0, len(sentences)):\n",
    "        words_curr_sent = df_subset[df_subset[\"Sentence\"] == \"Sentence \" + str(i)][\"Token\"]\n",
    "        new_feats = sentiment_feats(sentences[i], words_curr_sent)\n",
    "        feats = feats.append([new_feats])\n",
    "    feats.columns = [\"Sentence sent\", \"Neg_sent_score\", \"Neu_sent_score\", \"Pos_sent_score\", \"Sent_score\", \"word_to_sentence_sent_ratio\", \"prev_word_sent\", \"next_word_sent\"]\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = sentiment_feats_all(sentences, tmp[[\"Sentence\", \"Token\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the most common bigrams and trigrams that occur throughout the text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bigrams_trigrams(sentences):\n",
    "    all_bigrams = []\n",
    "    all_trigrams = []\n",
    "    for sentence in sentences:\n",
    "        lower_sent = [s.lower() for s in sentence]\n",
    "        bigrams = nltk.bigrams(lower_sent)\n",
    "        trigrams = nltk.trigrams(lower_sent)\n",
    "        for bigram in bigrams:\n",
    "            all_bigrams.append(bigram)\n",
    "        for trigram in trigrams:\n",
    "            all_trigrams.append(trigram)\n",
    "    freqs_bg = nltk.FreqDist(all_bigrams)\n",
    "    freqs_tg = nltk.FreqDist(all_trigrams)\n",
    "    bg_cnt = Counter()\n",
    "    tg_cnt = Counter()\n",
    "    for k, v in freqs_bg.items():\n",
    "        bg_cnt[k] = v\n",
    "    for k, v in freqs_tg.items():\n",
    "        tg_cnt[k] = v\n",
    "    return bg_cnt.most_common(30), tg_cnt.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams, trigrams = extract_bigrams_trigrams(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_ngrams(ngrams, words):\n",
    "    feats = pd.DataFrame()\n",
    "    n = len(ngrams[0][0])\n",
    "    ngram_tuples = [ngram[0] for ngram in ngrams]\n",
    "    ngram_counts = [ngram[1] for ngram in ngrams]\n",
    "    words = words.tolist()\n",
    "    is_in_sent = []\n",
    "    is_predecessor = []\n",
    "    \n",
    "    if n == 2:\n",
    "        curr_sent_ngrams = nltk.bigrams(words)\n",
    "        curr_sent_ngrams = [ngram for ngram in curr_sent_ngrams]\n",
    "    elif n == 3:\n",
    "        curr_sent_ngrams = nltk.trigrams(words)\n",
    "        curr_sent_ngrams = [ngram for ngram in curr_sent_ngrams]\n",
    "    for ngram in ngram_tuples:\n",
    "        if ngram in curr_sent_ngrams:\n",
    "            is_in_sent.append(True)\n",
    "        else:\n",
    "            is_in_sent.append(False)\n",
    "    \n",
    "    for i in range(0, len(words)):\n",
    "        tmp = []\n",
    "        flag = True\n",
    "        feats = feats.append([is_in_sent])\n",
    "        if n == 2:\n",
    "            for tup in ngram_tuples:\n",
    "                for j in range(0, i):\n",
    "                    if i == 0:\n",
    "                        flag = False\n",
    "                    elif i == 1:\n",
    "                        flag = False\n",
    "                    elif i >= 2:\n",
    "                        if j+2 < i:\n",
    "                            if (words[j], words[j+1]) == tup:\n",
    "                                flag = True\n",
    "                            else:\n",
    "                                flag = False\n",
    "                tmp.append(flag)\n",
    "        \n",
    "        elif n == 3:\n",
    "            for tup in ngram_tuples:\n",
    "                for j in range(0, i):\n",
    "                    if i == 0:\n",
    "                        flag = False\n",
    "                    elif i == 1:\n",
    "                        flag = False\n",
    "                    elif i == 2:\n",
    "                        flag = False\n",
    "                    elif i >= 3:\n",
    "                        if j+3 < i:\n",
    "                            if(words[j], words[j+1], words[j+2]) == tup:\n",
    "                                flag = True\n",
    "                            else:\n",
    "                                flag = False\n",
    "                tmp.append(flag)\n",
    "        is_predecessor.append(tmp)\n",
    "        \n",
    "    print(len(words))\n",
    "    \n",
    "def tf_idf_ngrams_all(sentences, df_subset, ngrams):\n",
    "    feats = pd.DataFrame()\n",
    "    for i in range(0, len(sentences)):\n",
    "        words_curr_sent = df_subset[df_subset[\"Sentence\"] == \"Sentence \" + str(i)][\"Token\"]\n",
    "        ngram_feats = tf_idf_ngrams(ngrams, words_curr_sent)\n",
    "        break\n",
    "        #feats = feats.append([ngram_feats])\n",
    "    feats.columns = []\n",
    "    #return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    }
   ],
   "source": [
    "tf_idf_ngrams_all(sentences, tmp[[\"Sentence\", \"Token\"]], bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('looking', 'for'), 581),\n",
       " (('of', 'the'), 365),\n",
       " (('i', 've'), 362),\n",
       " (('movies', 'that'), 349),\n",
       " (('movies', 'like'), 286),\n",
       " (('im', 'looking'), 262),\n",
       " (('in', 'the'), 249),\n",
       " (('it', 's'), 193),\n",
       " (('to', 'watch'), 190),\n",
       " (('a', 'movie'), 186),\n",
       " (('for', 'a'), 182),\n",
       " (('movies', 'with'), 178),\n",
       " (('and', 'i'), 176),\n",
       " (('and', 'the'), 165),\n",
       " (('but', 'i'), 162),\n",
       " (('similar', 'to'), 159),\n",
       " (('would', 'be'), 157),\n",
       " (('like', 'the'), 149),\n",
       " (('movies', 'i'), 145),\n",
       " (('that', 'are'), 144),\n",
       " (('i', 'have'), 142),\n",
       " (('that', 'i'), 141),\n",
       " (('to', 'be'), 140),\n",
       " (('i', 'am'), 139),\n",
       " (('for', 'movies'), 139),\n",
       " (('i', 'love'), 126),\n",
       " (('ve', 'seen'), 123),\n",
       " (('any', 'suggestions'), 122),\n",
       " (('i', 'really'), 119),\n",
       " (('movies', 'where'), 118)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_feats(sentence, words):\n",
    "    negations = []\n",
    "    specific patterns = []\n",
    "    most_freq_bigrams = []\n",
    "    most_freq_trigrams = []\n",
    "    opening_movie_indics = []\n",
    "    closing_movie_indics = []\n",
    "    stopwords = []\n",
    "    \n",
    "    return list()\n",
    "\n",
    "def lexical_feats_all():\n",
    "    \n",
    "    return list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Im', 'NNP', 'O')\n",
      "('looking', 'VBG', 'O')\n",
      "('for', 'IN', 'O')\n",
      "('movies', 'NNS', 'O')\n",
      "('feature', 'VBP', 'O')\n",
      "('disfunctional', 'JJ', 'O')\n",
      "('unlikable', 'JJ', 'O')\n",
      "('protagonists', 'NNS', 'O')\n",
      "('who', 'WP', 'O')\n",
      "('are', 'VBP', 'O')\n",
      "('toxic', 'JJ', 'O')\n",
      "('Movies', 'NNS', 'O')\n",
      "('like', 'IN', 'O')\n",
      "('Nightcrawler', 'NNP', 'O')\n",
      "('Little', 'NNP', 'O')\n",
      "('Miss', 'NNP', 'O')\n",
      "('Sunshine', 'NNP', 'O')\n",
      "('Taxi', 'NNP', 'O')\n",
      "('Driver', 'NNP', 'O')\n",
      "('There', 'EX', 'O')\n",
      "('Will', 'NNP', 'O')\n",
      "('Be', 'NNP', 'O')\n",
      "('Blood', 'NNP', 'O')\n",
      "('Hesher', 'NNP', 'O')\n",
      "('Wolf', 'NNP', 'O')\n",
      "('of', 'IN', 'O')\n",
      "('Wall', 'NNP', 'O')\n",
      "('Street', 'NNP', 'O')\n",
      "('or', 'CC', 'O')\n",
      "('Closer', 'NNP', 'O')\n",
      "('where', 'WRB', 'O')\n",
      "('the', 'DT', 'B-NP')\n",
      "('movie', 'NN', 'I-NP')\n",
      "('s', 'JJ', 'B-NP')\n",
      "('protagonist', 'NN', 'I-NP')\n",
      "('s', 'NN', 'B-NP')\n",
      "('is', 'VBZ', 'O')\n",
      "('completely', 'RB', 'O')\n",
      "('destructive', 'JJ', 'O')\n",
      "('to', 'TO', 'O')\n",
      "('them', 'PRP', 'O')\n",
      "('self', 'PRP', 'O')\n",
      "('and', 'CC', 'O')\n",
      "('those', 'DT', 'O')\n",
      "('around', 'IN', 'O')\n",
      "('them', 'PRP', 'O')\n"
     ]
    }
   ],
   "source": [
    "pos_tags = pos_tag(words)\n",
    "pattern = 'NP: {<DT>?<JJ>*<NN>}'\n",
    "chunker = RegexpParser(pattern)\n",
    "chunks = chunker.parse(tags)\n",
    "chunk_list = []\n",
    "tagged_chunks = tree2conlltags(chunks)\n",
    "for c in tagged_chunks:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
