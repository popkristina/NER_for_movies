{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path\n",
    "os.chdir('D:/TU_Graz/Thesis/Datasets/Reddit_original')\n",
    "\n",
    "# Read datasets\n",
    "movie_titles = pd.read_csv(\"movie_titles.csv\", sep = ';')\n",
    "comments = pd.read_csv(\"comments.csv\", sep = ';')\n",
    "comments = comments.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies=[]\n",
    "for line in comments['movie_ids']:\n",
    "    parts = line.split('|')\n",
    "    movies_sub = \"\"\n",
    "    for index, row in movie_titles.iterrows():\n",
    "        if row['movie_id'] in parts:\n",
    "            movies_sub = movies_sub + \"|\" + row['movie_title']\n",
    "    movies.append(movies_sub[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c =  {'text' : comments['reddit_comment_text'],\n",
    "      'movies' : pd.Series(movies)\n",
    "     }\n",
    "data_simplified = pd.DataFrame(c)   \n",
    "data_simplified.to_csv(\"../Reddit_preprocessed/comments_simplified.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tokenize import regexp_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_tokenized = []\n",
    "splitted_words_all = []\n",
    "for index, row in data_simplified.iterrows(): \n",
    "    text = re.sub(\"<br/>\", \" \", row['text'])\n",
    "    text = re.sub(\"<br>\", \" \", text)\n",
    "    text = re.sub(\"[\\[\\{\\(][Rr][Ee][Qq][\\w]*[\\]\\}\\)]\", \"\", text)\n",
    "    text = re.sub(\"[\\[\\{\\(][Ss][Uu][Gg][Gg][Ee][Ss][Tt][/\\w]*[\\]\\}\\)]\", \"\", text)\n",
    "    url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|''[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    text = re.sub(url_regex, \"URL_HERE\", text)\n",
    "    text = re.sub(\"^ \", \"\", text)\n",
    "    tokens = WordPunctTokenizer().tokenize(row['text'])\n",
    "    for token in tokens:\n",
    "        splitted_words_all.append(token)\n",
    "    sentences_tokenized.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_indicators = []\n",
    "index = 0\n",
    "for sentence in sentences_tokenized:\n",
    "    for word in sentence:\n",
    "        sentence_indicators.append(\"Sentence \" + str(index))\n",
    "    index = index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = {'Sentence' : pd.Series(sentence_indicators),\n",
    "     'Words' : pd.Series(splitted_words_all)}\n",
    "data_tokenized = pd.DataFrame(t)   \n",
    "data_tokenized.to_csv(\"../Reddit_preprocessed/comments_tokenized.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
