{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk import RegexpParser\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    # Define path\n",
    "    os.chdir('D:/TU_Graz/Thesis/Datasets/Reddit_original')\n",
    "\n",
    "    # Read datasets\n",
    "    submissions = pd.read_csv(\"submissions.csv\", sep = ';')\n",
    "    movie_titles = pd.read_csv(\"movie_titles.csv\", sep = ';')\n",
    "    test_set_ids_list = pd.read_csv(\"submission_ids_test_set.txt\")\n",
    "    test_set_ids_list = list(test_set_ids_list[\"5c1dp2\"])\n",
    "    submissions = submissions.fillna(\"\")\n",
    "    \n",
    "    return submissions, movie_titles, test_set_ids_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to make a dataset with only columns for the submission text (submission title and text concatenated), the movies, actors and genres identified in the submission. (We'll be interested whether they are positive or negative later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make new lists of positive and negative movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_lists(dataframe, column):\n",
    "    movies = []\n",
    "    for line in submissions[column]:\n",
    "        parts = line.split('|')\n",
    "        movies_sub = \"\"\n",
    "        for index, row in movie_titles.iterrows():\n",
    "            if row['movie_id'] in parts:\n",
    "                movies_sub = movies_sub + \"|\" + row['movie_title']\n",
    "        movies.append(movies_sub[1:])\n",
    "    return movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the new dataset with data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df(submissions, pos_movies, neg_movies):\n",
    "    sub_modified = pd.DataFrame()\n",
    "    sub_modified['text'] = submissions['reddit_submission_title'] + \" \" + submissions['reddit_submission_text'] \n",
    "    sub_modified['genres'] = submissions['positive_genres'] + '|' + submissions['negative_genres']\n",
    "    sub_modified['actors'] = submissions['positive_actors'] + '|' + submissions['negative_actors']\n",
    "    sub_modified['movies'] = pd.Series(pos_movies) + '|' + pd.Series(neg_movies)\n",
    "    sub_modified['keywords'] = submissions['positive_keywords'] + '|' + submissions['negative_keywords']\n",
    "    \n",
    "    return sub_modified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some entries might end with '|' because of the concatenation, or if no words were detected, just '|' stands, so that should be fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_vertical_line(df, column):\n",
    "    col = []\n",
    "    for line in sub_modified[column]:\n",
    "        if len(line) == 1:\n",
    "            line = \"\"\n",
    "        if line.startswith('|'):\n",
    "            line = line[1:]\n",
    "        if line.endswith('|'):\n",
    "            line = line[:-1]\n",
    "        col.append(line)\n",
    "    return col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce redundant and not informative words in the text, we remove the URL's and the opening tags like: [Request] which indicate that it's a request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentences(sub_modified):\n",
    "    sentences = []\n",
    "    for index, row in sub_modified.iterrows():\n",
    "        text = re.sub(\"<br/>\", \" NEW_LINE \", row['text'])\n",
    "        text = re.sub(\"<br>\", \" NEW_LINE \", text)\n",
    "        text = re.sub(\"-\", \"- \", text)\n",
    "        url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|''[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "        text = re.sub(url_regex, \"\", text)\n",
    "        text = re.sub(\"[\\[\\{\\(][Rr][Ee][Qq][\\w]*[\\]\\}\\)]\", \"\", text)\n",
    "        text = re.sub(\"[\\[\\{\\(][Ss][Uu][Gg][Gg][Ee][Ss][Tt][/\\w]*[\\]\\}\\)]\", \"\", text)\n",
    "        text = re.sub(\"^ \", \"\", text)\n",
    "        #tokens = WordPunctTokenizer().tokenize(text)\n",
    "        sentences.append(text)\n",
    "        #print(tokens)\n",
    "        \n",
    "        return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_based_on_id(data_simplfied):\n",
    "    # Split the dataset so that we can get a test dataset we'll continuosly use based on previously defined sub. ids\n",
    "    train = pd.DataFrame()\n",
    "    test = pd.DataFrame()\n",
    "    for index, row in data_simplified.iterrows():\n",
    "        if row['id'] in test_set_ids_list:\n",
    "            test = test.append([row])\n",
    "        else:\n",
    "            train = train.append([row])\n",
    "    train.columns = data_simplified.columns\n",
    "    test.columns = data_simplified.columns\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the sentences, perform part-of-speech tagging with nltk pos tagger and create a list of words and tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(data):\n",
    "    sentences_tokenized = []\n",
    "    splitted_words_all = []\n",
    "    pos_tags = []\n",
    "    chunk_list = []\n",
    "    curr_id = []\n",
    "    for index, row in data.iterrows(): \n",
    "        #text = re.sub(\"I'm\", \"Im\", row['text']) i'm, i'll, I'll, We're, we're, you're, You're, They're, they'll, don't..\n",
    "        #tokens = WordPunctTokenizer().tokenize(row['text'])\n",
    "        #tokens = nltk.word_tokenize(row[\"text\"])\n",
    "    \n",
    "        # forms tokens out of alphabetic sequences, money expressions, and any other non-whitespace sequences\n",
    "        tokens = regexp_tokenize(row['text'], pattern = '\\w+|\\$[\\d\\.]+|\\S+') \n",
    "    \n",
    "        # forms tokens with removing the punctuation\n",
    "        #tokens = regexp_tokenize(row['text'], pattern = \"\\w+\")\n",
    "        tags = pos_tag(tokens)\n",
    "    \n",
    "        pattern = 'NP: {<DT>?<JJ>*<NN>}'\n",
    "        chunker = RegexpParser(pattern)\n",
    "        chunks = chunker.parse(tags)\n",
    "        tagged_chunks = tree2conlltags(chunks)\n",
    "\n",
    "        for token in tokens:\n",
    "            splitted_words_all.append(token)\n",
    "            curr_id.append(row['id'])\n",
    "        sentences_tokenized.append(tokens)\n",
    "        for tag in tags:\n",
    "            pos_tags.append(tag[1])\n",
    "        for chunk in tagged_chunks:\n",
    "            chunk_list.append(chunk[2])\n",
    "    \n",
    "    sentence_indicators = []\n",
    "    index = 0\n",
    "    for sentence in sentences_tokenized:\n",
    "        for word in sentence:\n",
    "            sentence_indicators.append(\"Sentence \" + str(index))\n",
    "        index = index + 1\n",
    "        \n",
    "    t = {'Sentence' : pd.Series(sentence_indicators),\n",
    "         'sent_id' : pd.Series(curr_id),\n",
    "         'Words' : pd.Series(splitted_words_all),\n",
    "         'POS_tag' : pd.Series(pos_tags),\n",
    "         'Chunk_tag' : pd.Series(chunk_list)\n",
    "        }\n",
    "    \n",
    "    final_data = pd.DataFrame(t)\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "submissions, movie_titles, test_set_ids_list = read_data()\n",
    "\n",
    "# Make positive and negative lists of movies\n",
    "pos_movies = make_lists(submissions, 'positive_movie_ids')\n",
    "neg_movies = make_lists(submissions, 'negative_movie_ids')\n",
    "\n",
    "# Make a new updated dataset\n",
    "sub_modified = make_df(submissions, pos_movies, neg_movies)\n",
    "\n",
    "# Fix the vertical line for all entities\n",
    "genres = fix_vertical_line(sub_modified, 'genres')\n",
    "actors = fix_vertical_line(sub_modified, 'actors')\n",
    "movies = fix_vertical_line(sub_modified, 'movies')\n",
    "keywords = fix_vertical_line(sub_modified, 'keywords')\n",
    "\n",
    "# Preprocess sentences\n",
    "sentences = preprocess_sentences(sub_modified)\n",
    "\n",
    "# Create new dataset\n",
    "d = {'id' : pd.Series(submissions[\"reddit_submission_id\"]),\n",
    "     'text' : pd.Series(sentences),\n",
    "     'movies' : pd.Series(movies),\n",
    "     'pos_movies' : pd.Series(pos_movies),\n",
    "     'neg_movies' : pd.Series(neg_movies),\n",
    "     'genres' : pd.Series(genres),\n",
    "     'pos_genres' : submissions['positive_genres'],\n",
    "     'neg_genres' : submissions['negative_genres'],\n",
    "     'actor' : pd.Series(actors),\n",
    "     'pos_actor' : submissions['positive_actors'],\n",
    "     'neg_actor' : submissions['negative_actors'],\n",
    "     'keywords' : pd.Series(keywords),\n",
    "     'pos_keywords' : submissions['positive_keywords'],\n",
    "     'neg_keywords' : submissions['negative_keywords']}\n",
    "data_simplified = pd.DataFrame(d)   \n",
    "data_simplified.to_csv(\"../Reddit_preprocessed/submissions_simplified.csv\",index = False)\n",
    "\n",
    "# Split to train and test sets\n",
    "train, test = split_based_on_id(data_simplified)\n",
    "\n",
    "# Save the train and test datasets\n",
    "train.to_csv(\"../Reddit_preprocessed/train_submissions_simplified_new_line.csv\", index = False)\n",
    "test.to_csv(\"../Reddit_preprocessed/test_submissions_simplified_new_line.csv\", index = False)\n",
    "\n",
    "# Tokenize and pos tag etc. all of the data, and separately the train and test data\n",
    "data_tokenized = tokenizer(data)\n",
    "train_tokenized = tokenizer(train)\n",
    "test_tokenized = tokenizer(test)\n",
    "\n",
    "# ...and save them all\n",
    "data_tokenized.to_csv(\"../Reddit_preprocessed/submissions_tokenized_final_new_line.csv\", index = False)\n",
    "train_tokenized.to_csv(\"../Reddit_preprocessed/train_submissions_tokenized_final_new_line.csv\", index = False)\n",
    "test_tokenized.to_csv(\"../Reddit_preprocessed/test_submissions_tokenized_final_new_line.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
